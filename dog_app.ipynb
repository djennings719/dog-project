{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[1])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faces found in humans: 99%\n",
      "Faces found in dogs: 12%\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "faces_found_human = 0\n",
    "faces_found_dogs = 0\n",
    "\n",
    "for i in range(len(human_files_short)):\n",
    "    if face_detector(human_files_short[i]):\n",
    "        faces_found_human += 1\n",
    "    if face_detector(dog_files_short[i]):\n",
    "        faces_found_dogs += 1\n",
    "    \n",
    "print(\"Faces found in humans: {}%\".format(faces_found_human))\n",
    "print(\"Faces found in dogs: {}%\".format(faces_found_dogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__  This depends on what angle the camera was at during the picture taking.  If the actor is posing or holding the camera this would be ok.  If the actor is in a state other than facing the camera then this may require using a Deep Learning network to determing whether a face was present depending on the angle that the actor is at.  \n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dogs detected in humans: 1%\n",
      "Dogs detected in dogs: 100%\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "dogs_found_human = 0\n",
    "dogs_found_dogs = 0\n",
    "\n",
    "for i in range(len(human_files_short)):\n",
    "    if dog_detector(human_files_short[i]):\n",
    "        dogs_found_human += 1\n",
    "    if dog_detector(dog_files_short[i]):\n",
    "        dogs_found_dogs += 1\n",
    "    \n",
    "print(\"Dogs detected in humans: {}%\".format(dogs_found_human))\n",
    "print(\"Dogs detected in dogs: {}%\".format(dogs_found_dogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6680/6680 [00:39<00:00, 171.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 835/835 [00:04<00:00, 190.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 836/836 [00:04<00:00, 191.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ I attempted to model the given hint.  The model looks to be good as it is convolving layers and does not gather too many parameters in the process.  \n",
    "\n",
    "The strangest change from an MLP to a CNN is that you do not have to compress the data coming into the input layer thanks to the ability to use tensors.  \n",
    "\n",
    "The first layer is a Convolutional layer.  It will output of 16 filters from this level.  The kernel is 2x2 so it will preserve its length and width shape.  The input_shape has to be declared on this entry because it is the first layer of the network.  The input_shape is the same size as the input image.  The activation function is applied to each node in order to process the data through the network.  We will use the relu activation function.  \n",
    "\n",
    "We then use a MaxPooling layer to reduce the length and width of the previous output shape.  We are reducing the length and width by a factor of 2.  Pooling does not affect the depth of the tensor.\n",
    "\n",
    "We then feed our 16 layers into the next Convolutional layer with an output of 32 layers.  The kernel size and activation function remain the same.  \n",
    "\n",
    "Maxpooling again divides the length and width by 2.  \n",
    "\n",
    "Our final Convolutional layer has an input of 32 and an output of 64.  \n",
    "\n",
    "One final Maxpooling layer again divides our length and width.  \n",
    "\n",
    "We add a GAP layer which reduces the 4D tensor to a 2D tensor keeping only the batch size and channels.\n",
    "\n",
    "We then use a fully connected layer with the channels as input and 133 outputs(our set of dogs).  This is our classification layer and is finished by applying the sigmoig function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 110, 110, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 54, 54, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               8645      \n",
      "=================================================================\n",
      "Total params: 19,189\n",
      "Trainable params: 19,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(16, 2, input_shape=(224,224,3), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Conv2D(32, 2, activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Conv2D(64, 2, activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(133, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 2:53 - loss: 4.8894 - acc: 0.0000e+0 - ETA: 48s - loss: 4.8891 - acc: 0.0125    - ETA: 30s - loss: 4.8888 - acc: 0.01 - ETA: 23s - loss: 4.8882 - acc: 0.01 - ETA: 19s - loss: 4.8876 - acc: 0.00 - ETA: 16s - loss: 4.8903 - acc: 0.00 - ETA: 15s - loss: 4.8908 - acc: 0.00 - ETA: 13s - loss: 4.8904 - acc: 0.00 - ETA: 12s - loss: 4.8911 - acc: 0.00 - ETA: 12s - loss: 4.8912 - acc: 0.00 - ETA: 11s - loss: 4.8912 - acc: 0.00 - ETA: 10s - loss: 4.8909 - acc: 0.00 - ETA: 10s - loss: 4.8906 - acc: 0.00 - ETA: 10s - loss: 4.8907 - acc: 0.00 - ETA: 9s - loss: 4.8905 - acc: 0.0070 - ETA: 9s - loss: 4.8907 - acc: 0.006 - ETA: 9s - loss: 4.8908 - acc: 0.006 - ETA: 8s - loss: 4.8911 - acc: 0.005 - ETA: 8s - loss: 4.8905 - acc: 0.006 - ETA: 8s - loss: 4.8903 - acc: 0.006 - ETA: 8s - loss: 4.8900 - acc: 0.006 - ETA: 7s - loss: 4.8898 - acc: 0.006 - ETA: 7s - loss: 4.8893 - acc: 0.006 - ETA: 7s - loss: 4.8897 - acc: 0.006 - ETA: 7s - loss: 4.8891 - acc: 0.006 - ETA: 7s - loss: 4.8890 - acc: 0.005 - ETA: 7s - loss: 4.8892 - acc: 0.006 - ETA: 6s - loss: 4.8891 - acc: 0.006 - ETA: 6s - loss: 4.8894 - acc: 0.005 - ETA: 6s - loss: 4.8887 - acc: 0.005 - ETA: 6s - loss: 4.8891 - acc: 0.005 - ETA: 6s - loss: 4.8887 - acc: 0.006 - ETA: 6s - loss: 4.8885 - acc: 0.007 - ETA: 6s - loss: 4.8880 - acc: 0.008 - ETA: 6s - loss: 4.8874 - acc: 0.007 - ETA: 5s - loss: 4.8873 - acc: 0.007 - ETA: 5s - loss: 4.8867 - acc: 0.007 - ETA: 5s - loss: 4.8873 - acc: 0.007 - ETA: 5s - loss: 4.8878 - acc: 0.007 - ETA: 5s - loss: 4.8877 - acc: 0.008 - ETA: 5s - loss: 4.8877 - acc: 0.007 - ETA: 5s - loss: 4.8874 - acc: 0.008 - ETA: 5s - loss: 4.8877 - acc: 0.008 - ETA: 5s - loss: 4.8878 - acc: 0.008 - ETA: 5s - loss: 4.8877 - acc: 0.008 - ETA: 4s - loss: 4.8876 - acc: 0.008 - ETA: 4s - loss: 4.8870 - acc: 0.007 - ETA: 4s - loss: 4.8872 - acc: 0.008 - ETA: 4s - loss: 4.8874 - acc: 0.007 - ETA: 4s - loss: 4.8869 - acc: 0.008 - ETA: 4s - loss: 4.8872 - acc: 0.007 - ETA: 4s - loss: 4.8871 - acc: 0.007 - ETA: 4s - loss: 4.8870 - acc: 0.007 - ETA: 4s - loss: 4.8872 - acc: 0.007 - ETA: 4s - loss: 4.8873 - acc: 0.007 - ETA: 4s - loss: 4.8871 - acc: 0.007 - ETA: 4s - loss: 4.8874 - acc: 0.007 - ETA: 3s - loss: 4.8874 - acc: 0.007 - ETA: 3s - loss: 4.8873 - acc: 0.007 - ETA: 3s - loss: 4.8873 - acc: 0.007 - ETA: 3s - loss: 4.8873 - acc: 0.007 - ETA: 3s - loss: 4.8873 - acc: 0.007 - ETA: 3s - loss: 4.8872 - acc: 0.007 - ETA: 3s - loss: 4.8870 - acc: 0.007 - ETA: 3s - loss: 4.8870 - acc: 0.007 - ETA: 3s - loss: 4.8868 - acc: 0.007 - ETA: 3s - loss: 4.8870 - acc: 0.007 - ETA: 3s - loss: 4.8872 - acc: 0.007 - ETA: 3s - loss: 4.8870 - acc: 0.007 - ETA: 3s - loss: 4.8871 - acc: 0.007 - ETA: 2s - loss: 4.8865 - acc: 0.007 - ETA: 2s - loss: 4.8863 - acc: 0.007 - ETA: 2s - loss: 4.8863 - acc: 0.007 - ETA: 2s - loss: 4.8865 - acc: 0.007 - ETA: 2s - loss: 4.8865 - acc: 0.008 - ETA: 2s - loss: 4.8865 - acc: 0.008 - ETA: 2s - loss: 4.8864 - acc: 0.008 - ETA: 2s - loss: 4.8862 - acc: 0.008 - ETA: 2s - loss: 4.8860 - acc: 0.008 - ETA: 2s - loss: 4.8865 - acc: 0.008 - ETA: 2s - loss: 4.8866 - acc: 0.008 - ETA: 2s - loss: 4.8868 - acc: 0.008 - ETA: 2s - loss: 4.8867 - acc: 0.008 - ETA: 1s - loss: 4.8866 - acc: 0.008 - ETA: 1s - loss: 4.8865 - acc: 0.008 - ETA: 1s - loss: 4.8863 - acc: 0.008 - ETA: 1s - loss: 4.8858 - acc: 0.008 - ETA: 1s - loss: 4.8855 - acc: 0.008 - ETA: 1s - loss: 4.8852 - acc: 0.008 - ETA: 1s - loss: 4.8854 - acc: 0.008 - ETA: 1s - loss: 4.8856 - acc: 0.008 - ETA: 1s - loss: 4.8853 - acc: 0.008 - ETA: 1s - loss: 4.8851 - acc: 0.008 - ETA: 1s - loss: 4.8851 - acc: 0.008 - ETA: 1s - loss: 4.8854 - acc: 0.008 - ETA: 1s - loss: 4.8852 - acc: 0.007 - ETA: 1s - loss: 4.8847 - acc: 0.008 - ETA: 0s - loss: 4.8845 - acc: 0.008 - ETA: 0s - loss: 4.8843 - acc: 0.008 - ETA: 0s - loss: 4.8845 - acc: 0.007 - ETA: 0s - loss: 4.8846 - acc: 0.008 - ETA: 0s - loss: 4.8846 - acc: 0.008 - ETA: 0s - loss: 4.8846 - acc: 0.008 - ETA: 0s - loss: 4.8843 - acc: 0.008 - ETA: 0s - loss: 4.8845 - acc: 0.008 - ETA: 0s - loss: 4.8843 - acc: 0.008 - ETA: 0s - loss: 4.8842 - acc: 0.008 - ETA: 0s - loss: 4.8840 - acc: 0.008 - ETA: 0s - loss: 4.8838 - acc: 0.008 - ETA: 0s - loss: 4.8837 - acc: 0.008 - ETA: 0s - loss: 4.8838 - acc: 0.008 - 8s 1ms/step - loss: 4.8834 - acc: 0.0085 - val_loss: 4.8671 - val_acc: 0.0096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.86710, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 4.8953 - acc: 0.0000e+0 - ETA: 7s - loss: 4.8830 - acc: 0.0000e+0 - ETA: 7s - loss: 4.8896 - acc: 0.0071    - ETA: 7s - loss: 4.8744 - acc: 0.005 - ETA: 6s - loss: 4.8672 - acc: 0.003 - ETA: 6s - loss: 4.8642 - acc: 0.003 - ETA: 6s - loss: 4.8646 - acc: 0.005 - ETA: 6s - loss: 4.8658 - acc: 0.004 - ETA: 6s - loss: 4.8691 - acc: 0.008 - ETA: 6s - loss: 4.8671 - acc: 0.007 - ETA: 6s - loss: 4.8702 - acc: 0.008 - ETA: 6s - loss: 4.8685 - acc: 0.008 - ETA: 6s - loss: 4.8639 - acc: 0.009 - ETA: 6s - loss: 4.8650 - acc: 0.008 - ETA: 6s - loss: 4.8630 - acc: 0.008 - ETA: 6s - loss: 4.8660 - acc: 0.007 - ETA: 6s - loss: 4.8623 - acc: 0.007 - ETA: 6s - loss: 4.8643 - acc: 0.007 - ETA: 6s - loss: 4.8625 - acc: 0.008 - ETA: 5s - loss: 4.8615 - acc: 0.007 - ETA: 5s - loss: 4.8594 - acc: 0.007 - ETA: 5s - loss: 4.8601 - acc: 0.007 - ETA: 5s - loss: 4.8610 - acc: 0.007 - ETA: 5s - loss: 4.8609 - acc: 0.007 - ETA: 5s - loss: 4.8600 - acc: 0.008 - ETA: 5s - loss: 4.8605 - acc: 0.008 - ETA: 5s - loss: 4.8597 - acc: 0.009 - ETA: 5s - loss: 4.8597 - acc: 0.010 - ETA: 5s - loss: 4.8577 - acc: 0.011 - ETA: 5s - loss: 4.8581 - acc: 0.010 - ETA: 5s - loss: 4.8575 - acc: 0.011 - ETA: 5s - loss: 4.8581 - acc: 0.010 - ETA: 5s - loss: 4.8567 - acc: 0.010 - ETA: 5s - loss: 4.8574 - acc: 0.010 - ETA: 4s - loss: 4.8580 - acc: 0.010 - ETA: 4s - loss: 4.8587 - acc: 0.010 - ETA: 4s - loss: 4.8598 - acc: 0.010 - ETA: 4s - loss: 4.8594 - acc: 0.010 - ETA: 4s - loss: 4.8599 - acc: 0.010 - ETA: 4s - loss: 4.8595 - acc: 0.011 - ETA: 4s - loss: 4.8596 - acc: 0.011 - ETA: 4s - loss: 4.8618 - acc: 0.010 - ETA: 4s - loss: 4.8620 - acc: 0.010 - ETA: 4s - loss: 4.8622 - acc: 0.010 - ETA: 4s - loss: 4.8623 - acc: 0.010 - ETA: 4s - loss: 4.8624 - acc: 0.011 - ETA: 4s - loss: 4.8626 - acc: 0.011 - ETA: 4s - loss: 4.8629 - acc: 0.010 - ETA: 4s - loss: 4.8630 - acc: 0.010 - ETA: 4s - loss: 4.8628 - acc: 0.011 - ETA: 3s - loss: 4.8622 - acc: 0.010 - ETA: 3s - loss: 4.8633 - acc: 0.011 - ETA: 3s - loss: 4.8637 - acc: 0.010 - ETA: 3s - loss: 4.8635 - acc: 0.010 - ETA: 3s - loss: 4.8639 - acc: 0.011 - ETA: 3s - loss: 4.8642 - acc: 0.010 - ETA: 3s - loss: 4.8641 - acc: 0.010 - ETA: 3s - loss: 4.8645 - acc: 0.010 - ETA: 3s - loss: 4.8651 - acc: 0.010 - ETA: 3s - loss: 4.8650 - acc: 0.010 - ETA: 3s - loss: 4.8648 - acc: 0.010 - ETA: 3s - loss: 4.8652 - acc: 0.010 - ETA: 3s - loss: 4.8656 - acc: 0.011 - ETA: 3s - loss: 4.8656 - acc: 0.010 - ETA: 3s - loss: 4.8655 - acc: 0.011 - ETA: 2s - loss: 4.8656 - acc: 0.011 - ETA: 2s - loss: 4.8654 - acc: 0.011 - ETA: 2s - loss: 4.8646 - acc: 0.011 - ETA: 2s - loss: 4.8642 - acc: 0.012 - ETA: 2s - loss: 4.8645 - acc: 0.011 - ETA: 2s - loss: 4.8642 - acc: 0.012 - ETA: 2s - loss: 4.8642 - acc: 0.012 - ETA: 2s - loss: 4.8637 - acc: 0.012 - ETA: 2s - loss: 4.8645 - acc: 0.012 - ETA: 2s - loss: 4.8651 - acc: 0.011 - ETA: 2s - loss: 4.8655 - acc: 0.011 - ETA: 2s - loss: 4.8654 - acc: 0.011 - ETA: 2s - loss: 4.8657 - acc: 0.011 - ETA: 2s - loss: 4.8653 - acc: 0.011 - ETA: 2s - loss: 4.8658 - acc: 0.011 - ETA: 2s - loss: 4.8654 - acc: 0.011 - ETA: 1s - loss: 4.8651 - acc: 0.011 - ETA: 1s - loss: 4.8653 - acc: 0.011 - ETA: 1s - loss: 4.8652 - acc: 0.011 - ETA: 1s - loss: 4.8658 - acc: 0.011 - ETA: 1s - loss: 4.8660 - acc: 0.011 - ETA: 1s - loss: 4.8660 - acc: 0.012 - ETA: 1s - loss: 4.8663 - acc: 0.012 - ETA: 1s - loss: 4.8661 - acc: 0.012 - ETA: 1s - loss: 4.8660 - acc: 0.012 - ETA: 1s - loss: 4.8662 - acc: 0.012 - ETA: 1s - loss: 4.8663 - acc: 0.012 - ETA: 1s - loss: 4.8661 - acc: 0.012 - ETA: 1s - loss: 4.8661 - acc: 0.012 - ETA: 1s - loss: 4.8662 - acc: 0.012 - ETA: 1s - loss: 4.8663 - acc: 0.012 - ETA: 0s - loss: 4.8665 - acc: 0.012 - ETA: 0s - loss: 4.8662 - acc: 0.012 - ETA: 0s - loss: 4.8661 - acc: 0.012 - ETA: 0s - loss: 4.8660 - acc: 0.012 - ETA: 0s - loss: 4.8657 - acc: 0.012 - ETA: 0s - loss: 4.8666 - acc: 0.012 - ETA: 0s - loss: 4.8668 - acc: 0.012 - ETA: 0s - loss: 4.8667 - acc: 0.012 - ETA: 0s - loss: 4.8667 - acc: 0.012 - ETA: 0s - loss: 4.8666 - acc: 0.012 - ETA: 0s - loss: 4.8663 - acc: 0.011 - ETA: 0s - loss: 4.8659 - acc: 0.012 - ETA: 0s - loss: 4.8662 - acc: 0.012 - ETA: 0s - loss: 4.8668 - acc: 0.012 - ETA: 0s - loss: 4.8668 - acc: 0.012 - 8s 1ms/step - loss: 4.8668 - acc: 0.0121 - val_loss: 4.8605 - val_acc: 0.0096\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.86710 to 4.86045, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 4.7782 - acc: 0.0000e+0 - ETA: 7s - loss: 4.8211 - acc: 0.0000e+0 - ETA: 7s - loss: 4.8370 - acc: 0.0000e+0 - ETA: 7s - loss: 4.8269 - acc: 0.0050    - ETA: 6s - loss: 4.8333 - acc: 0.003 - ETA: 6s - loss: 4.8245 - acc: 0.009 - ETA: 6s - loss: 4.8239 - acc: 0.007 - ETA: 6s - loss: 4.8251 - acc: 0.006 - ETA: 6s - loss: 4.8270 - acc: 0.008 - ETA: 6s - loss: 4.8252 - acc: 0.007 - ETA: 6s - loss: 4.8322 - acc: 0.012 - ETA: 6s - loss: 4.8361 - acc: 0.011 - ETA: 6s - loss: 4.8364 - acc: 0.013 - ETA: 6s - loss: 4.8385 - acc: 0.012 - ETA: 6s - loss: 4.8384 - acc: 0.012 - ETA: 6s - loss: 4.8377 - acc: 0.013 - ETA: 6s - loss: 4.8421 - acc: 0.013 - ETA: 6s - loss: 4.8394 - acc: 0.012 - ETA: 6s - loss: 4.8396 - acc: 0.012 - ETA: 6s - loss: 4.8374 - acc: 0.012 - ETA: 5s - loss: 4.8340 - acc: 0.012 - ETA: 5s - loss: 4.8362 - acc: 0.013 - ETA: 5s - loss: 4.8351 - acc: 0.014 - ETA: 5s - loss: 4.8336 - acc: 0.013 - ETA: 5s - loss: 4.8341 - acc: 0.013 - ETA: 5s - loss: 4.8335 - acc: 0.013 - ETA: 5s - loss: 4.8341 - acc: 0.013 - ETA: 5s - loss: 4.8361 - acc: 0.013 - ETA: 5s - loss: 4.8341 - acc: 0.013 - ETA: 5s - loss: 4.8341 - acc: 0.013 - ETA: 5s - loss: 4.8348 - acc: 0.013 - ETA: 5s - loss: 4.8365 - acc: 0.013 - ETA: 5s - loss: 4.8380 - acc: 0.014 - ETA: 5s - loss: 4.8382 - acc: 0.014 - ETA: 5s - loss: 4.8397 - acc: 0.013 - ETA: 4s - loss: 4.8392 - acc: 0.014 - ETA: 4s - loss: 4.8397 - acc: 0.014 - ETA: 4s - loss: 4.8403 - acc: 0.013 - ETA: 4s - loss: 4.8416 - acc: 0.013 - ETA: 4s - loss: 4.8418 - acc: 0.014 - ETA: 4s - loss: 4.8410 - acc: 0.014 - ETA: 4s - loss: 4.8415 - acc: 0.014 - ETA: 4s - loss: 4.8427 - acc: 0.015 - ETA: 4s - loss: 4.8430 - acc: 0.015 - ETA: 4s - loss: 4.8424 - acc: 0.015 - ETA: 4s - loss: 4.8410 - acc: 0.015 - ETA: 4s - loss: 4.8402 - acc: 0.014 - ETA: 4s - loss: 4.8393 - acc: 0.015 - ETA: 4s - loss: 4.8383 - acc: 0.014 - ETA: 4s - loss: 4.8389 - acc: 0.014 - ETA: 3s - loss: 4.8394 - acc: 0.014 - ETA: 3s - loss: 4.8385 - acc: 0.014 - ETA: 3s - loss: 4.8385 - acc: 0.014 - ETA: 3s - loss: 4.8373 - acc: 0.014 - ETA: 3s - loss: 4.8372 - acc: 0.014 - ETA: 3s - loss: 4.8368 - acc: 0.014 - ETA: 3s - loss: 4.8372 - acc: 0.013 - ETA: 3s - loss: 4.8371 - acc: 0.014 - ETA: 3s - loss: 4.8366 - acc: 0.013 - ETA: 3s - loss: 4.8368 - acc: 0.013 - ETA: 3s - loss: 4.8363 - acc: 0.013 - ETA: 3s - loss: 4.8359 - acc: 0.013 - ETA: 3s - loss: 4.8369 - acc: 0.013 - ETA: 3s - loss: 4.8369 - acc: 0.013 - ETA: 3s - loss: 4.8361 - acc: 0.014 - ETA: 3s - loss: 4.8365 - acc: 0.014 - ETA: 2s - loss: 4.8352 - acc: 0.014 - ETA: 2s - loss: 4.8351 - acc: 0.014 - ETA: 2s - loss: 4.8346 - acc: 0.013 - ETA: 2s - loss: 4.8344 - acc: 0.014 - ETA: 2s - loss: 4.8347 - acc: 0.014 - ETA: 2s - loss: 4.8350 - acc: 0.014 - ETA: 2s - loss: 4.8354 - acc: 0.014 - ETA: 2s - loss: 4.8353 - acc: 0.014 - ETA: 2s - loss: 4.8347 - acc: 0.014 - ETA: 2s - loss: 4.8352 - acc: 0.014 - ETA: 2s - loss: 4.8349 - acc: 0.014 - ETA: 2s - loss: 4.8345 - acc: 0.014 - ETA: 2s - loss: 4.8340 - acc: 0.014 - ETA: 2s - loss: 4.8338 - acc: 0.014 - ETA: 2s - loss: 4.8333 - acc: 0.014 - ETA: 1s - loss: 4.8347 - acc: 0.014 - ETA: 1s - loss: 4.8347 - acc: 0.014 - ETA: 1s - loss: 4.8338 - acc: 0.014 - ETA: 1s - loss: 4.8331 - acc: 0.014 - ETA: 1s - loss: 4.8339 - acc: 0.015 - ETA: 1s - loss: 4.8346 - acc: 0.015 - ETA: 1s - loss: 4.8353 - acc: 0.015 - ETA: 1s - loss: 4.8364 - acc: 0.015 - ETA: 1s - loss: 4.8368 - acc: 0.015 - ETA: 1s - loss: 4.8379 - acc: 0.014 - ETA: 1s - loss: 4.8375 - acc: 0.014 - ETA: 1s - loss: 4.8379 - acc: 0.014 - ETA: 1s - loss: 4.8376 - acc: 0.015 - ETA: 1s - loss: 4.8379 - acc: 0.015 - ETA: 1s - loss: 4.8384 - acc: 0.015 - ETA: 0s - loss: 4.8386 - acc: 0.015 - ETA: 0s - loss: 4.8379 - acc: 0.015 - ETA: 0s - loss: 4.8376 - acc: 0.015 - ETA: 0s - loss: 4.8374 - acc: 0.015 - ETA: 0s - loss: 4.8368 - acc: 0.015 - ETA: 0s - loss: 4.8373 - acc: 0.014 - ETA: 0s - loss: 4.8364 - acc: 0.015 - ETA: 0s - loss: 4.8366 - acc: 0.015 - ETA: 0s - loss: 4.8363 - acc: 0.014 - ETA: 0s - loss: 4.8361 - acc: 0.014 - ETA: 0s - loss: 4.8367 - acc: 0.014 - ETA: 0s - loss: 4.8367 - acc: 0.014 - ETA: 0s - loss: 4.8371 - acc: 0.014 - ETA: 0s - loss: 4.8369 - acc: 0.014 - ETA: 0s - loss: 4.8364 - acc: 0.014 - 8s 1ms/step - loss: 4.8360 - acc: 0.0147 - val_loss: 4.8209 - val_acc: 0.0156\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.86045 to 4.82093, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 4.7659 - acc: 0.050 - ETA: 7s - loss: 4.8308 - acc: 0.012 - ETA: 7s - loss: 4.7938 - acc: 0.007 - ETA: 7s - loss: 4.7699 - acc: 0.005 - ETA: 6s - loss: 4.7688 - acc: 0.007 - ETA: 6s - loss: 4.7865 - acc: 0.009 - ETA: 6s - loss: 4.7925 - acc: 0.013 - ETA: 6s - loss: 4.7977 - acc: 0.011 - ETA: 6s - loss: 4.7919 - acc: 0.012 - ETA: 6s - loss: 4.7888 - acc: 0.012 - ETA: 6s - loss: 4.7958 - acc: 0.014 - ETA: 6s - loss: 4.7997 - acc: 0.014 - ETA: 6s - loss: 4.8015 - acc: 0.013 - ETA: 6s - loss: 4.7978 - acc: 0.012 - ETA: 6s - loss: 4.7980 - acc: 0.015 - ETA: 6s - loss: 4.7964 - acc: 0.014 - ETA: 6s - loss: 4.7928 - acc: 0.015 - ETA: 6s - loss: 4.7925 - acc: 0.014 - ETA: 6s - loss: 4.7916 - acc: 0.015 - ETA: 5s - loss: 4.7931 - acc: 0.015 - ETA: 5s - loss: 4.7943 - acc: 0.016 - ETA: 5s - loss: 4.7942 - acc: 0.015 - ETA: 5s - loss: 4.7944 - acc: 0.014 - ETA: 5s - loss: 4.7948 - acc: 0.014 - ETA: 5s - loss: 4.7958 - acc: 0.014 - ETA: 5s - loss: 4.7953 - acc: 0.013 - ETA: 5s - loss: 4.7980 - acc: 0.013 - ETA: 5s - loss: 4.7990 - acc: 0.014 - ETA: 5s - loss: 4.8005 - acc: 0.014 - ETA: 5s - loss: 4.7982 - acc: 0.014 - ETA: 5s - loss: 4.7985 - acc: 0.014 - ETA: 5s - loss: 4.7986 - acc: 0.013 - ETA: 5s - loss: 4.7987 - acc: 0.013 - ETA: 5s - loss: 4.7985 - acc: 0.013 - ETA: 5s - loss: 4.7983 - acc: 0.014 - ETA: 4s - loss: 4.7994 - acc: 0.014 - ETA: 4s - loss: 4.7985 - acc: 0.014 - ETA: 4s - loss: 4.7984 - acc: 0.015 - ETA: 4s - loss: 4.7985 - acc: 0.015 - ETA: 4s - loss: 4.7959 - acc: 0.015 - ETA: 4s - loss: 4.7945 - acc: 0.015 - ETA: 4s - loss: 4.7949 - acc: 0.015 - ETA: 4s - loss: 4.7943 - acc: 0.015 - ETA: 4s - loss: 4.7949 - acc: 0.016 - ETA: 4s - loss: 4.7956 - acc: 0.016 - ETA: 4s - loss: 4.7975 - acc: 0.016 - ETA: 4s - loss: 4.7975 - acc: 0.015 - ETA: 4s - loss: 4.7978 - acc: 0.015 - ETA: 4s - loss: 4.7966 - acc: 0.015 - ETA: 4s - loss: 4.7958 - acc: 0.015 - ETA: 3s - loss: 4.7937 - acc: 0.016 - ETA: 3s - loss: 4.7949 - acc: 0.016 - ETA: 3s - loss: 4.7960 - acc: 0.016 - ETA: 3s - loss: 4.7946 - acc: 0.015 - ETA: 3s - loss: 4.7958 - acc: 0.015 - ETA: 3s - loss: 4.7962 - acc: 0.015 - ETA: 3s - loss: 4.7965 - acc: 0.015 - ETA: 3s - loss: 4.7950 - acc: 0.015 - ETA: 3s - loss: 4.7940 - acc: 0.016 - ETA: 3s - loss: 4.7947 - acc: 0.016 - ETA: 3s - loss: 4.7944 - acc: 0.016 - ETA: 3s - loss: 4.7936 - acc: 0.016 - ETA: 3s - loss: 4.7944 - acc: 0.016 - ETA: 3s - loss: 4.7954 - acc: 0.016 - ETA: 3s - loss: 4.7959 - acc: 0.015 - ETA: 2s - loss: 4.7969 - acc: 0.015 - ETA: 2s - loss: 4.7978 - acc: 0.015 - ETA: 2s - loss: 4.7984 - acc: 0.015 - ETA: 2s - loss: 4.7987 - acc: 0.015 - ETA: 2s - loss: 4.7986 - acc: 0.015 - ETA: 2s - loss: 4.7988 - acc: 0.015 - ETA: 2s - loss: 4.7994 - acc: 0.015 - ETA: 2s - loss: 4.7990 - acc: 0.015 - ETA: 2s - loss: 4.7994 - acc: 0.015 - ETA: 2s - loss: 4.7999 - acc: 0.015 - ETA: 2s - loss: 4.7994 - acc: 0.015 - ETA: 2s - loss: 4.7989 - acc: 0.016 - ETA: 2s - loss: 4.7975 - acc: 0.015 - ETA: 2s - loss: 4.7964 - acc: 0.016 - ETA: 2s - loss: 4.7962 - acc: 0.016 - ETA: 2s - loss: 4.7959 - acc: 0.016 - ETA: 1s - loss: 4.7961 - acc: 0.016 - ETA: 1s - loss: 4.7963 - acc: 0.016 - ETA: 1s - loss: 4.7957 - acc: 0.016 - ETA: 1s - loss: 4.7957 - acc: 0.016 - ETA: 1s - loss: 4.7956 - acc: 0.016 - ETA: 1s - loss: 4.7958 - acc: 0.016 - ETA: 1s - loss: 4.7965 - acc: 0.016 - ETA: 1s - loss: 4.7959 - acc: 0.016 - ETA: 1s - loss: 4.7956 - acc: 0.016 - ETA: 1s - loss: 4.7950 - acc: 0.016 - ETA: 1s - loss: 4.7948 - acc: 0.016 - ETA: 1s - loss: 4.7951 - acc: 0.016 - ETA: 1s - loss: 4.7946 - acc: 0.016 - ETA: 1s - loss: 4.7938 - acc: 0.016 - ETA: 1s - loss: 4.7942 - acc: 0.016 - ETA: 0s - loss: 4.7948 - acc: 0.016 - ETA: 0s - loss: 4.7946 - acc: 0.016 - ETA: 0s - loss: 4.7944 - acc: 0.016 - ETA: 0s - loss: 4.7954 - acc: 0.016 - ETA: 0s - loss: 4.7941 - acc: 0.016 - ETA: 0s - loss: 4.7935 - acc: 0.016 - ETA: 0s - loss: 4.7931 - acc: 0.016 - ETA: 0s - loss: 4.7928 - acc: 0.016 - ETA: 0s - loss: 4.7925 - acc: 0.016 - ETA: 0s - loss: 4.7925 - acc: 0.016 - ETA: 0s - loss: 4.7929 - acc: 0.016 - ETA: 0s - loss: 4.7928 - acc: 0.016 - ETA: 0s - loss: 4.7927 - acc: 0.016 - ETA: 0s - loss: 4.7925 - acc: 0.016 - ETA: 0s - loss: 4.7922 - acc: 0.016 - 8s 1ms/step - loss: 4.7922 - acc: 0.0165 - val_loss: 4.7970 - val_acc: 0.0168\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.82093 to 4.79696, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 4.6857 - acc: 0.050 - ETA: 6s - loss: 4.7545 - acc: 0.050 - ETA: 7s - loss: 4.7816 - acc: 0.028 - ETA: 7s - loss: 4.7665 - acc: 0.025 - ETA: 6s - loss: 4.7794 - acc: 0.023 - ETA: 6s - loss: 4.7656 - acc: 0.018 - ETA: 6s - loss: 4.7784 - acc: 0.023 - ETA: 6s - loss: 4.7705 - acc: 0.022 - ETA: 6s - loss: 4.7695 - acc: 0.022 - ETA: 6s - loss: 4.7650 - acc: 0.021 - ETA: 6s - loss: 4.7600 - acc: 0.022 - ETA: 6s - loss: 4.7619 - acc: 0.022 - ETA: 6s - loss: 4.7600 - acc: 0.023 - ETA: 6s - loss: 4.7591 - acc: 0.021 - ETA: 6s - loss: 4.7635 - acc: 0.020 - ETA: 6s - loss: 4.7607 - acc: 0.020 - ETA: 6s - loss: 4.7659 - acc: 0.023 - ETA: 6s - loss: 4.7717 - acc: 0.022 - ETA: 6s - loss: 4.7709 - acc: 0.020 - ETA: 5s - loss: 4.7702 - acc: 0.019 - ETA: 5s - loss: 4.7655 - acc: 0.021 - ETA: 5s - loss: 4.7653 - acc: 0.021 - ETA: 5s - loss: 4.7647 - acc: 0.020 - ETA: 5s - loss: 4.7653 - acc: 0.020 - ETA: 5s - loss: 4.7701 - acc: 0.019 - ETA: 5s - loss: 4.7726 - acc: 0.019 - ETA: 5s - loss: 4.7738 - acc: 0.019 - ETA: 5s - loss: 4.7722 - acc: 0.019 - ETA: 5s - loss: 4.7741 - acc: 0.018 - ETA: 5s - loss: 4.7744 - acc: 0.018 - ETA: 5s - loss: 4.7727 - acc: 0.017 - ETA: 5s - loss: 4.7726 - acc: 0.017 - ETA: 5s - loss: 4.7735 - acc: 0.017 - ETA: 5s - loss: 4.7742 - acc: 0.017 - ETA: 5s - loss: 4.7747 - acc: 0.017 - ETA: 4s - loss: 4.7763 - acc: 0.016 - ETA: 4s - loss: 4.7763 - acc: 0.016 - ETA: 4s - loss: 4.7753 - acc: 0.017 - ETA: 4s - loss: 4.7742 - acc: 0.017 - ETA: 4s - loss: 4.7715 - acc: 0.017 - ETA: 4s - loss: 4.7698 - acc: 0.018 - ETA: 4s - loss: 4.7714 - acc: 0.017 - ETA: 4s - loss: 4.7716 - acc: 0.017 - ETA: 4s - loss: 4.7715 - acc: 0.017 - ETA: 4s - loss: 4.7711 - acc: 0.017 - ETA: 4s - loss: 4.7697 - acc: 0.016 - ETA: 4s - loss: 4.7696 - acc: 0.017 - ETA: 4s - loss: 4.7687 - acc: 0.016 - ETA: 4s - loss: 4.7667 - acc: 0.017 - ETA: 4s - loss: 4.7648 - acc: 0.017 - ETA: 3s - loss: 4.7668 - acc: 0.017 - ETA: 3s - loss: 4.7678 - acc: 0.017 - ETA: 3s - loss: 4.7653 - acc: 0.017 - ETA: 3s - loss: 4.7651 - acc: 0.017 - ETA: 3s - loss: 4.7673 - acc: 0.017 - ETA: 3s - loss: 4.7671 - acc: 0.017 - ETA: 3s - loss: 4.7657 - acc: 0.017 - ETA: 3s - loss: 4.7662 - acc: 0.018 - ETA: 3s - loss: 4.7672 - acc: 0.018 - ETA: 3s - loss: 4.7667 - acc: 0.018 - ETA: 3s - loss: 4.7672 - acc: 0.018 - ETA: 3s - loss: 4.7675 - acc: 0.018 - ETA: 3s - loss: 4.7689 - acc: 0.018 - ETA: 3s - loss: 4.7679 - acc: 0.018 - ETA: 3s - loss: 4.7659 - acc: 0.018 - ETA: 3s - loss: 4.7661 - acc: 0.019 - ETA: 2s - loss: 4.7663 - acc: 0.018 - ETA: 2s - loss: 4.7647 - acc: 0.019 - ETA: 2s - loss: 4.7644 - acc: 0.019 - ETA: 2s - loss: 4.7638 - acc: 0.019 - ETA: 2s - loss: 4.7646 - acc: 0.019 - ETA: 2s - loss: 4.7629 - acc: 0.019 - ETA: 2s - loss: 4.7643 - acc: 0.019 - ETA: 2s - loss: 4.7644 - acc: 0.019 - ETA: 2s - loss: 4.7648 - acc: 0.019 - ETA: 2s - loss: 4.7643 - acc: 0.020 - ETA: 2s - loss: 4.7637 - acc: 0.020 - ETA: 2s - loss: 4.7637 - acc: 0.020 - ETA: 2s - loss: 4.7628 - acc: 0.021 - ETA: 2s - loss: 4.7623 - acc: 0.021 - ETA: 2s - loss: 4.7617 - acc: 0.021 - ETA: 1s - loss: 4.7617 - acc: 0.021 - ETA: 1s - loss: 4.7628 - acc: 0.021 - ETA: 1s - loss: 4.7633 - acc: 0.020 - ETA: 1s - loss: 4.7634 - acc: 0.020 - ETA: 1s - loss: 4.7645 - acc: 0.020 - ETA: 1s - loss: 4.7638 - acc: 0.020 - ETA: 1s - loss: 4.7630 - acc: 0.020 - ETA: 1s - loss: 4.7627 - acc: 0.020 - ETA: 1s - loss: 4.7629 - acc: 0.020 - ETA: 1s - loss: 4.7624 - acc: 0.020 - ETA: 1s - loss: 4.7617 - acc: 0.020 - ETA: 1s - loss: 4.7615 - acc: 0.020 - ETA: 1s - loss: 4.7611 - acc: 0.020 - ETA: 1s - loss: 4.7604 - acc: 0.020 - ETA: 1s - loss: 4.7609 - acc: 0.019 - ETA: 0s - loss: 4.7603 - acc: 0.019 - ETA: 0s - loss: 4.7606 - acc: 0.019 - ETA: 0s - loss: 4.7608 - acc: 0.019 - ETA: 0s - loss: 4.7620 - acc: 0.019 - ETA: 0s - loss: 4.7610 - acc: 0.019 - ETA: 0s - loss: 4.7600 - acc: 0.020 - ETA: 0s - loss: 4.7592 - acc: 0.020 - ETA: 0s - loss: 4.7583 - acc: 0.020 - ETA: 0s - loss: 4.7584 - acc: 0.020 - ETA: 0s - loss: 4.7582 - acc: 0.020 - ETA: 0s - loss: 4.7586 - acc: 0.020 - ETA: 0s - loss: 4.7587 - acc: 0.019 - ETA: 0s - loss: 4.7582 - acc: 0.020 - ETA: 0s - loss: 4.7577 - acc: 0.019 - ETA: 0s - loss: 4.7583 - acc: 0.019 - ETA: 0s - loss: 4.7580 - acc: 0.019 - 8s 1ms/step - loss: 4.7580 - acc: 0.0196 - val_loss: 4.7793 - val_acc: 0.0180\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.79696 to 4.77932, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 4.6969 - acc: 0.050 - ETA: 7s - loss: 4.7806 - acc: 0.012 - ETA: 7s - loss: 4.7860 - acc: 0.014 - ETA: 7s - loss: 4.7630 - acc: 0.015 - ETA: 6s - loss: 4.7522 - acc: 0.019 - ETA: 6s - loss: 4.7683 - acc: 0.021 - ETA: 6s - loss: 4.7757 - acc: 0.018 - ETA: 6s - loss: 4.7580 - acc: 0.018 - ETA: 6s - loss: 4.7610 - acc: 0.020 - ETA: 6s - loss: 4.7561 - acc: 0.017 - ETA: 6s - loss: 4.7471 - acc: 0.021 - ETA: 6s - loss: 4.7401 - acc: 0.020 - ETA: 6s - loss: 4.7377 - acc: 0.018 - ETA: 6s - loss: 4.7336 - acc: 0.018 - ETA: 6s - loss: 4.7426 - acc: 0.017 - ETA: 6s - loss: 4.7454 - acc: 0.017 - ETA: 6s - loss: 4.7440 - acc: 0.020 - ETA: 6s - loss: 4.7396 - acc: 0.022 - ETA: 6s - loss: 4.7347 - acc: 0.023 - ETA: 6s - loss: 4.7372 - acc: 0.022 - ETA: 5s - loss: 4.7332 - acc: 0.022 - ETA: 5s - loss: 4.7325 - acc: 0.022 - ETA: 5s - loss: 4.7353 - acc: 0.023 - ETA: 5s - loss: 4.7396 - acc: 0.025 - ETA: 5s - loss: 4.7395 - acc: 0.024 - ETA: 5s - loss: 4.7437 - acc: 0.023 - ETA: 5s - loss: 4.7448 - acc: 0.023 - ETA: 5s - loss: 4.7426 - acc: 0.025 - ETA: 5s - loss: 4.7406 - acc: 0.025 - ETA: 5s - loss: 4.7415 - acc: 0.026 - ETA: 5s - loss: 4.7432 - acc: 0.025 - ETA: 5s - loss: 4.7430 - acc: 0.025 - ETA: 5s - loss: 4.7415 - acc: 0.024 - ETA: 5s - loss: 4.7397 - acc: 0.026 - ETA: 5s - loss: 4.7383 - acc: 0.025 - ETA: 4s - loss: 4.7378 - acc: 0.025 - ETA: 4s - loss: 4.7367 - acc: 0.025 - ETA: 4s - loss: 4.7388 - acc: 0.025 - ETA: 4s - loss: 4.7367 - acc: 0.024 - ETA: 4s - loss: 4.7361 - acc: 0.025 - ETA: 4s - loss: 4.7354 - acc: 0.024 - ETA: 4s - loss: 4.7334 - acc: 0.025 - ETA: 4s - loss: 4.7337 - acc: 0.024 - ETA: 4s - loss: 4.7316 - acc: 0.024 - ETA: 4s - loss: 4.7313 - acc: 0.024 - ETA: 4s - loss: 4.7307 - acc: 0.025 - ETA: 4s - loss: 4.7320 - acc: 0.024 - ETA: 4s - loss: 4.7339 - acc: 0.024 - ETA: 4s - loss: 4.7359 - acc: 0.024 - ETA: 4s - loss: 4.7342 - acc: 0.023 - ETA: 3s - loss: 4.7351 - acc: 0.023 - ETA: 3s - loss: 4.7368 - acc: 0.024 - ETA: 3s - loss: 4.7370 - acc: 0.024 - ETA: 3s - loss: 4.7366 - acc: 0.024 - ETA: 3s - loss: 4.7368 - acc: 0.024 - ETA: 3s - loss: 4.7365 - acc: 0.024 - ETA: 3s - loss: 4.7364 - acc: 0.024 - ETA: 3s - loss: 4.7355 - acc: 0.024 - ETA: 3s - loss: 4.7358 - acc: 0.024 - ETA: 3s - loss: 4.7331 - acc: 0.025 - ETA: 3s - loss: 4.7342 - acc: 0.024 - ETA: 3s - loss: 4.7356 - acc: 0.024 - ETA: 3s - loss: 4.7357 - acc: 0.024 - ETA: 3s - loss: 4.7368 - acc: 0.023 - ETA: 3s - loss: 4.7361 - acc: 0.023 - ETA: 3s - loss: 4.7361 - acc: 0.024 - ETA: 2s - loss: 4.7345 - acc: 0.024 - ETA: 2s - loss: 4.7347 - acc: 0.024 - ETA: 2s - loss: 4.7357 - acc: 0.023 - ETA: 2s - loss: 4.7344 - acc: 0.023 - ETA: 2s - loss: 4.7339 - acc: 0.023 - ETA: 2s - loss: 4.7333 - acc: 0.023 - ETA: 2s - loss: 4.7332 - acc: 0.023 - ETA: 2s - loss: 4.7338 - acc: 0.023 - ETA: 2s - loss: 4.7336 - acc: 0.023 - ETA: 2s - loss: 4.7327 - acc: 0.023 - ETA: 2s - loss: 4.7322 - acc: 0.023 - ETA: 2s - loss: 4.7318 - acc: 0.023 - ETA: 2s - loss: 4.7316 - acc: 0.022 - ETA: 2s - loss: 4.7312 - acc: 0.022 - ETA: 2s - loss: 4.7300 - acc: 0.023 - ETA: 1s - loss: 4.7297 - acc: 0.023 - ETA: 1s - loss: 4.7307 - acc: 0.023 - ETA: 1s - loss: 4.7304 - acc: 0.023 - ETA: 1s - loss: 4.7299 - acc: 0.022 - ETA: 1s - loss: 4.7307 - acc: 0.022 - ETA: 1s - loss: 4.7308 - acc: 0.023 - ETA: 1s - loss: 4.7295 - acc: 0.023 - ETA: 1s - loss: 4.7308 - acc: 0.023 - ETA: 1s - loss: 4.7305 - acc: 0.023 - ETA: 1s - loss: 4.7315 - acc: 0.023 - ETA: 1s - loss: 4.7321 - acc: 0.023 - ETA: 1s - loss: 4.7321 - acc: 0.022 - ETA: 1s - loss: 4.7330 - acc: 0.022 - ETA: 1s - loss: 4.7327 - acc: 0.022 - ETA: 1s - loss: 4.7337 - acc: 0.022 - ETA: 0s - loss: 4.7340 - acc: 0.022 - ETA: 0s - loss: 4.7335 - acc: 0.021 - ETA: 0s - loss: 4.7334 - acc: 0.021 - ETA: 0s - loss: 4.7336 - acc: 0.021 - ETA: 0s - loss: 4.7356 - acc: 0.021 - ETA: 0s - loss: 4.7343 - acc: 0.022 - ETA: 0s - loss: 4.7347 - acc: 0.022 - ETA: 0s - loss: 4.7345 - acc: 0.022 - ETA: 0s - loss: 4.7350 - acc: 0.022 - ETA: 0s - loss: 4.7347 - acc: 0.022 - ETA: 0s - loss: 4.7365 - acc: 0.022 - ETA: 0s - loss: 4.7367 - acc: 0.022 - ETA: 0s - loss: 4.7367 - acc: 0.022 - ETA: 0s - loss: 4.7363 - acc: 0.022 - ETA: 0s - loss: 4.7356 - acc: 0.022 - 8s 1ms/step - loss: 4.7360 - acc: 0.0222 - val_loss: 4.7620 - val_acc: 0.0204\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.77932 to 4.76200, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 4.6828 - acc: 0.0000e+0 - ETA: 8s - loss: 4.7478 - acc: 0.0167    - ETA: 7s - loss: 4.7110 - acc: 0.025 - ETA: 7s - loss: 4.6925 - acc: 0.027 - ETA: 7s - loss: 4.7103 - acc: 0.029 - ETA: 7s - loss: 4.7085 - acc: 0.030 - ETA: 7s - loss: 4.7047 - acc: 0.030 - ETA: 7s - loss: 4.7161 - acc: 0.033 - ETA: 6s - loss: 4.7343 - acc: 0.033 - ETA: 6s - loss: 4.7322 - acc: 0.029 - ETA: 6s - loss: 4.7380 - acc: 0.026 - ETA: 6s - loss: 4.7327 - acc: 0.027 - ETA: 6s - loss: 4.7355 - acc: 0.027 - ETA: 6s - loss: 4.7308 - acc: 0.025 - ETA: 6s - loss: 4.7271 - acc: 0.025 - ETA: 6s - loss: 4.7103 - acc: 0.025 - ETA: 6s - loss: 4.7100 - acc: 0.026 - ETA: 6s - loss: 4.7028 - acc: 0.026 - ETA: 6s - loss: 4.6946 - acc: 0.025 - ETA: 6s - loss: 4.6949 - acc: 0.025 - ETA: 6s - loss: 4.6992 - acc: 0.025 - ETA: 6s - loss: 4.6981 - acc: 0.025 - ETA: 5s - loss: 4.6990 - acc: 0.025 - ETA: 5s - loss: 4.6945 - acc: 0.024 - ETA: 5s - loss: 4.6935 - acc: 0.025 - ETA: 5s - loss: 4.6960 - acc: 0.025 - ETA: 5s - loss: 4.6944 - acc: 0.026 - ETA: 5s - loss: 4.6949 - acc: 0.027 - ETA: 5s - loss: 4.6955 - acc: 0.026 - ETA: 5s - loss: 4.6920 - acc: 0.027 - ETA: 5s - loss: 4.6925 - acc: 0.027 - ETA: 5s - loss: 4.6927 - acc: 0.028 - ETA: 5s - loss: 4.6927 - acc: 0.029 - ETA: 5s - loss: 4.6940 - acc: 0.028 - ETA: 5s - loss: 4.6969 - acc: 0.028 - ETA: 5s - loss: 4.6977 - acc: 0.028 - ETA: 4s - loss: 4.6983 - acc: 0.027 - ETA: 4s - loss: 4.6977 - acc: 0.027 - ETA: 4s - loss: 4.6965 - acc: 0.026 - ETA: 4s - loss: 4.6991 - acc: 0.026 - ETA: 4s - loss: 4.7000 - acc: 0.026 - ETA: 4s - loss: 4.6985 - acc: 0.027 - ETA: 4s - loss: 4.7017 - acc: 0.027 - ETA: 4s - loss: 4.7006 - acc: 0.026 - ETA: 4s - loss: 4.7024 - acc: 0.026 - ETA: 4s - loss: 4.7033 - acc: 0.026 - ETA: 4s - loss: 4.7048 - acc: 0.025 - ETA: 4s - loss: 4.7055 - acc: 0.025 - ETA: 4s - loss: 4.7046 - acc: 0.024 - ETA: 4s - loss: 4.7064 - acc: 0.024 - ETA: 4s - loss: 4.7081 - acc: 0.024 - ETA: 3s - loss: 4.7069 - acc: 0.024 - ETA: 3s - loss: 4.7082 - acc: 0.024 - ETA: 3s - loss: 4.7099 - acc: 0.024 - ETA: 3s - loss: 4.7095 - acc: 0.025 - ETA: 3s - loss: 4.7104 - acc: 0.025 - ETA: 3s - loss: 4.7100 - acc: 0.025 - ETA: 3s - loss: 4.7075 - acc: 0.026 - ETA: 3s - loss: 4.7088 - acc: 0.026 - ETA: 3s - loss: 4.7103 - acc: 0.025 - ETA: 3s - loss: 4.7103 - acc: 0.025 - ETA: 3s - loss: 4.7111 - acc: 0.026 - ETA: 3s - loss: 4.7107 - acc: 0.026 - ETA: 3s - loss: 4.7109 - acc: 0.025 - ETA: 3s - loss: 4.7114 - acc: 0.025 - ETA: 3s - loss: 4.7123 - acc: 0.025 - ETA: 3s - loss: 4.7135 - acc: 0.026 - ETA: 2s - loss: 4.7138 - acc: 0.026 - ETA: 2s - loss: 4.7130 - acc: 0.026 - ETA: 2s - loss: 4.7149 - acc: 0.025 - ETA: 2s - loss: 4.7135 - acc: 0.026 - ETA: 2s - loss: 4.7131 - acc: 0.026 - ETA: 2s - loss: 4.7143 - acc: 0.025 - ETA: 2s - loss: 4.7138 - acc: 0.025 - ETA: 2s - loss: 4.7135 - acc: 0.025 - ETA: 2s - loss: 4.7139 - acc: 0.025 - ETA: 2s - loss: 4.7141 - acc: 0.025 - ETA: 2s - loss: 4.7132 - acc: 0.025 - ETA: 2s - loss: 4.7138 - acc: 0.025 - ETA: 2s - loss: 4.7150 - acc: 0.025 - ETA: 2s - loss: 4.7147 - acc: 0.025 - ETA: 2s - loss: 4.7127 - acc: 0.025 - ETA: 1s - loss: 4.7114 - acc: 0.025 - ETA: 1s - loss: 4.7107 - acc: 0.025 - ETA: 1s - loss: 4.7121 - acc: 0.025 - ETA: 1s - loss: 4.7121 - acc: 0.024 - ETA: 1s - loss: 4.7118 - acc: 0.024 - ETA: 1s - loss: 4.7123 - acc: 0.024 - ETA: 1s - loss: 4.7122 - acc: 0.023 - ETA: 1s - loss: 4.7133 - acc: 0.023 - ETA: 1s - loss: 4.7132 - acc: 0.023 - ETA: 1s - loss: 4.7134 - acc: 0.024 - ETA: 1s - loss: 4.7124 - acc: 0.024 - ETA: 1s - loss: 4.7123 - acc: 0.024 - ETA: 1s - loss: 4.7113 - acc: 0.024 - ETA: 1s - loss: 4.7110 - acc: 0.024 - ETA: 1s - loss: 4.7116 - acc: 0.024 - ETA: 0s - loss: 4.7113 - acc: 0.024 - ETA: 0s - loss: 4.7122 - acc: 0.024 - ETA: 0s - loss: 4.7116 - acc: 0.024 - ETA: 0s - loss: 4.7117 - acc: 0.024 - ETA: 0s - loss: 4.7120 - acc: 0.024 - ETA: 0s - loss: 4.7116 - acc: 0.024 - ETA: 0s - loss: 4.7112 - acc: 0.024 - ETA: 0s - loss: 4.7120 - acc: 0.024 - ETA: 0s - loss: 4.7112 - acc: 0.024 - ETA: 0s - loss: 4.7105 - acc: 0.024 - ETA: 0s - loss: 4.7123 - acc: 0.024 - ETA: 0s - loss: 4.7127 - acc: 0.024 - ETA: 0s - loss: 4.7128 - acc: 0.023 - ETA: 0s - loss: 4.7127 - acc: 0.024 - ETA: 0s - loss: 4.7132 - acc: 0.024 - 8s 1ms/step - loss: 4.7127 - acc: 0.0244 - val_loss: 4.7456 - val_acc: 0.0263\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.76200 to 4.74563, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 4.7510 - acc: 0.0000e+0 - ETA: 6s - loss: 4.7547 - acc: 0.0000e+0 - ETA: 6s - loss: 4.7793 - acc: 0.0071    - ETA: 6s - loss: 4.7596 - acc: 0.015 - ETA: 6s - loss: 4.7306 - acc: 0.019 - ETA: 6s - loss: 4.7587 - acc: 0.015 - ETA: 6s - loss: 4.7346 - acc: 0.018 - ETA: 6s - loss: 4.7171 - acc: 0.015 - ETA: 6s - loss: 4.7162 - acc: 0.014 - ETA: 6s - loss: 4.7025 - acc: 0.014 - ETA: 6s - loss: 4.6979 - acc: 0.012 - ETA: 6s - loss: 4.6900 - acc: 0.013 - ETA: 6s - loss: 4.6827 - acc: 0.012 - ETA: 6s - loss: 4.6879 - acc: 0.011 - ETA: 6s - loss: 4.6888 - acc: 0.011 - ETA: 6s - loss: 4.6809 - acc: 0.013 - ETA: 6s - loss: 4.6780 - acc: 0.014 - ETA: 6s - loss: 4.6731 - acc: 0.014 - ETA: 6s - loss: 4.6756 - acc: 0.014 - ETA: 5s - loss: 4.6765 - acc: 0.014 - ETA: 5s - loss: 4.6766 - acc: 0.013 - ETA: 5s - loss: 4.6755 - acc: 0.015 - ETA: 5s - loss: 4.6761 - acc: 0.017 - ETA: 5s - loss: 4.6816 - acc: 0.017 - ETA: 5s - loss: 4.6787 - acc: 0.017 - ETA: 5s - loss: 4.6793 - acc: 0.017 - ETA: 5s - loss: 4.6788 - acc: 0.019 - ETA: 5s - loss: 4.6850 - acc: 0.018 - ETA: 5s - loss: 4.6837 - acc: 0.019 - ETA: 5s - loss: 4.6826 - acc: 0.019 - ETA: 5s - loss: 4.6834 - acc: 0.020 - ETA: 5s - loss: 4.6873 - acc: 0.019 - ETA: 5s - loss: 4.6859 - acc: 0.020 - ETA: 5s - loss: 4.6863 - acc: 0.021 - ETA: 4s - loss: 4.6888 - acc: 0.020 - ETA: 4s - loss: 4.6877 - acc: 0.020 - ETA: 4s - loss: 4.6863 - acc: 0.021 - ETA: 4s - loss: 4.6851 - acc: 0.021 - ETA: 4s - loss: 4.6807 - acc: 0.021 - ETA: 4s - loss: 4.6819 - acc: 0.022 - ETA: 4s - loss: 4.6801 - acc: 0.023 - ETA: 4s - loss: 4.6817 - acc: 0.023 - ETA: 4s - loss: 4.6813 - acc: 0.022 - ETA: 4s - loss: 4.6781 - acc: 0.022 - ETA: 4s - loss: 4.6783 - acc: 0.022 - ETA: 4s - loss: 4.6781 - acc: 0.022 - ETA: 4s - loss: 4.6754 - acc: 0.023 - ETA: 4s - loss: 4.6777 - acc: 0.023 - ETA: 4s - loss: 4.6768 - acc: 0.023 - ETA: 4s - loss: 4.6761 - acc: 0.023 - ETA: 3s - loss: 4.6778 - acc: 0.023 - ETA: 3s - loss: 4.6777 - acc: 0.023 - ETA: 3s - loss: 4.6805 - acc: 0.022 - ETA: 3s - loss: 4.6813 - acc: 0.023 - ETA: 3s - loss: 4.6779 - acc: 0.023 - ETA: 3s - loss: 4.6802 - acc: 0.023 - ETA: 3s - loss: 4.6800 - acc: 0.024 - ETA: 3s - loss: 4.6814 - acc: 0.024 - ETA: 3s - loss: 4.6813 - acc: 0.024 - ETA: 3s - loss: 4.6821 - acc: 0.025 - ETA: 3s - loss: 4.6809 - acc: 0.024 - ETA: 3s - loss: 4.6820 - acc: 0.025 - ETA: 3s - loss: 4.6836 - acc: 0.024 - ETA: 3s - loss: 4.6857 - acc: 0.024 - ETA: 3s - loss: 4.6847 - acc: 0.024 - ETA: 2s - loss: 4.6856 - acc: 0.024 - ETA: 2s - loss: 4.6844 - acc: 0.024 - ETA: 2s - loss: 4.6842 - acc: 0.024 - ETA: 2s - loss: 4.6850 - acc: 0.024 - ETA: 2s - loss: 4.6842 - acc: 0.025 - ETA: 2s - loss: 4.6855 - acc: 0.025 - ETA: 2s - loss: 4.6858 - acc: 0.024 - ETA: 2s - loss: 4.6866 - acc: 0.024 - ETA: 2s - loss: 4.6857 - acc: 0.024 - ETA: 2s - loss: 4.6858 - acc: 0.024 - ETA: 2s - loss: 4.6866 - acc: 0.024 - ETA: 2s - loss: 4.6868 - acc: 0.024 - ETA: 2s - loss: 4.6864 - acc: 0.024 - ETA: 2s - loss: 4.6865 - acc: 0.024 - ETA: 2s - loss: 4.6879 - acc: 0.025 - ETA: 2s - loss: 4.6870 - acc: 0.024 - ETA: 1s - loss: 4.6862 - acc: 0.024 - ETA: 1s - loss: 4.6862 - acc: 0.024 - ETA: 1s - loss: 4.6873 - acc: 0.024 - ETA: 1s - loss: 4.6873 - acc: 0.024 - ETA: 1s - loss: 4.6885 - acc: 0.024 - ETA: 1s - loss: 4.6882 - acc: 0.024 - ETA: 1s - loss: 4.6874 - acc: 0.024 - ETA: 1s - loss: 4.6875 - acc: 0.024 - ETA: 1s - loss: 4.6888 - acc: 0.023 - ETA: 1s - loss: 4.6881 - acc: 0.024 - ETA: 1s - loss: 4.6887 - acc: 0.024 - ETA: 1s - loss: 4.6895 - acc: 0.024 - ETA: 1s - loss: 4.6894 - acc: 0.024 - ETA: 1s - loss: 4.6896 - acc: 0.024 - ETA: 1s - loss: 4.6888 - acc: 0.024 - ETA: 0s - loss: 4.6875 - acc: 0.024 - ETA: 0s - loss: 4.6883 - acc: 0.024 - ETA: 0s - loss: 4.6881 - acc: 0.025 - ETA: 0s - loss: 4.6888 - acc: 0.025 - ETA: 0s - loss: 4.6899 - acc: 0.025 - ETA: 0s - loss: 4.6897 - acc: 0.025 - ETA: 0s - loss: 4.6900 - acc: 0.025 - ETA: 0s - loss: 4.6912 - acc: 0.025 - ETA: 0s - loss: 4.6905 - acc: 0.025 - ETA: 0s - loss: 4.6922 - acc: 0.025 - ETA: 0s - loss: 4.6907 - acc: 0.025 - ETA: 0s - loss: 4.6907 - acc: 0.025 - ETA: 0s - loss: 4.6905 - acc: 0.025 - ETA: 0s - loss: 4.6905 - acc: 0.025 - ETA: 0s - loss: 4.6905 - acc: 0.025 - 8s 1ms/step - loss: 4.6903 - acc: 0.0254 - val_loss: 4.7426 - val_acc: 0.0263\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.74563 to 4.74263, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 4.5807 - acc: 0.0000e+0 - ETA: 6s - loss: 4.6120 - acc: 0.0000e+0 - ETA: 6s - loss: 4.7107 - acc: 0.0000e+0 - ETA: 6s - loss: 4.6960 - acc: 0.0050    - ETA: 6s - loss: 4.6868 - acc: 0.015 - ETA: 6s - loss: 4.6667 - acc: 0.025 - ETA: 6s - loss: 4.6510 - acc: 0.021 - ETA: 6s - loss: 4.6402 - acc: 0.022 - ETA: 6s - loss: 4.6277 - acc: 0.026 - ETA: 6s - loss: 4.6306 - acc: 0.028 - ETA: 6s - loss: 4.6221 - acc: 0.029 - ETA: 6s - loss: 4.6209 - acc: 0.026 - ETA: 6s - loss: 4.6230 - acc: 0.025 - ETA: 6s - loss: 4.6297 - acc: 0.028 - ETA: 6s - loss: 4.6310 - acc: 0.030 - ETA: 6s - loss: 4.6349 - acc: 0.030 - ETA: 6s - loss: 4.6467 - acc: 0.028 - ETA: 6s - loss: 4.6531 - acc: 0.028 - ETA: 6s - loss: 4.6503 - acc: 0.030 - ETA: 5s - loss: 4.6532 - acc: 0.030 - ETA: 5s - loss: 4.6519 - acc: 0.030 - ETA: 5s - loss: 4.6551 - acc: 0.029 - ETA: 5s - loss: 4.6580 - acc: 0.029 - ETA: 5s - loss: 4.6583 - acc: 0.030 - ETA: 5s - loss: 4.6604 - acc: 0.029 - ETA: 5s - loss: 4.6643 - acc: 0.030 - ETA: 5s - loss: 4.6699 - acc: 0.030 - ETA: 5s - loss: 4.6685 - acc: 0.031 - ETA: 5s - loss: 4.6666 - acc: 0.030 - ETA: 5s - loss: 4.6670 - acc: 0.030 - ETA: 5s - loss: 4.6666 - acc: 0.029 - ETA: 5s - loss: 4.6688 - acc: 0.029 - ETA: 5s - loss: 4.6623 - acc: 0.031 - ETA: 5s - loss: 4.6642 - acc: 0.031 - ETA: 4s - loss: 4.6587 - acc: 0.031 - ETA: 4s - loss: 4.6599 - acc: 0.032 - ETA: 4s - loss: 4.6615 - acc: 0.031 - ETA: 4s - loss: 4.6618 - acc: 0.031 - ETA: 4s - loss: 4.6636 - acc: 0.031 - ETA: 4s - loss: 4.6688 - acc: 0.030 - ETA: 4s - loss: 4.6727 - acc: 0.030 - ETA: 4s - loss: 4.6708 - acc: 0.030 - ETA: 4s - loss: 4.6702 - acc: 0.030 - ETA: 4s - loss: 4.6715 - acc: 0.030 - ETA: 4s - loss: 4.6713 - acc: 0.030 - ETA: 4s - loss: 4.6724 - acc: 0.029 - ETA: 4s - loss: 4.6713 - acc: 0.029 - ETA: 4s - loss: 4.6695 - acc: 0.029 - ETA: 4s - loss: 4.6696 - acc: 0.030 - ETA: 4s - loss: 4.6686 - acc: 0.030 - ETA: 3s - loss: 4.6687 - acc: 0.030 - ETA: 3s - loss: 4.6673 - acc: 0.030 - ETA: 3s - loss: 4.6664 - acc: 0.030 - ETA: 3s - loss: 4.6673 - acc: 0.030 - ETA: 3s - loss: 4.6688 - acc: 0.031 - ETA: 3s - loss: 4.6692 - acc: 0.031 - ETA: 3s - loss: 4.6657 - acc: 0.031 - ETA: 3s - loss: 4.6668 - acc: 0.032 - ETA: 3s - loss: 4.6652 - acc: 0.032 - ETA: 3s - loss: 4.6637 - acc: 0.032 - ETA: 3s - loss: 4.6651 - acc: 0.032 - ETA: 3s - loss: 4.6651 - acc: 0.032 - ETA: 3s - loss: 4.6657 - acc: 0.032 - ETA: 3s - loss: 4.6659 - acc: 0.032 - ETA: 3s - loss: 4.6657 - acc: 0.032 - ETA: 2s - loss: 4.6672 - acc: 0.032 - ETA: 2s - loss: 4.6687 - acc: 0.032 - ETA: 2s - loss: 4.6686 - acc: 0.031 - ETA: 2s - loss: 4.6688 - acc: 0.032 - ETA: 2s - loss: 4.6698 - acc: 0.031 - ETA: 2s - loss: 4.6713 - acc: 0.031 - ETA: 2s - loss: 4.6715 - acc: 0.031 - ETA: 2s - loss: 4.6697 - acc: 0.031 - ETA: 2s - loss: 4.6714 - acc: 0.031 - ETA: 2s - loss: 4.6708 - acc: 0.030 - ETA: 2s - loss: 4.6707 - acc: 0.031 - ETA: 2s - loss: 4.6684 - acc: 0.031 - ETA: 2s - loss: 4.6680 - acc: 0.031 - ETA: 2s - loss: 4.6688 - acc: 0.031 - ETA: 2s - loss: 4.6697 - acc: 0.030 - ETA: 2s - loss: 4.6696 - acc: 0.030 - ETA: 1s - loss: 4.6682 - acc: 0.030 - ETA: 1s - loss: 4.6700 - acc: 0.030 - ETA: 1s - loss: 4.6700 - acc: 0.030 - ETA: 1s - loss: 4.6699 - acc: 0.030 - ETA: 1s - loss: 4.6703 - acc: 0.030 - ETA: 1s - loss: 4.6714 - acc: 0.030 - ETA: 1s - loss: 4.6711 - acc: 0.030 - ETA: 1s - loss: 4.6724 - acc: 0.030 - ETA: 1s - loss: 4.6718 - acc: 0.030 - ETA: 1s - loss: 4.6713 - acc: 0.030 - ETA: 1s - loss: 4.6727 - acc: 0.030 - ETA: 1s - loss: 4.6721 - acc: 0.030 - ETA: 1s - loss: 4.6715 - acc: 0.030 - ETA: 1s - loss: 4.6728 - acc: 0.030 - ETA: 1s - loss: 4.6732 - acc: 0.029 - ETA: 0s - loss: 4.6718 - acc: 0.029 - ETA: 0s - loss: 4.6710 - acc: 0.029 - ETA: 0s - loss: 4.6711 - acc: 0.029 - ETA: 0s - loss: 4.6714 - acc: 0.029 - ETA: 0s - loss: 4.6709 - acc: 0.030 - ETA: 0s - loss: 4.6718 - acc: 0.030 - ETA: 0s - loss: 4.6725 - acc: 0.030 - ETA: 0s - loss: 4.6718 - acc: 0.030 - ETA: 0s - loss: 4.6714 - acc: 0.030 - ETA: 0s - loss: 4.6718 - acc: 0.030 - ETA: 0s - loss: 4.6700 - acc: 0.030 - ETA: 0s - loss: 4.6688 - acc: 0.030 - ETA: 0s - loss: 4.6692 - acc: 0.030 - ETA: 0s - loss: 4.6686 - acc: 0.030 - ETA: 0s - loss: 4.6706 - acc: 0.029 - 8s 1ms/step - loss: 4.6708 - acc: 0.0299 - val_loss: 4.7344 - val_acc: 0.0287\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.74263 to 4.73436, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 4.6069 - acc: 0.0000e+0 - ETA: 7s - loss: 4.6190 - acc: 0.0125    - ETA: 6s - loss: 4.6266 - acc: 0.028 - ETA: 6s - loss: 4.6002 - acc: 0.030 - ETA: 6s - loss: 4.6433 - acc: 0.030 - ETA: 6s - loss: 4.6471 - acc: 0.034 - ETA: 6s - loss: 4.6482 - acc: 0.034 - ETA: 6s - loss: 4.6340 - acc: 0.036 - ETA: 6s - loss: 4.6552 - acc: 0.032 - ETA: 6s - loss: 4.6329 - acc: 0.037 - ETA: 6s - loss: 4.6294 - acc: 0.037 - ETA: 6s - loss: 4.6127 - acc: 0.038 - ETA: 6s - loss: 4.6081 - acc: 0.036 - ETA: 6s - loss: 4.6030 - acc: 0.041 - ETA: 6s - loss: 4.6024 - acc: 0.040 - ETA: 6s - loss: 4.6068 - acc: 0.038 - ETA: 6s - loss: 4.6056 - acc: 0.036 - ETA: 6s - loss: 4.6103 - acc: 0.035 - ETA: 6s - loss: 4.6157 - acc: 0.035 - ETA: 5s - loss: 4.6237 - acc: 0.034 - ETA: 5s - loss: 4.6190 - acc: 0.035 - ETA: 5s - loss: 4.6207 - acc: 0.035 - ETA: 5s - loss: 4.6295 - acc: 0.034 - ETA: 5s - loss: 4.6309 - acc: 0.033 - ETA: 5s - loss: 4.6295 - acc: 0.033 - ETA: 5s - loss: 4.6259 - acc: 0.032 - ETA: 5s - loss: 4.6259 - acc: 0.031 - ETA: 5s - loss: 4.6270 - acc: 0.032 - ETA: 5s - loss: 4.6313 - acc: 0.033 - ETA: 5s - loss: 4.6351 - acc: 0.034 - ETA: 5s - loss: 4.6354 - acc: 0.033 - ETA: 5s - loss: 4.6354 - acc: 0.033 - ETA: 5s - loss: 4.6332 - acc: 0.033 - ETA: 5s - loss: 4.6351 - acc: 0.032 - ETA: 5s - loss: 4.6311 - acc: 0.033 - ETA: 4s - loss: 4.6308 - acc: 0.032 - ETA: 4s - loss: 4.6338 - acc: 0.032 - ETA: 4s - loss: 4.6329 - acc: 0.031 - ETA: 4s - loss: 4.6326 - acc: 0.031 - ETA: 4s - loss: 4.6298 - acc: 0.030 - ETA: 4s - loss: 4.6274 - acc: 0.030 - ETA: 4s - loss: 4.6344 - acc: 0.031 - ETA: 4s - loss: 4.6354 - acc: 0.031 - ETA: 4s - loss: 4.6351 - acc: 0.031 - ETA: 4s - loss: 4.6373 - acc: 0.030 - ETA: 4s - loss: 4.6388 - acc: 0.031 - ETA: 4s - loss: 4.6355 - acc: 0.031 - ETA: 4s - loss: 4.6384 - acc: 0.031 - ETA: 4s - loss: 4.6382 - acc: 0.031 - ETA: 4s - loss: 4.6391 - acc: 0.031 - ETA: 3s - loss: 4.6379 - acc: 0.031 - ETA: 3s - loss: 4.6397 - acc: 0.031 - ETA: 3s - loss: 4.6413 - acc: 0.030 - ETA: 3s - loss: 4.6434 - acc: 0.030 - ETA: 3s - loss: 4.6443 - acc: 0.030 - ETA: 3s - loss: 4.6460 - acc: 0.030 - ETA: 3s - loss: 4.6477 - acc: 0.030 - ETA: 3s - loss: 4.6487 - acc: 0.030 - ETA: 3s - loss: 4.6490 - acc: 0.030 - ETA: 3s - loss: 4.6469 - acc: 0.030 - ETA: 3s - loss: 4.6467 - acc: 0.030 - ETA: 3s - loss: 4.6470 - acc: 0.030 - ETA: 3s - loss: 4.6459 - acc: 0.030 - ETA: 3s - loss: 4.6458 - acc: 0.030 - ETA: 3s - loss: 4.6448 - acc: 0.030 - ETA: 2s - loss: 4.6459 - acc: 0.030 - ETA: 2s - loss: 4.6452 - acc: 0.030 - ETA: 2s - loss: 4.6448 - acc: 0.031 - ETA: 2s - loss: 4.6455 - acc: 0.031 - ETA: 2s - loss: 4.6457 - acc: 0.031 - ETA: 2s - loss: 4.6452 - acc: 0.031 - ETA: 2s - loss: 4.6476 - acc: 0.031 - ETA: 2s - loss: 4.6480 - acc: 0.030 - ETA: 2s - loss: 4.6462 - acc: 0.031 - ETA: 2s - loss: 4.6467 - acc: 0.031 - ETA: 2s - loss: 4.6451 - acc: 0.031 - ETA: 2s - loss: 4.6465 - acc: 0.031 - ETA: 2s - loss: 4.6483 - acc: 0.031 - ETA: 2s - loss: 4.6479 - acc: 0.031 - ETA: 2s - loss: 4.6495 - acc: 0.031 - ETA: 2s - loss: 4.6485 - acc: 0.031 - ETA: 1s - loss: 4.6483 - acc: 0.031 - ETA: 1s - loss: 4.6479 - acc: 0.031 - ETA: 1s - loss: 4.6473 - acc: 0.031 - ETA: 1s - loss: 4.6466 - acc: 0.031 - ETA: 1s - loss: 4.6477 - acc: 0.031 - ETA: 1s - loss: 4.6474 - acc: 0.031 - ETA: 1s - loss: 4.6466 - acc: 0.031 - ETA: 1s - loss: 4.6466 - acc: 0.031 - ETA: 1s - loss: 4.6466 - acc: 0.031 - ETA: 1s - loss: 4.6463 - acc: 0.032 - ETA: 1s - loss: 4.6470 - acc: 0.031 - ETA: 1s - loss: 4.6477 - acc: 0.031 - ETA: 1s - loss: 4.6487 - acc: 0.031 - ETA: 1s - loss: 4.6480 - acc: 0.031 - ETA: 1s - loss: 4.6477 - acc: 0.031 - ETA: 0s - loss: 4.6462 - acc: 0.031 - ETA: 0s - loss: 4.6473 - acc: 0.031 - ETA: 0s - loss: 4.6475 - acc: 0.031 - ETA: 0s - loss: 4.6475 - acc: 0.031 - ETA: 0s - loss: 4.6477 - acc: 0.031 - ETA: 0s - loss: 4.6479 - acc: 0.031 - ETA: 0s - loss: 4.6480 - acc: 0.031 - ETA: 0s - loss: 4.6478 - acc: 0.031 - ETA: 0s - loss: 4.6478 - acc: 0.031 - ETA: 0s - loss: 4.6477 - acc: 0.031 - ETA: 0s - loss: 4.6471 - acc: 0.031 - ETA: 0s - loss: 4.6470 - acc: 0.031 - ETA: 0s - loss: 4.6466 - acc: 0.031 - ETA: 0s - loss: 4.6465 - acc: 0.032 - ETA: 0s - loss: 4.6470 - acc: 0.032 - 8s 1ms/step - loss: 4.6470 - acc: 0.0319 - val_loss: 4.7024 - val_acc: 0.0240\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.73436 to 4.70242, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 4.4521 - acc: 0.050 - ETA: 7s - loss: 4.5830 - acc: 0.025 - ETA: 7s - loss: 4.5233 - acc: 0.028 - ETA: 6s - loss: 4.6120 - acc: 0.030 - ETA: 6s - loss: 4.6308 - acc: 0.026 - ETA: 6s - loss: 4.6176 - acc: 0.028 - ETA: 6s - loss: 4.6106 - acc: 0.023 - ETA: 6s - loss: 4.6088 - acc: 0.025 - ETA: 6s - loss: 4.6296 - acc: 0.024 - ETA: 6s - loss: 4.6223 - acc: 0.026 - ETA: 6s - loss: 4.6171 - acc: 0.030 - ETA: 6s - loss: 4.6212 - acc: 0.029 - ETA: 6s - loss: 4.6180 - acc: 0.029 - ETA: 6s - loss: 4.6265 - acc: 0.030 - ETA: 6s - loss: 4.6231 - acc: 0.030 - ETA: 6s - loss: 4.6275 - acc: 0.029 - ETA: 6s - loss: 4.6283 - acc: 0.029 - ETA: 6s - loss: 4.6283 - acc: 0.027 - ETA: 6s - loss: 4.6225 - acc: 0.028 - ETA: 5s - loss: 4.6323 - acc: 0.031 - ETA: 5s - loss: 4.6347 - acc: 0.032 - ETA: 5s - loss: 4.6292 - acc: 0.033 - ETA: 5s - loss: 4.6349 - acc: 0.035 - ETA: 5s - loss: 4.6348 - acc: 0.035 - ETA: 5s - loss: 4.6380 - acc: 0.034 - ETA: 5s - loss: 4.6379 - acc: 0.034 - ETA: 5s - loss: 4.6358 - acc: 0.035 - ETA: 5s - loss: 4.6314 - acc: 0.034 - ETA: 5s - loss: 4.6290 - acc: 0.034 - ETA: 5s - loss: 4.6307 - acc: 0.033 - ETA: 5s - loss: 4.6329 - acc: 0.034 - ETA: 5s - loss: 4.6282 - acc: 0.035 - ETA: 5s - loss: 4.6311 - acc: 0.035 - ETA: 5s - loss: 4.6277 - acc: 0.035 - ETA: 5s - loss: 4.6225 - acc: 0.034 - ETA: 4s - loss: 4.6253 - acc: 0.034 - ETA: 4s - loss: 4.6267 - acc: 0.033 - ETA: 4s - loss: 4.6247 - acc: 0.033 - ETA: 4s - loss: 4.6219 - acc: 0.032 - ETA: 4s - loss: 4.6217 - acc: 0.032 - ETA: 4s - loss: 4.6244 - acc: 0.032 - ETA: 4s - loss: 4.6252 - acc: 0.032 - ETA: 4s - loss: 4.6244 - acc: 0.033 - ETA: 4s - loss: 4.6240 - acc: 0.033 - ETA: 4s - loss: 4.6227 - acc: 0.032 - ETA: 4s - loss: 4.6208 - acc: 0.032 - ETA: 4s - loss: 4.6193 - acc: 0.032 - ETA: 4s - loss: 4.6171 - acc: 0.033 - ETA: 4s - loss: 4.6165 - acc: 0.033 - ETA: 4s - loss: 4.6143 - acc: 0.033 - ETA: 3s - loss: 4.6144 - acc: 0.033 - ETA: 3s - loss: 4.6181 - acc: 0.033 - ETA: 3s - loss: 4.6177 - acc: 0.033 - ETA: 3s - loss: 4.6198 - acc: 0.034 - ETA: 3s - loss: 4.6197 - acc: 0.034 - ETA: 3s - loss: 4.6178 - acc: 0.034 - ETA: 3s - loss: 4.6173 - acc: 0.034 - ETA: 3s - loss: 4.6146 - acc: 0.034 - ETA: 3s - loss: 4.6162 - acc: 0.035 - ETA: 3s - loss: 4.6182 - acc: 0.034 - ETA: 3s - loss: 4.6167 - acc: 0.034 - ETA: 3s - loss: 4.6162 - acc: 0.035 - ETA: 3s - loss: 4.6162 - acc: 0.034 - ETA: 3s - loss: 4.6153 - acc: 0.034 - ETA: 3s - loss: 4.6155 - acc: 0.034 - ETA: 2s - loss: 4.6161 - acc: 0.034 - ETA: 2s - loss: 4.6147 - acc: 0.034 - ETA: 2s - loss: 4.6157 - acc: 0.034 - ETA: 2s - loss: 4.6177 - acc: 0.034 - ETA: 2s - loss: 4.6175 - acc: 0.035 - ETA: 2s - loss: 4.6167 - acc: 0.035 - ETA: 2s - loss: 4.6179 - acc: 0.035 - ETA: 2s - loss: 4.6181 - acc: 0.034 - ETA: 2s - loss: 4.6176 - acc: 0.034 - ETA: 2s - loss: 4.6197 - acc: 0.034 - ETA: 2s - loss: 4.6183 - acc: 0.034 - ETA: 2s - loss: 4.6164 - acc: 0.034 - ETA: 2s - loss: 4.6163 - acc: 0.034 - ETA: 2s - loss: 4.6173 - acc: 0.034 - ETA: 2s - loss: 4.6182 - acc: 0.034 - ETA: 2s - loss: 4.6193 - acc: 0.034 - ETA: 1s - loss: 4.6208 - acc: 0.034 - ETA: 1s - loss: 4.6224 - acc: 0.033 - ETA: 1s - loss: 4.6223 - acc: 0.034 - ETA: 1s - loss: 4.6225 - acc: 0.034 - ETA: 1s - loss: 4.6230 - acc: 0.034 - ETA: 1s - loss: 4.6229 - acc: 0.034 - ETA: 1s - loss: 4.6223 - acc: 0.034 - ETA: 1s - loss: 4.6217 - acc: 0.033 - ETA: 1s - loss: 4.6205 - acc: 0.033 - ETA: 1s - loss: 4.6217 - acc: 0.033 - ETA: 1s - loss: 4.6228 - acc: 0.033 - ETA: 1s - loss: 4.6226 - acc: 0.033 - ETA: 1s - loss: 4.6230 - acc: 0.033 - ETA: 1s - loss: 4.6230 - acc: 0.033 - ETA: 1s - loss: 4.6242 - acc: 0.032 - ETA: 0s - loss: 4.6236 - acc: 0.032 - ETA: 0s - loss: 4.6238 - acc: 0.032 - ETA: 0s - loss: 4.6253 - acc: 0.032 - ETA: 0s - loss: 4.6253 - acc: 0.033 - ETA: 0s - loss: 4.6254 - acc: 0.032 - ETA: 0s - loss: 4.6255 - acc: 0.032 - ETA: 0s - loss: 4.6256 - acc: 0.032 - ETA: 0s - loss: 4.6267 - acc: 0.032 - ETA: 0s - loss: 4.6263 - acc: 0.032 - ETA: 0s - loss: 4.6274 - acc: 0.032 - ETA: 0s - loss: 4.6269 - acc: 0.032 - ETA: 0s - loss: 4.6286 - acc: 0.032 - ETA: 0s - loss: 4.6294 - acc: 0.032 - ETA: 0s - loss: 4.6295 - acc: 0.032 - ETA: 0s - loss: 4.6299 - acc: 0.032 - 8s 1ms/step - loss: 4.6307 - acc: 0.0325 - val_loss: 4.6812 - val_acc: 0.0335\n",
      "\n",
      "Epoch 00011: val_loss improved from 4.70242 to 4.68119, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 4.4228 - acc: 0.100 - ETA: 7s - loss: 4.4723 - acc: 0.062 - ETA: 7s - loss: 4.5201 - acc: 0.057 - ETA: 7s - loss: 4.5562 - acc: 0.055 - ETA: 6s - loss: 4.5590 - acc: 0.057 - ETA: 6s - loss: 4.5826 - acc: 0.053 - ETA: 6s - loss: 4.6004 - acc: 0.047 - ETA: 6s - loss: 4.6093 - acc: 0.050 - ETA: 6s - loss: 4.6243 - acc: 0.048 - ETA: 6s - loss: 4.6166 - acc: 0.044 - ETA: 6s - loss: 4.6110 - acc: 0.046 - ETA: 6s - loss: 4.6107 - acc: 0.042 - ETA: 6s - loss: 4.5994 - acc: 0.045 - ETA: 6s - loss: 4.5986 - acc: 0.042 - ETA: 6s - loss: 4.6002 - acc: 0.041 - ETA: 6s - loss: 4.6011 - acc: 0.042 - ETA: 6s - loss: 4.6006 - acc: 0.042 - ETA: 6s - loss: 4.6071 - acc: 0.040 - ETA: 6s - loss: 4.6033 - acc: 0.040 - ETA: 5s - loss: 4.5994 - acc: 0.041 - ETA: 5s - loss: 4.6006 - acc: 0.041 - ETA: 5s - loss: 4.6114 - acc: 0.040 - ETA: 5s - loss: 4.6133 - acc: 0.042 - ETA: 5s - loss: 4.6173 - acc: 0.042 - ETA: 5s - loss: 4.6161 - acc: 0.042 - ETA: 5s - loss: 4.6219 - acc: 0.042 - ETA: 5s - loss: 4.6203 - acc: 0.041 - ETA: 5s - loss: 4.6186 - acc: 0.040 - ETA: 5s - loss: 4.6184 - acc: 0.039 - ETA: 5s - loss: 4.6193 - acc: 0.039 - ETA: 5s - loss: 4.6202 - acc: 0.040 - ETA: 5s - loss: 4.6229 - acc: 0.038 - ETA: 5s - loss: 4.6199 - acc: 0.038 - ETA: 5s - loss: 4.6230 - acc: 0.038 - ETA: 5s - loss: 4.6239 - acc: 0.038 - ETA: 4s - loss: 4.6251 - acc: 0.038 - ETA: 4s - loss: 4.6183 - acc: 0.039 - ETA: 4s - loss: 4.6170 - acc: 0.038 - ETA: 4s - loss: 4.6168 - acc: 0.039 - ETA: 4s - loss: 4.6180 - acc: 0.041 - ETA: 4s - loss: 4.6181 - acc: 0.042 - ETA: 4s - loss: 4.6125 - acc: 0.041 - ETA: 4s - loss: 4.6100 - acc: 0.042 - ETA: 4s - loss: 4.6134 - acc: 0.041 - ETA: 4s - loss: 4.6140 - acc: 0.040 - ETA: 4s - loss: 4.6186 - acc: 0.040 - ETA: 4s - loss: 4.6188 - acc: 0.040 - ETA: 4s - loss: 4.6180 - acc: 0.040 - ETA: 4s - loss: 4.6187 - acc: 0.039 - ETA: 4s - loss: 4.6196 - acc: 0.039 - ETA: 3s - loss: 4.6196 - acc: 0.039 - ETA: 3s - loss: 4.6176 - acc: 0.039 - ETA: 3s - loss: 4.6192 - acc: 0.039 - ETA: 3s - loss: 4.6199 - acc: 0.038 - ETA: 3s - loss: 4.6190 - acc: 0.038 - ETA: 3s - loss: 4.6182 - acc: 0.038 - ETA: 3s - loss: 4.6165 - acc: 0.039 - ETA: 3s - loss: 4.6166 - acc: 0.039 - ETA: 3s - loss: 4.6141 - acc: 0.038 - ETA: 3s - loss: 4.6129 - acc: 0.039 - ETA: 3s - loss: 4.6103 - acc: 0.039 - ETA: 3s - loss: 4.6102 - acc: 0.039 - ETA: 3s - loss: 4.6146 - acc: 0.039 - ETA: 3s - loss: 4.6117 - acc: 0.039 - ETA: 3s - loss: 4.6115 - acc: 0.039 - ETA: 2s - loss: 4.6132 - acc: 0.039 - ETA: 2s - loss: 4.6114 - acc: 0.039 - ETA: 2s - loss: 4.6119 - acc: 0.039 - ETA: 2s - loss: 4.6113 - acc: 0.040 - ETA: 2s - loss: 4.6121 - acc: 0.039 - ETA: 2s - loss: 4.6120 - acc: 0.039 - ETA: 2s - loss: 4.6143 - acc: 0.039 - ETA: 2s - loss: 4.6125 - acc: 0.039 - ETA: 2s - loss: 4.6114 - acc: 0.039 - ETA: 2s - loss: 4.6109 - acc: 0.039 - ETA: 2s - loss: 4.6114 - acc: 0.039 - ETA: 2s - loss: 4.6108 - acc: 0.039 - ETA: 2s - loss: 4.6117 - acc: 0.039 - ETA: 2s - loss: 4.6107 - acc: 0.040 - ETA: 2s - loss: 4.6103 - acc: 0.040 - ETA: 2s - loss: 4.6100 - acc: 0.039 - ETA: 1s - loss: 4.6104 - acc: 0.040 - ETA: 1s - loss: 4.6105 - acc: 0.040 - ETA: 1s - loss: 4.6113 - acc: 0.040 - ETA: 1s - loss: 4.6128 - acc: 0.039 - ETA: 1s - loss: 4.6132 - acc: 0.039 - ETA: 1s - loss: 4.6144 - acc: 0.039 - ETA: 1s - loss: 4.6145 - acc: 0.039 - ETA: 1s - loss: 4.6151 - acc: 0.039 - ETA: 1s - loss: 4.6132 - acc: 0.039 - ETA: 1s - loss: 4.6115 - acc: 0.039 - ETA: 1s - loss: 4.6123 - acc: 0.039 - ETA: 1s - loss: 4.6138 - acc: 0.039 - ETA: 1s - loss: 4.6151 - acc: 0.039 - ETA: 1s - loss: 4.6148 - acc: 0.039 - ETA: 1s - loss: 4.6166 - acc: 0.039 - ETA: 0s - loss: 4.6183 - acc: 0.038 - ETA: 0s - loss: 4.6187 - acc: 0.038 - ETA: 0s - loss: 4.6175 - acc: 0.039 - ETA: 0s - loss: 4.6163 - acc: 0.039 - ETA: 0s - loss: 4.6164 - acc: 0.038 - ETA: 0s - loss: 4.6172 - acc: 0.038 - ETA: 0s - loss: 4.6157 - acc: 0.038 - ETA: 0s - loss: 4.6149 - acc: 0.038 - ETA: 0s - loss: 4.6156 - acc: 0.038 - ETA: 0s - loss: 4.6145 - acc: 0.038 - ETA: 0s - loss: 4.6135 - acc: 0.039 - ETA: 0s - loss: 4.6130 - acc: 0.039 - ETA: 0s - loss: 4.6123 - acc: 0.038 - ETA: 0s - loss: 4.6111 - acc: 0.039 - ETA: 0s - loss: 4.6097 - acc: 0.039 - 8s 1ms/step - loss: 4.6105 - acc: 0.0386 - val_loss: 4.6881 - val_acc: 0.0263\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 4.68119\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 4.3981 - acc: 0.050 - ETA: 7s - loss: 4.4313 - acc: 0.062 - ETA: 7s - loss: 4.4214 - acc: 0.050 - ETA: 6s - loss: 4.4638 - acc: 0.050 - ETA: 6s - loss: 4.4181 - acc: 0.053 - ETA: 6s - loss: 4.4479 - acc: 0.053 - ETA: 6s - loss: 4.4473 - acc: 0.055 - ETA: 6s - loss: 4.4724 - acc: 0.052 - ETA: 6s - loss: 4.4801 - acc: 0.048 - ETA: 6s - loss: 4.4901 - acc: 0.048 - ETA: 6s - loss: 4.5014 - acc: 0.050 - ETA: 6s - loss: 4.5099 - acc: 0.050 - ETA: 6s - loss: 4.5280 - acc: 0.047 - ETA: 6s - loss: 4.5240 - acc: 0.048 - ETA: 6s - loss: 4.5258 - acc: 0.047 - ETA: 6s - loss: 4.5302 - acc: 0.046 - ETA: 6s - loss: 4.5372 - acc: 0.046 - ETA: 6s - loss: 4.5477 - acc: 0.045 - ETA: 6s - loss: 4.5483 - acc: 0.043 - ETA: 5s - loss: 4.5560 - acc: 0.041 - ETA: 5s - loss: 4.5666 - acc: 0.039 - ETA: 5s - loss: 4.5728 - acc: 0.038 - ETA: 5s - loss: 4.5743 - acc: 0.037 - ETA: 5s - loss: 4.5737 - acc: 0.038 - ETA: 5s - loss: 4.5807 - acc: 0.037 - ETA: 5s - loss: 4.5834 - acc: 0.038 - ETA: 5s - loss: 4.5810 - acc: 0.037 - ETA: 5s - loss: 4.5753 - acc: 0.037 - ETA: 5s - loss: 4.5719 - acc: 0.039 - ETA: 5s - loss: 4.5707 - acc: 0.039 - ETA: 5s - loss: 4.5669 - acc: 0.038 - ETA: 5s - loss: 4.5720 - acc: 0.039 - ETA: 5s - loss: 4.5703 - acc: 0.040 - ETA: 5s - loss: 4.5683 - acc: 0.041 - ETA: 4s - loss: 4.5709 - acc: 0.040 - ETA: 4s - loss: 4.5704 - acc: 0.042 - ETA: 4s - loss: 4.5718 - acc: 0.043 - ETA: 4s - loss: 4.5707 - acc: 0.042 - ETA: 4s - loss: 4.5710 - acc: 0.040 - ETA: 4s - loss: 4.5713 - acc: 0.040 - ETA: 4s - loss: 4.5727 - acc: 0.040 - ETA: 4s - loss: 4.5764 - acc: 0.041 - ETA: 4s - loss: 4.5771 - acc: 0.040 - ETA: 4s - loss: 4.5781 - acc: 0.040 - ETA: 4s - loss: 4.5778 - acc: 0.039 - ETA: 4s - loss: 4.5808 - acc: 0.039 - ETA: 4s - loss: 4.5788 - acc: 0.040 - ETA: 4s - loss: 4.5803 - acc: 0.040 - ETA: 4s - loss: 4.5819 - acc: 0.039 - ETA: 4s - loss: 4.5837 - acc: 0.039 - ETA: 3s - loss: 4.5862 - acc: 0.039 - ETA: 3s - loss: 4.5865 - acc: 0.039 - ETA: 3s - loss: 4.5884 - acc: 0.038 - ETA: 3s - loss: 4.5894 - acc: 0.038 - ETA: 3s - loss: 4.5903 - acc: 0.038 - ETA: 3s - loss: 4.5887 - acc: 0.039 - ETA: 3s - loss: 4.5883 - acc: 0.038 - ETA: 3s - loss: 4.5894 - acc: 0.039 - ETA: 3s - loss: 4.5911 - acc: 0.039 - ETA: 3s - loss: 4.5941 - acc: 0.039 - ETA: 3s - loss: 4.5943 - acc: 0.039 - ETA: 3s - loss: 4.5947 - acc: 0.038 - ETA: 3s - loss: 4.5949 - acc: 0.038 - ETA: 3s - loss: 4.5948 - acc: 0.038 - ETA: 3s - loss: 4.5942 - acc: 0.038 - ETA: 2s - loss: 4.5947 - acc: 0.038 - ETA: 2s - loss: 4.5934 - acc: 0.037 - ETA: 2s - loss: 4.5922 - acc: 0.037 - ETA: 2s - loss: 4.5915 - acc: 0.038 - ETA: 2s - loss: 4.5919 - acc: 0.038 - ETA: 2s - loss: 4.5922 - acc: 0.038 - ETA: 2s - loss: 4.5927 - acc: 0.037 - ETA: 2s - loss: 4.5928 - acc: 0.038 - ETA: 2s - loss: 4.5929 - acc: 0.038 - ETA: 2s - loss: 4.5935 - acc: 0.038 - ETA: 2s - loss: 4.5914 - acc: 0.038 - ETA: 2s - loss: 4.5930 - acc: 0.038 - ETA: 2s - loss: 4.5943 - acc: 0.038 - ETA: 2s - loss: 4.5936 - acc: 0.038 - ETA: 2s - loss: 4.5939 - acc: 0.037 - ETA: 2s - loss: 4.5921 - acc: 0.037 - ETA: 1s - loss: 4.5916 - acc: 0.037 - ETA: 1s - loss: 4.5924 - acc: 0.037 - ETA: 1s - loss: 4.5919 - acc: 0.037 - ETA: 1s - loss: 4.5928 - acc: 0.038 - ETA: 1s - loss: 4.5923 - acc: 0.038 - ETA: 1s - loss: 4.5927 - acc: 0.038 - ETA: 1s - loss: 4.5950 - acc: 0.038 - ETA: 1s - loss: 4.5935 - acc: 0.038 - ETA: 1s - loss: 4.5929 - acc: 0.038 - ETA: 1s - loss: 4.5915 - acc: 0.038 - ETA: 1s - loss: 4.5921 - acc: 0.038 - ETA: 1s - loss: 4.5930 - acc: 0.038 - ETA: 1s - loss: 4.5944 - acc: 0.038 - ETA: 1s - loss: 4.5934 - acc: 0.038 - ETA: 1s - loss: 4.5923 - acc: 0.038 - ETA: 0s - loss: 4.5927 - acc: 0.038 - ETA: 0s - loss: 4.5919 - acc: 0.038 - ETA: 0s - loss: 4.5926 - acc: 0.038 - ETA: 0s - loss: 4.5922 - acc: 0.037 - ETA: 0s - loss: 4.5915 - acc: 0.037 - ETA: 0s - loss: 4.5912 - acc: 0.037 - ETA: 0s - loss: 4.5894 - acc: 0.037 - ETA: 0s - loss: 4.5906 - acc: 0.037 - ETA: 0s - loss: 4.5924 - acc: 0.037 - ETA: 0s - loss: 4.5925 - acc: 0.037 - ETA: 0s - loss: 4.5922 - acc: 0.037 - ETA: 0s - loss: 4.5924 - acc: 0.037 - ETA: 0s - loss: 4.5923 - acc: 0.037 - ETA: 0s - loss: 4.5921 - acc: 0.037 - ETA: 0s - loss: 4.5911 - acc: 0.037 - 8s 1ms/step - loss: 4.5900 - acc: 0.0379 - val_loss: 4.6518 - val_acc: 0.0263\n",
      "\n",
      "Epoch 00013: val_loss improved from 4.68119 to 4.65182, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 4.6359 - acc: 0.0000e+0 - ETA: 7s - loss: 4.5716 - acc: 0.0375    - ETA: 6s - loss: 4.5229 - acc: 0.042 - ETA: 6s - loss: 4.5301 - acc: 0.060 - ETA: 6s - loss: 4.5322 - acc: 0.053 - ETA: 7s - loss: 4.5033 - acc: 0.060 - ETA: 6s - loss: 4.5091 - acc: 0.058 - ETA: 6s - loss: 4.5101 - acc: 0.057 - ETA: 6s - loss: 4.5186 - acc: 0.050 - ETA: 6s - loss: 4.5260 - acc: 0.050 - ETA: 6s - loss: 4.5193 - acc: 0.050 - ETA: 6s - loss: 4.5135 - acc: 0.050 - ETA: 6s - loss: 4.5171 - acc: 0.050 - ETA: 6s - loss: 4.5242 - acc: 0.047 - ETA: 6s - loss: 4.5203 - acc: 0.048 - ETA: 6s - loss: 4.5189 - acc: 0.050 - ETA: 6s - loss: 4.5266 - acc: 0.051 - ETA: 6s - loss: 4.5237 - acc: 0.050 - ETA: 6s - loss: 4.5321 - acc: 0.050 - ETA: 6s - loss: 4.5458 - acc: 0.048 - ETA: 5s - loss: 4.5386 - acc: 0.048 - ETA: 5s - loss: 4.5450 - acc: 0.046 - ETA: 5s - loss: 4.5498 - acc: 0.046 - ETA: 5s - loss: 4.5461 - acc: 0.047 - ETA: 5s - loss: 4.5476 - acc: 0.047 - ETA: 5s - loss: 4.5481 - acc: 0.046 - ETA: 5s - loss: 4.5469 - acc: 0.046 - ETA: 5s - loss: 4.5513 - acc: 0.045 - ETA: 5s - loss: 4.5538 - acc: 0.045 - ETA: 5s - loss: 4.5533 - acc: 0.046 - ETA: 5s - loss: 4.5589 - acc: 0.046 - ETA: 5s - loss: 4.5596 - acc: 0.045 - ETA: 5s - loss: 4.5667 - acc: 0.044 - ETA: 5s - loss: 4.5641 - acc: 0.044 - ETA: 5s - loss: 4.5682 - acc: 0.043 - ETA: 4s - loss: 4.5662 - acc: 0.044 - ETA: 4s - loss: 4.5665 - acc: 0.044 - ETA: 4s - loss: 4.5631 - acc: 0.045 - ETA: 4s - loss: 4.5655 - acc: 0.044 - ETA: 4s - loss: 4.5679 - acc: 0.044 - ETA: 4s - loss: 4.5706 - acc: 0.043 - ETA: 4s - loss: 4.5649 - acc: 0.043 - ETA: 4s - loss: 4.5698 - acc: 0.042 - ETA: 4s - loss: 4.5683 - acc: 0.043 - ETA: 4s - loss: 4.5684 - acc: 0.043 - ETA: 4s - loss: 4.5685 - acc: 0.043 - ETA: 4s - loss: 4.5690 - acc: 0.043 - ETA: 4s - loss: 4.5686 - acc: 0.044 - ETA: 4s - loss: 4.5643 - acc: 0.044 - ETA: 4s - loss: 4.5635 - acc: 0.043 - ETA: 3s - loss: 4.5616 - acc: 0.044 - ETA: 3s - loss: 4.5636 - acc: 0.043 - ETA: 3s - loss: 4.5611 - acc: 0.044 - ETA: 3s - loss: 4.5637 - acc: 0.044 - ETA: 3s - loss: 4.5620 - acc: 0.044 - ETA: 3s - loss: 4.5621 - acc: 0.043 - ETA: 3s - loss: 4.5607 - acc: 0.043 - ETA: 3s - loss: 4.5603 - acc: 0.042 - ETA: 3s - loss: 4.5570 - acc: 0.042 - ETA: 3s - loss: 4.5578 - acc: 0.042 - ETA: 3s - loss: 4.5605 - acc: 0.043 - ETA: 3s - loss: 4.5609 - acc: 0.042 - ETA: 3s - loss: 4.5591 - acc: 0.043 - ETA: 3s - loss: 4.5605 - acc: 0.042 - ETA: 3s - loss: 4.5613 - acc: 0.042 - ETA: 3s - loss: 4.5609 - acc: 0.041 - ETA: 2s - loss: 4.5598 - acc: 0.041 - ETA: 2s - loss: 4.5593 - acc: 0.041 - ETA: 2s - loss: 4.5599 - acc: 0.041 - ETA: 2s - loss: 4.5591 - acc: 0.042 - ETA: 2s - loss: 4.5596 - acc: 0.041 - ETA: 2s - loss: 4.5609 - acc: 0.041 - ETA: 2s - loss: 4.5600 - acc: 0.041 - ETA: 2s - loss: 4.5608 - acc: 0.040 - ETA: 2s - loss: 4.5613 - acc: 0.040 - ETA: 2s - loss: 4.5611 - acc: 0.040 - ETA: 2s - loss: 4.5607 - acc: 0.040 - ETA: 2s - loss: 4.5605 - acc: 0.040 - ETA: 2s - loss: 4.5593 - acc: 0.040 - ETA: 2s - loss: 4.5584 - acc: 0.040 - ETA: 2s - loss: 4.5615 - acc: 0.040 - ETA: 1s - loss: 4.5625 - acc: 0.039 - ETA: 1s - loss: 4.5642 - acc: 0.040 - ETA: 1s - loss: 4.5648 - acc: 0.040 - ETA: 1s - loss: 4.5629 - acc: 0.040 - ETA: 1s - loss: 4.5627 - acc: 0.041 - ETA: 1s - loss: 4.5619 - acc: 0.040 - ETA: 1s - loss: 4.5610 - acc: 0.040 - ETA: 1s - loss: 4.5613 - acc: 0.041 - ETA: 1s - loss: 4.5625 - acc: 0.040 - ETA: 1s - loss: 4.5633 - acc: 0.040 - ETA: 1s - loss: 4.5650 - acc: 0.040 - ETA: 1s - loss: 4.5643 - acc: 0.040 - ETA: 1s - loss: 4.5629 - acc: 0.041 - ETA: 1s - loss: 4.5623 - acc: 0.041 - ETA: 1s - loss: 4.5627 - acc: 0.041 - ETA: 0s - loss: 4.5632 - acc: 0.041 - ETA: 0s - loss: 4.5638 - acc: 0.042 - ETA: 0s - loss: 4.5649 - acc: 0.042 - ETA: 0s - loss: 4.5644 - acc: 0.042 - ETA: 0s - loss: 4.5643 - acc: 0.043 - ETA: 0s - loss: 4.5662 - acc: 0.043 - ETA: 0s - loss: 4.5657 - acc: 0.043 - ETA: 0s - loss: 4.5662 - acc: 0.042 - ETA: 0s - loss: 4.5661 - acc: 0.042 - ETA: 0s - loss: 4.5652 - acc: 0.042 - ETA: 0s - loss: 4.5661 - acc: 0.042 - ETA: 0s - loss: 4.5656 - acc: 0.042 - ETA: 0s - loss: 4.5665 - acc: 0.042 - ETA: 0s - loss: 4.5662 - acc: 0.042 - ETA: 0s - loss: 4.5669 - acc: 0.042 - ETA: 0s - loss: 4.5671 - acc: 0.042 - 8s 1ms/step - loss: 4.5674 - acc: 0.0424 - val_loss: 4.6451 - val_acc: 0.0371\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.65182 to 4.64511, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 4.4818 - acc: 0.100 - ETA: 7s - loss: 4.5118 - acc: 0.112 - ETA: 7s - loss: 4.5526 - acc: 0.085 - ETA: 6s - loss: 4.5391 - acc: 0.070 - ETA: 6s - loss: 4.5226 - acc: 0.069 - ETA: 6s - loss: 4.5116 - acc: 0.059 - ETA: 6s - loss: 4.4909 - acc: 0.052 - ETA: 6s - loss: 4.5060 - acc: 0.050 - ETA: 6s - loss: 4.4969 - acc: 0.050 - ETA: 6s - loss: 4.4800 - acc: 0.057 - ETA: 6s - loss: 4.4660 - acc: 0.053 - ETA: 6s - loss: 4.4702 - acc: 0.051 - ETA: 6s - loss: 4.4665 - acc: 0.054 - ETA: 6s - loss: 4.4678 - acc: 0.053 - ETA: 6s - loss: 4.4797 - acc: 0.051 - ETA: 6s - loss: 4.4857 - acc: 0.051 - ETA: 6s - loss: 4.5007 - acc: 0.051 - ETA: 6s - loss: 4.5016 - acc: 0.050 - ETA: 6s - loss: 4.4986 - acc: 0.051 - ETA: 5s - loss: 4.4950 - acc: 0.052 - ETA: 5s - loss: 4.4967 - acc: 0.050 - ETA: 5s - loss: 4.4946 - acc: 0.050 - ETA: 5s - loss: 4.4961 - acc: 0.050 - ETA: 5s - loss: 4.5043 - acc: 0.050 - ETA: 5s - loss: 4.4999 - acc: 0.050 - ETA: 5s - loss: 4.5040 - acc: 0.049 - ETA: 5s - loss: 4.5044 - acc: 0.050 - ETA: 5s - loss: 4.5056 - acc: 0.050 - ETA: 5s - loss: 4.5087 - acc: 0.049 - ETA: 5s - loss: 4.5087 - acc: 0.048 - ETA: 5s - loss: 4.5039 - acc: 0.048 - ETA: 5s - loss: 4.5020 - acc: 0.049 - ETA: 5s - loss: 4.5035 - acc: 0.049 - ETA: 5s - loss: 4.5060 - acc: 0.049 - ETA: 5s - loss: 4.5109 - acc: 0.049 - ETA: 4s - loss: 4.5152 - acc: 0.047 - ETA: 4s - loss: 4.5145 - acc: 0.047 - ETA: 4s - loss: 4.5135 - acc: 0.048 - ETA: 4s - loss: 4.5198 - acc: 0.047 - ETA: 4s - loss: 4.5192 - acc: 0.047 - ETA: 4s - loss: 4.5204 - acc: 0.048 - ETA: 4s - loss: 4.5211 - acc: 0.047 - ETA: 4s - loss: 4.5268 - acc: 0.046 - ETA: 4s - loss: 4.5262 - acc: 0.047 - ETA: 4s - loss: 4.5276 - acc: 0.046 - ETA: 4s - loss: 4.5258 - acc: 0.046 - ETA: 4s - loss: 4.5272 - acc: 0.045 - ETA: 4s - loss: 4.5274 - acc: 0.045 - ETA: 4s - loss: 4.5291 - acc: 0.045 - ETA: 4s - loss: 4.5282 - acc: 0.046 - ETA: 3s - loss: 4.5274 - acc: 0.046 - ETA: 3s - loss: 4.5243 - acc: 0.046 - ETA: 3s - loss: 4.5264 - acc: 0.045 - ETA: 3s - loss: 4.5268 - acc: 0.046 - ETA: 3s - loss: 4.5278 - acc: 0.046 - ETA: 3s - loss: 4.5293 - acc: 0.046 - ETA: 3s - loss: 4.5306 - acc: 0.045 - ETA: 3s - loss: 4.5302 - acc: 0.046 - ETA: 3s - loss: 4.5323 - acc: 0.046 - ETA: 3s - loss: 4.5333 - acc: 0.045 - ETA: 3s - loss: 4.5324 - acc: 0.046 - ETA: 3s - loss: 4.5315 - acc: 0.047 - ETA: 3s - loss: 4.5304 - acc: 0.047 - ETA: 3s - loss: 4.5300 - acc: 0.047 - ETA: 3s - loss: 4.5297 - acc: 0.047 - ETA: 3s - loss: 4.5302 - acc: 0.047 - ETA: 2s - loss: 4.5305 - acc: 0.047 - ETA: 2s - loss: 4.5306 - acc: 0.047 - ETA: 2s - loss: 4.5315 - acc: 0.047 - ETA: 2s - loss: 4.5310 - acc: 0.047 - ETA: 2s - loss: 4.5299 - acc: 0.047 - ETA: 2s - loss: 4.5311 - acc: 0.046 - ETA: 2s - loss: 4.5316 - acc: 0.046 - ETA: 2s - loss: 4.5316 - acc: 0.045 - ETA: 2s - loss: 4.5326 - acc: 0.045 - ETA: 2s - loss: 4.5342 - acc: 0.046 - ETA: 2s - loss: 4.5337 - acc: 0.045 - ETA: 2s - loss: 4.5359 - acc: 0.045 - ETA: 2s - loss: 4.5357 - acc: 0.045 - ETA: 2s - loss: 4.5370 - acc: 0.045 - ETA: 2s - loss: 4.5391 - acc: 0.045 - ETA: 1s - loss: 4.5405 - acc: 0.044 - ETA: 1s - loss: 4.5397 - acc: 0.044 - ETA: 1s - loss: 4.5387 - acc: 0.045 - ETA: 1s - loss: 4.5380 - acc: 0.045 - ETA: 1s - loss: 4.5377 - acc: 0.045 - ETA: 1s - loss: 4.5374 - acc: 0.045 - ETA: 1s - loss: 4.5382 - acc: 0.046 - ETA: 1s - loss: 4.5394 - acc: 0.045 - ETA: 1s - loss: 4.5393 - acc: 0.045 - ETA: 1s - loss: 4.5397 - acc: 0.045 - ETA: 1s - loss: 4.5410 - acc: 0.045 - ETA: 1s - loss: 4.5405 - acc: 0.046 - ETA: 1s - loss: 4.5409 - acc: 0.046 - ETA: 1s - loss: 4.5402 - acc: 0.046 - ETA: 1s - loss: 4.5418 - acc: 0.046 - ETA: 0s - loss: 4.5431 - acc: 0.046 - ETA: 0s - loss: 4.5440 - acc: 0.046 - ETA: 0s - loss: 4.5442 - acc: 0.046 - ETA: 0s - loss: 4.5448 - acc: 0.047 - ETA: 0s - loss: 4.5456 - acc: 0.046 - ETA: 0s - loss: 4.5439 - acc: 0.047 - ETA: 0s - loss: 4.5423 - acc: 0.046 - ETA: 0s - loss: 4.5425 - acc: 0.046 - ETA: 0s - loss: 4.5441 - acc: 0.046 - ETA: 0s - loss: 4.5434 - acc: 0.046 - ETA: 0s - loss: 4.5448 - acc: 0.046 - ETA: 0s - loss: 4.5459 - acc: 0.046 - ETA: 0s - loss: 4.5461 - acc: 0.045 - ETA: 0s - loss: 4.5460 - acc: 0.045 - ETA: 0s - loss: 4.5463 - acc: 0.045 - 8s 1ms/step - loss: 4.5461 - acc: 0.0457 - val_loss: 4.6120 - val_acc: 0.0323\n",
      "\n",
      "Epoch 00015: val_loss improved from 4.64511 to 4.61203, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 4.3213 - acc: 0.100 - ETA: 7s - loss: 4.3754 - acc: 0.062 - ETA: 6s - loss: 4.4224 - acc: 0.071 - ETA: 6s - loss: 4.4561 - acc: 0.055 - ETA: 6s - loss: 4.4537 - acc: 0.069 - ETA: 6s - loss: 4.4643 - acc: 0.065 - ETA: 6s - loss: 4.4649 - acc: 0.060 - ETA: 6s - loss: 4.4954 - acc: 0.056 - ETA: 6s - loss: 4.5020 - acc: 0.056 - ETA: 6s - loss: 4.5241 - acc: 0.057 - ETA: 6s - loss: 4.5360 - acc: 0.054 - ETA: 6s - loss: 4.5434 - acc: 0.051 - ETA: 6s - loss: 4.5394 - acc: 0.048 - ETA: 6s - loss: 4.5577 - acc: 0.046 - ETA: 6s - loss: 4.5513 - acc: 0.045 - ETA: 6s - loss: 4.5370 - acc: 0.047 - ETA: 6s - loss: 4.5197 - acc: 0.050 - ETA: 6s - loss: 4.5218 - acc: 0.048 - ETA: 6s - loss: 4.5283 - acc: 0.046 - ETA: 5s - loss: 4.5288 - acc: 0.044 - ETA: 5s - loss: 4.5241 - acc: 0.045 - ETA: 5s - loss: 4.5178 - acc: 0.046 - ETA: 5s - loss: 4.5118 - acc: 0.046 - ETA: 5s - loss: 4.5089 - acc: 0.045 - ETA: 5s - loss: 4.5051 - acc: 0.046 - ETA: 5s - loss: 4.5174 - acc: 0.046 - ETA: 5s - loss: 4.5202 - acc: 0.046 - ETA: 5s - loss: 4.5150 - acc: 0.047 - ETA: 5s - loss: 4.5140 - acc: 0.048 - ETA: 5s - loss: 4.5174 - acc: 0.047 - ETA: 5s - loss: 4.5189 - acc: 0.049 - ETA: 5s - loss: 4.5200 - acc: 0.048 - ETA: 5s - loss: 4.5217 - acc: 0.047 - ETA: 5s - loss: 4.5154 - acc: 0.049 - ETA: 5s - loss: 4.5151 - acc: 0.048 - ETA: 4s - loss: 4.5191 - acc: 0.047 - ETA: 4s - loss: 4.5237 - acc: 0.047 - ETA: 4s - loss: 4.5172 - acc: 0.047 - ETA: 4s - loss: 4.5211 - acc: 0.047 - ETA: 4s - loss: 4.5210 - acc: 0.046 - ETA: 4s - loss: 4.5208 - acc: 0.045 - ETA: 4s - loss: 4.5227 - acc: 0.046 - ETA: 4s - loss: 4.5245 - acc: 0.045 - ETA: 4s - loss: 4.5258 - acc: 0.045 - ETA: 4s - loss: 4.5263 - acc: 0.046 - ETA: 4s - loss: 4.5278 - acc: 0.047 - ETA: 4s - loss: 4.5272 - acc: 0.047 - ETA: 4s - loss: 4.5284 - acc: 0.048 - ETA: 4s - loss: 4.5279 - acc: 0.047 - ETA: 4s - loss: 4.5283 - acc: 0.048 - ETA: 3s - loss: 4.5291 - acc: 0.048 - ETA: 3s - loss: 4.5303 - acc: 0.048 - ETA: 3s - loss: 4.5311 - acc: 0.047 - ETA: 3s - loss: 4.5318 - acc: 0.047 - ETA: 3s - loss: 4.5291 - acc: 0.048 - ETA: 3s - loss: 4.5263 - acc: 0.048 - ETA: 3s - loss: 4.5254 - acc: 0.049 - ETA: 3s - loss: 4.5252 - acc: 0.048 - ETA: 3s - loss: 4.5288 - acc: 0.048 - ETA: 3s - loss: 4.5303 - acc: 0.047 - ETA: 3s - loss: 4.5309 - acc: 0.047 - ETA: 3s - loss: 4.5302 - acc: 0.047 - ETA: 3s - loss: 4.5290 - acc: 0.048 - ETA: 3s - loss: 4.5303 - acc: 0.048 - ETA: 3s - loss: 4.5298 - acc: 0.049 - ETA: 2s - loss: 4.5301 - acc: 0.049 - ETA: 2s - loss: 4.5294 - acc: 0.050 - ETA: 2s - loss: 4.5293 - acc: 0.050 - ETA: 2s - loss: 4.5302 - acc: 0.050 - ETA: 2s - loss: 4.5322 - acc: 0.050 - ETA: 2s - loss: 4.5317 - acc: 0.050 - ETA: 2s - loss: 4.5317 - acc: 0.050 - ETA: 2s - loss: 4.5324 - acc: 0.050 - ETA: 2s - loss: 4.5312 - acc: 0.050 - ETA: 2s - loss: 4.5295 - acc: 0.050 - ETA: 2s - loss: 4.5275 - acc: 0.050 - ETA: 2s - loss: 4.5294 - acc: 0.049 - ETA: 2s - loss: 4.5320 - acc: 0.049 - ETA: 2s - loss: 4.5311 - acc: 0.049 - ETA: 2s - loss: 4.5310 - acc: 0.049 - ETA: 2s - loss: 4.5325 - acc: 0.049 - ETA: 1s - loss: 4.5310 - acc: 0.049 - ETA: 1s - loss: 4.5307 - acc: 0.049 - ETA: 1s - loss: 4.5282 - acc: 0.050 - ETA: 1s - loss: 4.5262 - acc: 0.050 - ETA: 1s - loss: 4.5263 - acc: 0.049 - ETA: 1s - loss: 4.5253 - acc: 0.049 - ETA: 1s - loss: 4.5263 - acc: 0.049 - ETA: 1s - loss: 4.5269 - acc: 0.049 - ETA: 1s - loss: 4.5285 - acc: 0.048 - ETA: 1s - loss: 4.5282 - acc: 0.049 - ETA: 1s - loss: 4.5284 - acc: 0.049 - ETA: 1s - loss: 4.5283 - acc: 0.049 - ETA: 1s - loss: 4.5261 - acc: 0.048 - ETA: 1s - loss: 4.5251 - acc: 0.049 - ETA: 1s - loss: 4.5237 - acc: 0.049 - ETA: 0s - loss: 4.5246 - acc: 0.049 - ETA: 0s - loss: 4.5243 - acc: 0.049 - ETA: 0s - loss: 4.5230 - acc: 0.049 - ETA: 0s - loss: 4.5214 - acc: 0.050 - ETA: 0s - loss: 4.5208 - acc: 0.050 - ETA: 0s - loss: 4.5196 - acc: 0.050 - ETA: 0s - loss: 4.5216 - acc: 0.050 - ETA: 0s - loss: 4.5207 - acc: 0.050 - ETA: 0s - loss: 4.5218 - acc: 0.050 - ETA: 0s - loss: 4.5218 - acc: 0.049 - ETA: 0s - loss: 4.5234 - acc: 0.049 - ETA: 0s - loss: 4.5237 - acc: 0.049 - ETA: 0s - loss: 4.5230 - acc: 0.049 - ETA: 0s - loss: 4.5234 - acc: 0.049 - ETA: 0s - loss: 4.5226 - acc: 0.049 - 8s 1ms/step - loss: 4.5217 - acc: 0.0494 - val_loss: 4.6294 - val_acc: 0.0419\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 4.61203\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 4.4673 - acc: 0.100 - ETA: 7s - loss: 4.5609 - acc: 0.050 - ETA: 6s - loss: 4.5633 - acc: 0.050 - ETA: 6s - loss: 4.5018 - acc: 0.040 - ETA: 6s - loss: 4.5226 - acc: 0.038 - ETA: 6s - loss: 4.4846 - acc: 0.040 - ETA: 6s - loss: 4.4864 - acc: 0.050 - ETA: 6s - loss: 4.4823 - acc: 0.050 - ETA: 6s - loss: 4.4945 - acc: 0.046 - ETA: 6s - loss: 4.4929 - acc: 0.044 - ETA: 6s - loss: 4.4926 - acc: 0.045 - ETA: 6s - loss: 4.4966 - acc: 0.042 - ETA: 6s - loss: 4.5018 - acc: 0.040 - ETA: 6s - loss: 4.4977 - acc: 0.041 - ETA: 6s - loss: 4.4937 - acc: 0.039 - ETA: 6s - loss: 4.4863 - acc: 0.043 - ETA: 6s - loss: 4.4833 - acc: 0.043 - ETA: 6s - loss: 4.4985 - acc: 0.046 - ETA: 6s - loss: 4.4941 - acc: 0.045 - ETA: 5s - loss: 4.4939 - acc: 0.043 - ETA: 5s - loss: 4.4916 - acc: 0.044 - ETA: 5s - loss: 4.4895 - acc: 0.045 - ETA: 5s - loss: 4.4894 - acc: 0.046 - ETA: 5s - loss: 4.4903 - acc: 0.045 - ETA: 5s - loss: 4.4950 - acc: 0.045 - ETA: 5s - loss: 4.4916 - acc: 0.047 - ETA: 5s - loss: 4.4880 - acc: 0.048 - ETA: 5s - loss: 4.4870 - acc: 0.048 - ETA: 5s - loss: 4.4875 - acc: 0.047 - ETA: 5s - loss: 4.4913 - acc: 0.049 - ETA: 5s - loss: 4.4891 - acc: 0.048 - ETA: 5s - loss: 4.4887 - acc: 0.048 - ETA: 5s - loss: 4.4910 - acc: 0.047 - ETA: 5s - loss: 4.4905 - acc: 0.047 - ETA: 4s - loss: 4.4910 - acc: 0.047 - ETA: 4s - loss: 4.4916 - acc: 0.046 - ETA: 4s - loss: 4.4962 - acc: 0.047 - ETA: 4s - loss: 4.4958 - acc: 0.048 - ETA: 4s - loss: 4.4909 - acc: 0.049 - ETA: 4s - loss: 4.4895 - acc: 0.049 - ETA: 4s - loss: 4.4943 - acc: 0.050 - ETA: 4s - loss: 4.4918 - acc: 0.050 - ETA: 4s - loss: 4.4938 - acc: 0.051 - ETA: 4s - loss: 4.4941 - acc: 0.050 - ETA: 4s - loss: 4.4942 - acc: 0.050 - ETA: 4s - loss: 4.4944 - acc: 0.049 - ETA: 4s - loss: 4.4948 - acc: 0.048 - ETA: 4s - loss: 4.4962 - acc: 0.048 - ETA: 4s - loss: 4.4962 - acc: 0.048 - ETA: 4s - loss: 4.4987 - acc: 0.048 - ETA: 3s - loss: 4.4980 - acc: 0.048 - ETA: 3s - loss: 4.5009 - acc: 0.048 - ETA: 3s - loss: 4.5006 - acc: 0.048 - ETA: 3s - loss: 4.5041 - acc: 0.048 - ETA: 3s - loss: 4.5045 - acc: 0.048 - ETA: 3s - loss: 4.5043 - acc: 0.049 - ETA: 3s - loss: 4.5048 - acc: 0.048 - ETA: 3s - loss: 4.5070 - acc: 0.048 - ETA: 3s - loss: 4.5052 - acc: 0.049 - ETA: 3s - loss: 4.5035 - acc: 0.050 - ETA: 3s - loss: 4.5034 - acc: 0.049 - ETA: 3s - loss: 4.5026 - acc: 0.049 - ETA: 3s - loss: 4.5051 - acc: 0.048 - ETA: 3s - loss: 4.5062 - acc: 0.048 - ETA: 3s - loss: 4.5066 - acc: 0.048 - ETA: 2s - loss: 4.5072 - acc: 0.049 - ETA: 2s - loss: 4.5045 - acc: 0.049 - ETA: 2s - loss: 4.5019 - acc: 0.049 - ETA: 2s - loss: 4.5009 - acc: 0.050 - ETA: 2s - loss: 4.4997 - acc: 0.049 - ETA: 2s - loss: 4.4976 - acc: 0.050 - ETA: 2s - loss: 4.4992 - acc: 0.050 - ETA: 2s - loss: 4.5001 - acc: 0.051 - ETA: 2s - loss: 4.4986 - acc: 0.051 - ETA: 2s - loss: 4.4965 - acc: 0.051 - ETA: 2s - loss: 4.4983 - acc: 0.051 - ETA: 2s - loss: 4.4986 - acc: 0.050 - ETA: 2s - loss: 4.4975 - acc: 0.050 - ETA: 2s - loss: 4.4979 - acc: 0.050 - ETA: 2s - loss: 4.4978 - acc: 0.050 - ETA: 2s - loss: 4.4995 - acc: 0.050 - ETA: 1s - loss: 4.4994 - acc: 0.050 - ETA: 1s - loss: 4.4987 - acc: 0.049 - ETA: 1s - loss: 4.4984 - acc: 0.049 - ETA: 1s - loss: 4.5000 - acc: 0.049 - ETA: 1s - loss: 4.5005 - acc: 0.049 - ETA: 1s - loss: 4.5027 - acc: 0.049 - ETA: 1s - loss: 4.5020 - acc: 0.049 - ETA: 1s - loss: 4.5008 - acc: 0.049 - ETA: 1s - loss: 4.5025 - acc: 0.048 - ETA: 1s - loss: 4.5043 - acc: 0.048 - ETA: 1s - loss: 4.5049 - acc: 0.048 - ETA: 1s - loss: 4.5053 - acc: 0.048 - ETA: 1s - loss: 4.5051 - acc: 0.048 - ETA: 1s - loss: 4.5023 - acc: 0.049 - ETA: 1s - loss: 4.5012 - acc: 0.050 - ETA: 0s - loss: 4.5001 - acc: 0.050 - ETA: 0s - loss: 4.4985 - acc: 0.050 - ETA: 0s - loss: 4.4980 - acc: 0.050 - ETA: 0s - loss: 4.4972 - acc: 0.050 - ETA: 0s - loss: 4.4978 - acc: 0.050 - ETA: 0s - loss: 4.4981 - acc: 0.050 - ETA: 0s - loss: 4.4989 - acc: 0.049 - ETA: 0s - loss: 4.4999 - acc: 0.049 - ETA: 0s - loss: 4.5000 - acc: 0.049 - ETA: 0s - loss: 4.4987 - acc: 0.049 - ETA: 0s - loss: 4.4990 - acc: 0.049 - ETA: 0s - loss: 4.4997 - acc: 0.048 - ETA: 0s - loss: 4.4993 - acc: 0.048 - ETA: 0s - loss: 4.4979 - acc: 0.048 - ETA: 0s - loss: 4.4989 - acc: 0.048 - 8s 1ms/step - loss: 4.4983 - acc: 0.0482 - val_loss: 4.6040 - val_acc: 0.0359\n",
      "\n",
      "Epoch 00017: val_loss improved from 4.61203 to 4.60399, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 4.7657 - acc: 0.100 - ETA: 7s - loss: 4.5570 - acc: 0.050 - ETA: 7s - loss: 4.4882 - acc: 0.035 - ETA: 6s - loss: 4.4820 - acc: 0.040 - ETA: 6s - loss: 4.4621 - acc: 0.042 - ETA: 6s - loss: 4.4633 - acc: 0.050 - ETA: 6s - loss: 4.4366 - acc: 0.052 - ETA: 6s - loss: 4.4288 - acc: 0.056 - ETA: 6s - loss: 4.4306 - acc: 0.052 - ETA: 6s - loss: 4.4413 - acc: 0.048 - ETA: 6s - loss: 4.4410 - acc: 0.050 - ETA: 6s - loss: 4.4567 - acc: 0.047 - ETA: 6s - loss: 4.4508 - acc: 0.048 - ETA: 6s - loss: 4.4443 - acc: 0.051 - ETA: 6s - loss: 4.4447 - acc: 0.054 - ETA: 6s - loss: 4.4425 - acc: 0.054 - ETA: 6s - loss: 4.4444 - acc: 0.055 - ETA: 6s - loss: 4.4447 - acc: 0.057 - ETA: 6s - loss: 4.4490 - acc: 0.057 - ETA: 5s - loss: 4.4417 - acc: 0.057 - ETA: 5s - loss: 4.4442 - acc: 0.059 - ETA: 5s - loss: 4.4450 - acc: 0.057 - ETA: 5s - loss: 4.4507 - acc: 0.058 - ETA: 5s - loss: 4.4508 - acc: 0.060 - ETA: 5s - loss: 4.4502 - acc: 0.060 - ETA: 5s - loss: 4.4522 - acc: 0.061 - ETA: 5s - loss: 4.4491 - acc: 0.060 - ETA: 5s - loss: 4.4513 - acc: 0.061 - ETA: 5s - loss: 4.4522 - acc: 0.060 - ETA: 5s - loss: 4.4523 - acc: 0.060 - ETA: 5s - loss: 4.4563 - acc: 0.059 - ETA: 5s - loss: 4.4615 - acc: 0.058 - ETA: 5s - loss: 4.4670 - acc: 0.057 - ETA: 5s - loss: 4.4750 - acc: 0.056 - ETA: 4s - loss: 4.4744 - acc: 0.056 - ETA: 4s - loss: 4.4744 - acc: 0.056 - ETA: 4s - loss: 4.4780 - acc: 0.055 - ETA: 4s - loss: 4.4749 - acc: 0.055 - ETA: 4s - loss: 4.4740 - acc: 0.055 - ETA: 4s - loss: 4.4716 - acc: 0.057 - ETA: 4s - loss: 4.4725 - acc: 0.057 - ETA: 4s - loss: 4.4745 - acc: 0.056 - ETA: 4s - loss: 4.4722 - acc: 0.056 - ETA: 4s - loss: 4.4774 - acc: 0.056 - ETA: 4s - loss: 4.4783 - acc: 0.056 - ETA: 4s - loss: 4.4752 - acc: 0.057 - ETA: 4s - loss: 4.4745 - acc: 0.059 - ETA: 4s - loss: 4.4712 - acc: 0.059 - ETA: 4s - loss: 4.4723 - acc: 0.059 - ETA: 4s - loss: 4.4738 - acc: 0.059 - ETA: 3s - loss: 4.4715 - acc: 0.059 - ETA: 3s - loss: 4.4704 - acc: 0.059 - ETA: 3s - loss: 4.4670 - acc: 0.059 - ETA: 3s - loss: 4.4700 - acc: 0.058 - ETA: 3s - loss: 4.4713 - acc: 0.058 - ETA: 3s - loss: 4.4752 - acc: 0.057 - ETA: 3s - loss: 4.4778 - acc: 0.058 - ETA: 3s - loss: 4.4787 - acc: 0.058 - ETA: 3s - loss: 4.4769 - acc: 0.058 - ETA: 3s - loss: 4.4747 - acc: 0.059 - ETA: 3s - loss: 4.4770 - acc: 0.059 - ETA: 3s - loss: 4.4793 - acc: 0.058 - ETA: 3s - loss: 4.4792 - acc: 0.058 - ETA: 3s - loss: 4.4816 - acc: 0.058 - ETA: 3s - loss: 4.4819 - acc: 0.058 - ETA: 2s - loss: 4.4835 - acc: 0.057 - ETA: 2s - loss: 4.4786 - acc: 0.058 - ETA: 2s - loss: 4.4786 - acc: 0.058 - ETA: 2s - loss: 4.4766 - acc: 0.058 - ETA: 2s - loss: 4.4776 - acc: 0.058 - ETA: 2s - loss: 4.4767 - acc: 0.058 - ETA: 2s - loss: 4.4791 - acc: 0.057 - ETA: 2s - loss: 4.4808 - acc: 0.057 - ETA: 2s - loss: 4.4812 - acc: 0.057 - ETA: 2s - loss: 4.4810 - acc: 0.057 - ETA: 2s - loss: 4.4794 - acc: 0.058 - ETA: 2s - loss: 4.4779 - acc: 0.058 - ETA: 2s - loss: 4.4778 - acc: 0.059 - ETA: 2s - loss: 4.4784 - acc: 0.058 - ETA: 2s - loss: 4.4777 - acc: 0.058 - ETA: 2s - loss: 4.4762 - acc: 0.058 - ETA: 1s - loss: 4.4797 - acc: 0.058 - ETA: 1s - loss: 4.4794 - acc: 0.058 - ETA: 1s - loss: 4.4786 - acc: 0.058 - ETA: 1s - loss: 4.4780 - acc: 0.058 - ETA: 1s - loss: 4.4796 - acc: 0.058 - ETA: 1s - loss: 4.4794 - acc: 0.058 - ETA: 1s - loss: 4.4811 - acc: 0.058 - ETA: 1s - loss: 4.4820 - acc: 0.058 - ETA: 1s - loss: 4.4814 - acc: 0.058 - ETA: 1s - loss: 4.4806 - acc: 0.058 - ETA: 1s - loss: 4.4826 - acc: 0.057 - ETA: 1s - loss: 4.4817 - acc: 0.057 - ETA: 1s - loss: 4.4794 - acc: 0.057 - ETA: 1s - loss: 4.4780 - acc: 0.057 - ETA: 1s - loss: 4.4776 - acc: 0.057 - ETA: 0s - loss: 4.4781 - acc: 0.057 - ETA: 0s - loss: 4.4772 - acc: 0.057 - ETA: 0s - loss: 4.4780 - acc: 0.057 - ETA: 0s - loss: 4.4788 - acc: 0.057 - ETA: 0s - loss: 4.4792 - acc: 0.057 - ETA: 0s - loss: 4.4793 - acc: 0.057 - ETA: 0s - loss: 4.4800 - acc: 0.056 - ETA: 0s - loss: 4.4804 - acc: 0.056 - ETA: 0s - loss: 4.4815 - acc: 0.056 - ETA: 0s - loss: 4.4806 - acc: 0.055 - ETA: 0s - loss: 4.4796 - acc: 0.055 - ETA: 0s - loss: 4.4801 - acc: 0.055 - ETA: 0s - loss: 4.4805 - acc: 0.056 - ETA: 0s - loss: 4.4806 - acc: 0.055 - ETA: 0s - loss: 4.4808 - acc: 0.055 - 8s 1ms/step - loss: 4.4808 - acc: 0.0561 - val_loss: 4.5726 - val_acc: 0.0467\n",
      "\n",
      "Epoch 00018: val_loss improved from 4.60399 to 4.57261, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 4.6929 - acc: 0.050 - ETA: 6s - loss: 4.5454 - acc: 0.087 - ETA: 6s - loss: 4.6151 - acc: 0.050 - ETA: 6s - loss: 4.6337 - acc: 0.045 - ETA: 6s - loss: 4.6007 - acc: 0.057 - ETA: 6s - loss: 4.6004 - acc: 0.059 - ETA: 6s - loss: 4.5739 - acc: 0.055 - ETA: 6s - loss: 4.5743 - acc: 0.047 - ETA: 6s - loss: 4.5347 - acc: 0.052 - ETA: 6s - loss: 4.5170 - acc: 0.053 - ETA: 6s - loss: 4.5230 - acc: 0.048 - ETA: 6s - loss: 4.5091 - acc: 0.057 - ETA: 6s - loss: 4.5135 - acc: 0.056 - ETA: 6s - loss: 4.5169 - acc: 0.055 - ETA: 6s - loss: 4.5145 - acc: 0.055 - ETA: 6s - loss: 4.5107 - acc: 0.055 - ETA: 6s - loss: 4.5116 - acc: 0.056 - ETA: 6s - loss: 4.5070 - acc: 0.055 - ETA: 6s - loss: 4.5029 - acc: 0.052 - ETA: 5s - loss: 4.4922 - acc: 0.055 - ETA: 5s - loss: 4.4883 - acc: 0.056 - ETA: 5s - loss: 4.4818 - acc: 0.055 - ETA: 5s - loss: 4.4811 - acc: 0.053 - ETA: 5s - loss: 4.4780 - acc: 0.054 - ETA: 5s - loss: 4.4783 - acc: 0.054 - ETA: 5s - loss: 4.4725 - acc: 0.055 - ETA: 5s - loss: 4.4747 - acc: 0.055 - ETA: 5s - loss: 4.4774 - acc: 0.053 - ETA: 5s - loss: 4.4803 - acc: 0.054 - ETA: 5s - loss: 4.4743 - acc: 0.055 - ETA: 5s - loss: 4.4748 - acc: 0.054 - ETA: 5s - loss: 4.4745 - acc: 0.054 - ETA: 5s - loss: 4.4770 - acc: 0.054 - ETA: 5s - loss: 4.4751 - acc: 0.054 - ETA: 4s - loss: 4.4685 - acc: 0.053 - ETA: 4s - loss: 4.4695 - acc: 0.053 - ETA: 4s - loss: 4.4708 - acc: 0.052 - ETA: 4s - loss: 4.4740 - acc: 0.052 - ETA: 4s - loss: 4.4720 - acc: 0.053 - ETA: 4s - loss: 4.4722 - acc: 0.053 - ETA: 4s - loss: 4.4738 - acc: 0.053 - ETA: 4s - loss: 4.4774 - acc: 0.054 - ETA: 4s - loss: 4.4809 - acc: 0.053 - ETA: 4s - loss: 4.4804 - acc: 0.053 - ETA: 4s - loss: 4.4807 - acc: 0.053 - ETA: 4s - loss: 4.4803 - acc: 0.053 - ETA: 4s - loss: 4.4755 - acc: 0.054 - ETA: 4s - loss: 4.4757 - acc: 0.053 - ETA: 4s - loss: 4.4761 - acc: 0.053 - ETA: 4s - loss: 4.4745 - acc: 0.054 - ETA: 3s - loss: 4.4770 - acc: 0.054 - ETA: 3s - loss: 4.4778 - acc: 0.054 - ETA: 3s - loss: 4.4757 - acc: 0.055 - ETA: 3s - loss: 4.4760 - acc: 0.054 - ETA: 3s - loss: 4.4791 - acc: 0.054 - ETA: 3s - loss: 4.4787 - acc: 0.053 - ETA: 3s - loss: 4.4782 - acc: 0.053 - ETA: 3s - loss: 4.4784 - acc: 0.053 - ETA: 3s - loss: 4.4767 - acc: 0.053 - ETA: 3s - loss: 4.4759 - acc: 0.054 - ETA: 3s - loss: 4.4763 - acc: 0.055 - ETA: 3s - loss: 4.4733 - acc: 0.054 - ETA: 3s - loss: 4.4732 - acc: 0.054 - ETA: 3s - loss: 4.4724 - acc: 0.054 - ETA: 3s - loss: 4.4717 - acc: 0.053 - ETA: 2s - loss: 4.4721 - acc: 0.053 - ETA: 2s - loss: 4.4734 - acc: 0.054 - ETA: 2s - loss: 4.4737 - acc: 0.055 - ETA: 2s - loss: 4.4727 - acc: 0.055 - ETA: 2s - loss: 4.4698 - acc: 0.055 - ETA: 2s - loss: 4.4651 - acc: 0.056 - ETA: 2s - loss: 4.4648 - acc: 0.056 - ETA: 2s - loss: 4.4644 - acc: 0.056 - ETA: 2s - loss: 4.4643 - acc: 0.056 - ETA: 2s - loss: 4.4648 - acc: 0.056 - ETA: 2s - loss: 4.4656 - acc: 0.056 - ETA: 2s - loss: 4.4667 - acc: 0.057 - ETA: 2s - loss: 4.4669 - acc: 0.056 - ETA: 2s - loss: 4.4644 - acc: 0.056 - ETA: 2s - loss: 4.4641 - acc: 0.056 - ETA: 2s - loss: 4.4633 - acc: 0.056 - ETA: 1s - loss: 4.4653 - acc: 0.056 - ETA: 1s - loss: 4.4645 - acc: 0.056 - ETA: 1s - loss: 4.4639 - acc: 0.056 - ETA: 1s - loss: 4.4640 - acc: 0.056 - ETA: 1s - loss: 4.4666 - acc: 0.055 - ETA: 1s - loss: 4.4665 - acc: 0.055 - ETA: 1s - loss: 4.4652 - acc: 0.055 - ETA: 1s - loss: 4.4648 - acc: 0.055 - ETA: 1s - loss: 4.4645 - acc: 0.055 - ETA: 1s - loss: 4.4639 - acc: 0.055 - ETA: 1s - loss: 4.4646 - acc: 0.055 - ETA: 1s - loss: 4.4655 - acc: 0.055 - ETA: 1s - loss: 4.4649 - acc: 0.055 - ETA: 1s - loss: 4.4654 - acc: 0.055 - ETA: 1s - loss: 4.4633 - acc: 0.056 - ETA: 0s - loss: 4.4605 - acc: 0.056 - ETA: 0s - loss: 4.4627 - acc: 0.056 - ETA: 0s - loss: 4.4616 - acc: 0.056 - ETA: 0s - loss: 4.4610 - acc: 0.055 - ETA: 0s - loss: 4.4605 - acc: 0.056 - ETA: 0s - loss: 4.4594 - acc: 0.056 - ETA: 0s - loss: 4.4584 - acc: 0.056 - ETA: 0s - loss: 4.4579 - acc: 0.056 - ETA: 0s - loss: 4.4576 - acc: 0.056 - ETA: 0s - loss: 4.4576 - acc: 0.056 - ETA: 0s - loss: 4.4571 - acc: 0.056 - ETA: 0s - loss: 4.4559 - acc: 0.056 - ETA: 0s - loss: 4.4560 - acc: 0.055 - ETA: 0s - loss: 4.4568 - acc: 0.055 - ETA: 0s - loss: 4.4555 - acc: 0.055 - 8s 1ms/step - loss: 4.4550 - acc: 0.0560 - val_loss: 4.5388 - val_acc: 0.0455\n",
      "\n",
      "Epoch 00019: val_loss improved from 4.57261 to 4.53877, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 4.3949 - acc: 0.100 - ETA: 7s - loss: 4.2883 - acc: 0.075 - ETA: 7s - loss: 4.4111 - acc: 0.064 - ETA: 7s - loss: 4.4482 - acc: 0.065 - ETA: 6s - loss: 4.4049 - acc: 0.061 - ETA: 6s - loss: 4.3953 - acc: 0.062 - ETA: 6s - loss: 4.4365 - acc: 0.057 - ETA: 6s - loss: 4.4618 - acc: 0.054 - ETA: 6s - loss: 4.4466 - acc: 0.058 - ETA: 6s - loss: 4.4465 - acc: 0.058 - ETA: 6s - loss: 4.4313 - acc: 0.061 - ETA: 6s - loss: 4.4290 - acc: 0.061 - ETA: 6s - loss: 4.4250 - acc: 0.060 - ETA: 6s - loss: 4.4263 - acc: 0.060 - ETA: 6s - loss: 4.4332 - acc: 0.057 - ETA: 6s - loss: 4.4213 - acc: 0.059 - ETA: 6s - loss: 4.4222 - acc: 0.058 - ETA: 6s - loss: 4.4171 - acc: 0.056 - ETA: 6s - loss: 4.4136 - acc: 0.055 - ETA: 5s - loss: 4.4182 - acc: 0.056 - ETA: 5s - loss: 4.4154 - acc: 0.058 - ETA: 5s - loss: 4.4112 - acc: 0.057 - ETA: 5s - loss: 4.4064 - acc: 0.057 - ETA: 5s - loss: 4.4142 - acc: 0.056 - ETA: 5s - loss: 4.4159 - acc: 0.057 - ETA: 5s - loss: 4.4140 - acc: 0.057 - ETA: 5s - loss: 4.4197 - acc: 0.058 - ETA: 5s - loss: 4.4194 - acc: 0.057 - ETA: 5s - loss: 4.4281 - acc: 0.057 - ETA: 5s - loss: 4.4260 - acc: 0.056 - ETA: 5s - loss: 4.4312 - acc: 0.056 - ETA: 5s - loss: 4.4329 - acc: 0.056 - ETA: 5s - loss: 4.4350 - acc: 0.057 - ETA: 5s - loss: 4.4355 - acc: 0.057 - ETA: 5s - loss: 4.4354 - acc: 0.056 - ETA: 4s - loss: 4.4392 - acc: 0.056 - ETA: 4s - loss: 4.4401 - acc: 0.056 - ETA: 4s - loss: 4.4451 - acc: 0.056 - ETA: 4s - loss: 4.4444 - acc: 0.057 - ETA: 4s - loss: 4.4484 - acc: 0.058 - ETA: 4s - loss: 4.4469 - acc: 0.058 - ETA: 4s - loss: 4.4485 - acc: 0.058 - ETA: 4s - loss: 4.4467 - acc: 0.058 - ETA: 4s - loss: 4.4447 - acc: 0.058 - ETA: 4s - loss: 4.4478 - acc: 0.057 - ETA: 4s - loss: 4.4448 - acc: 0.057 - ETA: 4s - loss: 4.4463 - acc: 0.056 - ETA: 4s - loss: 4.4435 - acc: 0.057 - ETA: 4s - loss: 4.4423 - acc: 0.057 - ETA: 4s - loss: 4.4469 - acc: 0.056 - ETA: 3s - loss: 4.4468 - acc: 0.057 - ETA: 3s - loss: 4.4459 - acc: 0.056 - ETA: 3s - loss: 4.4475 - acc: 0.055 - ETA: 3s - loss: 4.4459 - acc: 0.056 - ETA: 3s - loss: 4.4462 - acc: 0.057 - ETA: 3s - loss: 4.4420 - acc: 0.058 - ETA: 3s - loss: 4.4402 - acc: 0.058 - ETA: 3s - loss: 4.4409 - acc: 0.058 - ETA: 3s - loss: 4.4432 - acc: 0.057 - ETA: 3s - loss: 4.4423 - acc: 0.057 - ETA: 3s - loss: 4.4442 - acc: 0.057 - ETA: 3s - loss: 4.4447 - acc: 0.057 - ETA: 3s - loss: 4.4470 - acc: 0.057 - ETA: 3s - loss: 4.4460 - acc: 0.057 - ETA: 3s - loss: 4.4440 - acc: 0.057 - ETA: 2s - loss: 4.4428 - acc: 0.057 - ETA: 2s - loss: 4.4417 - acc: 0.057 - ETA: 2s - loss: 4.4434 - acc: 0.057 - ETA: 2s - loss: 4.4435 - acc: 0.057 - ETA: 2s - loss: 4.4442 - acc: 0.057 - ETA: 2s - loss: 4.4445 - acc: 0.057 - ETA: 2s - loss: 4.4445 - acc: 0.057 - ETA: 2s - loss: 4.4416 - acc: 0.056 - ETA: 2s - loss: 4.4405 - acc: 0.056 - ETA: 2s - loss: 4.4406 - acc: 0.057 - ETA: 2s - loss: 4.4417 - acc: 0.056 - ETA: 2s - loss: 4.4385 - acc: 0.057 - ETA: 2s - loss: 4.4386 - acc: 0.058 - ETA: 2s - loss: 4.4412 - acc: 0.057 - ETA: 2s - loss: 4.4409 - acc: 0.057 - ETA: 2s - loss: 4.4396 - acc: 0.058 - ETA: 1s - loss: 4.4401 - acc: 0.058 - ETA: 1s - loss: 4.4397 - acc: 0.058 - ETA: 1s - loss: 4.4390 - acc: 0.059 - ETA: 1s - loss: 4.4392 - acc: 0.059 - ETA: 1s - loss: 4.4388 - acc: 0.059 - ETA: 1s - loss: 4.4396 - acc: 0.059 - ETA: 1s - loss: 4.4417 - acc: 0.059 - ETA: 1s - loss: 4.4428 - acc: 0.059 - ETA: 1s - loss: 4.4424 - acc: 0.059 - ETA: 1s - loss: 4.4440 - acc: 0.059 - ETA: 1s - loss: 4.4441 - acc: 0.059 - ETA: 1s - loss: 4.4422 - acc: 0.059 - ETA: 1s - loss: 4.4406 - acc: 0.059 - ETA: 1s - loss: 4.4402 - acc: 0.059 - ETA: 1s - loss: 4.4387 - acc: 0.059 - ETA: 0s - loss: 4.4380 - acc: 0.059 - ETA: 0s - loss: 4.4381 - acc: 0.059 - ETA: 0s - loss: 4.4386 - acc: 0.059 - ETA: 0s - loss: 4.4388 - acc: 0.059 - ETA: 0s - loss: 4.4388 - acc: 0.059 - ETA: 0s - loss: 4.4394 - acc: 0.059 - ETA: 0s - loss: 4.4378 - acc: 0.059 - ETA: 0s - loss: 4.4385 - acc: 0.059 - ETA: 0s - loss: 4.4378 - acc: 0.059 - ETA: 0s - loss: 4.4366 - acc: 0.059 - ETA: 0s - loss: 4.4355 - acc: 0.059 - ETA: 0s - loss: 4.4368 - acc: 0.059 - ETA: 0s - loss: 4.4360 - acc: 0.059 - ETA: 0s - loss: 4.4378 - acc: 0.059 - ETA: 0s - loss: 4.4395 - acc: 0.059 - 8s 1ms/step - loss: 4.4375 - acc: 0.0594 - val_loss: 4.5557 - val_acc: 0.0455\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 4.53877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2470d123390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 5.7416%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 2:17 - loss: 15.2162 - acc: 0.0000e+ - ETA: 9s - loss: 14.7372 - acc: 0.0094     - ETA: 4s - loss: 14.9358 - acc: 0.01 - ETA: 3s - loss: 14.7594 - acc: 0.01 - ETA: 2s - loss: 14.5343 - acc: 0.02 - ETA: 2s - loss: 14.2591 - acc: 0.02 - ETA: 1s - loss: 14.0011 - acc: 0.03 - ETA: 1s - loss: 13.8741 - acc: 0.03 - ETA: 1s - loss: 13.6997 - acc: 0.04 - ETA: 1s - loss: 13.5334 - acc: 0.05 - ETA: 1s - loss: 13.3935 - acc: 0.05 - ETA: 0s - loss: 13.2225 - acc: 0.06 - ETA: 0s - loss: 13.0726 - acc: 0.06 - ETA: 0s - loss: 12.9301 - acc: 0.07 - ETA: 0s - loss: 12.8272 - acc: 0.08 - ETA: 0s - loss: 12.7229 - acc: 0.08 - ETA: 0s - loss: 12.6218 - acc: 0.09 - ETA: 0s - loss: 12.4948 - acc: 0.09 - ETA: 0s - loss: 12.4146 - acc: 0.10 - ETA: 0s - loss: 12.3087 - acc: 0.10 - ETA: 0s - loss: 12.1672 - acc: 0.11 - ETA: 0s - loss: 12.1089 - acc: 0.11 - 2s 252us/step - loss: 12.0473 - acc: 0.1211 - val_loss: 10.1125 - val_acc: 0.2515\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.11250, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 11.9336 - acc: 0.25 - ETA: 1s - loss: 10.6445 - acc: 0.26 - ETA: 1s - loss: 10.4387 - acc: 0.26 - ETA: 0s - loss: 10.0995 - acc: 0.27 - ETA: 0s - loss: 9.7907 - acc: 0.2802 - ETA: 0s - loss: 9.6700 - acc: 0.289 - ETA: 0s - loss: 9.4562 - acc: 0.304 - ETA: 0s - loss: 9.4599 - acc: 0.302 - ETA: 0s - loss: 9.4568 - acc: 0.305 - ETA: 0s - loss: 9.5129 - acc: 0.302 - ETA: 0s - loss: 9.4173 - acc: 0.305 - ETA: 0s - loss: 9.4028 - acc: 0.306 - ETA: 0s - loss: 9.4263 - acc: 0.304 - ETA: 0s - loss: 9.4176 - acc: 0.305 - ETA: 0s - loss: 9.4224 - acc: 0.305 - ETA: 0s - loss: 9.4544 - acc: 0.305 - ETA: 0s - loss: 9.4039 - acc: 0.307 - ETA: 0s - loss: 9.3977 - acc: 0.307 - ETA: 0s - loss: 9.3839 - acc: 0.310 - ETA: 0s - loss: 9.3542 - acc: 0.312 - ETA: 0s - loss: 9.3594 - acc: 0.311 - ETA: 0s - loss: 9.3435 - acc: 0.312 - 1s 178us/step - loss: 9.3288 - acc: 0.3139 - val_loss: 9.2571 - val_acc: 0.3246\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.11250 to 9.25711, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 8.9871 - acc: 0.400 - ETA: 1s - loss: 9.4114 - acc: 0.350 - ETA: 1s - loss: 9.1876 - acc: 0.364 - ETA: 0s - loss: 9.0004 - acc: 0.375 - ETA: 0s - loss: 8.9691 - acc: 0.379 - ETA: 0s - loss: 8.8927 - acc: 0.378 - ETA: 0s - loss: 8.9267 - acc: 0.377 - ETA: 0s - loss: 8.9281 - acc: 0.378 - ETA: 0s - loss: 8.8962 - acc: 0.379 - ETA: 0s - loss: 8.8509 - acc: 0.383 - ETA: 0s - loss: 8.7472 - acc: 0.388 - ETA: 0s - loss: 8.6984 - acc: 0.390 - ETA: 0s - loss: 8.7323 - acc: 0.389 - ETA: 0s - loss: 8.7761 - acc: 0.386 - ETA: 0s - loss: 8.8143 - acc: 0.384 - ETA: 0s - loss: 8.7891 - acc: 0.386 - ETA: 0s - loss: 8.8042 - acc: 0.385 - ETA: 0s - loss: 8.7160 - acc: 0.390 - ETA: 0s - loss: 8.7106 - acc: 0.391 - ETA: 0s - loss: 8.7149 - acc: 0.392 - ETA: 0s - loss: 8.7289 - acc: 0.391 - ETA: 0s - loss: 8.7294 - acc: 0.391 - 1s 178us/step - loss: 8.7189 - acc: 0.3916 - val_loss: 8.9235 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.25711 to 8.92348, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 6.9568 - acc: 0.500 - ETA: 1s - loss: 8.0910 - acc: 0.443 - ETA: 1s - loss: 7.9737 - acc: 0.448 - ETA: 0s - loss: 8.3627 - acc: 0.425 - ETA: 0s - loss: 8.2621 - acc: 0.435 - ETA: 0s - loss: 8.2713 - acc: 0.432 - ETA: 0s - loss: 8.2203 - acc: 0.434 - ETA: 0s - loss: 8.2934 - acc: 0.428 - ETA: 0s - loss: 8.2649 - acc: 0.430 - ETA: 0s - loss: 8.2806 - acc: 0.431 - ETA: 0s - loss: 8.3438 - acc: 0.428 - ETA: 0s - loss: 8.2920 - acc: 0.431 - ETA: 0s - loss: 8.2738 - acc: 0.431 - ETA: 0s - loss: 8.2969 - acc: 0.430 - ETA: 0s - loss: 8.2759 - acc: 0.431 - ETA: 0s - loss: 8.3035 - acc: 0.429 - ETA: 0s - loss: 8.2970 - acc: 0.430 - ETA: 0s - loss: 8.3278 - acc: 0.427 - ETA: 0s - loss: 8.3044 - acc: 0.428 - ETA: 0s - loss: 8.3186 - acc: 0.427 - ETA: 0s - loss: 8.3714 - acc: 0.424 - ETA: 0s - loss: 8.3290 - acc: 0.427 - 1s 181us/step - loss: 8.3046 - acc: 0.4286 - val_loss: 8.6485 - val_acc: 0.3725\n",
      "\n",
      "Epoch 00004: val_loss improved from 8.92348 to 8.64853, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 5.9775 - acc: 0.600 - ETA: 1s - loss: 8.1267 - acc: 0.446 - ETA: 1s - loss: 7.5746 - acc: 0.482 - ETA: 0s - loss: 7.8587 - acc: 0.469 - ETA: 0s - loss: 8.1039 - acc: 0.455 - ETA: 0s - loss: 8.0661 - acc: 0.460 - ETA: 0s - loss: 8.1312 - acc: 0.455 - ETA: 0s - loss: 8.1225 - acc: 0.455 - ETA: 0s - loss: 8.0476 - acc: 0.460 - ETA: 0s - loss: 8.0694 - acc: 0.460 - ETA: 0s - loss: 8.0517 - acc: 0.460 - ETA: 0s - loss: 8.0925 - acc: 0.456 - ETA: 0s - loss: 8.0801 - acc: 0.457 - ETA: 0s - loss: 8.0327 - acc: 0.459 - ETA: 0s - loss: 8.0276 - acc: 0.457 - ETA: 0s - loss: 8.0616 - acc: 0.456 - ETA: 0s - loss: 8.0269 - acc: 0.457 - ETA: 0s - loss: 7.9642 - acc: 0.462 - ETA: 0s - loss: 7.9602 - acc: 0.462 - ETA: 0s - loss: 7.9656 - acc: 0.461 - ETA: 0s - loss: 7.9626 - acc: 0.462 - ETA: 0s - loss: 7.9999 - acc: 0.458 - ETA: 0s - loss: 8.0272 - acc: 0.456 - 1s 185us/step - loss: 8.0107 - acc: 0.4572 - val_loss: 8.4433 - val_acc: 0.3904\n",
      "\n",
      "Epoch 00005: val_loss improved from 8.64853 to 8.44335, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 8.9784 - acc: 0.400 - ETA: 1s - loss: 7.6577 - acc: 0.506 - ETA: 1s - loss: 7.8071 - acc: 0.496 - ETA: 0s - loss: 7.7318 - acc: 0.493 - ETA: 0s - loss: 7.6783 - acc: 0.492 - ETA: 0s - loss: 7.6581 - acc: 0.492 - ETA: 0s - loss: 7.5355 - acc: 0.501 - ETA: 0s - loss: 7.5886 - acc: 0.494 - ETA: 0s - loss: 7.5515 - acc: 0.495 - ETA: 0s - loss: 7.5269 - acc: 0.496 - ETA: 0s - loss: 7.5733 - acc: 0.493 - ETA: 0s - loss: 7.5977 - acc: 0.492 - ETA: 0s - loss: 7.5873 - acc: 0.492 - ETA: 0s - loss: 7.5980 - acc: 0.492 - ETA: 0s - loss: 7.5879 - acc: 0.493 - ETA: 0s - loss: 7.6321 - acc: 0.489 - ETA: 0s - loss: 7.6438 - acc: 0.487 - ETA: 0s - loss: 7.6632 - acc: 0.486 - ETA: 0s - loss: 7.6920 - acc: 0.484 - ETA: 0s - loss: 7.6969 - acc: 0.483 - ETA: 0s - loss: 7.6974 - acc: 0.483 - ETA: 0s - loss: 7.7003 - acc: 0.482 - 1s 183us/step - loss: 7.7018 - acc: 0.4828 - val_loss: 8.1953 - val_acc: 0.4108\n",
      "\n",
      "Epoch 00006: val_loss improved from 8.44335 to 8.19529, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 7.9102 - acc: 0.400 - ETA: 1s - loss: 7.3691 - acc: 0.506 - ETA: 1s - loss: 7.6361 - acc: 0.493 - ETA: 0s - loss: 7.3907 - acc: 0.508 - ETA: 0s - loss: 7.4164 - acc: 0.504 - ETA: 0s - loss: 7.3442 - acc: 0.507 - ETA: 0s - loss: 7.3533 - acc: 0.506 - ETA: 0s - loss: 7.5042 - acc: 0.499 - ETA: 0s - loss: 7.5569 - acc: 0.496 - ETA: 0s - loss: 7.5045 - acc: 0.500 - ETA: 0s - loss: 7.4715 - acc: 0.501 - ETA: 0s - loss: 7.4541 - acc: 0.502 - ETA: 0s - loss: 7.4830 - acc: 0.500 - ETA: 0s - loss: 7.4978 - acc: 0.498 - ETA: 0s - loss: 7.4341 - acc: 0.503 - ETA: 0s - loss: 7.4176 - acc: 0.503 - ETA: 0s - loss: 7.3747 - acc: 0.505 - ETA: 0s - loss: 7.4302 - acc: 0.502 - ETA: 0s - loss: 7.4226 - acc: 0.502 - ETA: 0s - loss: 7.4366 - acc: 0.500 - ETA: 0s - loss: 7.4401 - acc: 0.500 - ETA: 0s - loss: 7.4209 - acc: 0.502 - ETA: 0s - loss: 7.3953 - acc: 0.503 - 1s 186us/step - loss: 7.3959 - acc: 0.5036 - val_loss: 7.9457 - val_acc: 0.4228\n",
      "\n",
      "Epoch 00007: val_loss improved from 8.19529 to 7.94569, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 0s - loss: 7.3125 - acc: 0.450 - ETA: 1s - loss: 6.9924 - acc: 0.535 - ETA: 1s - loss: 6.9843 - acc: 0.535 - ETA: 1s - loss: 6.8230 - acc: 0.545 - ETA: 0s - loss: 7.0025 - acc: 0.534 - ETA: 0s - loss: 7.1551 - acc: 0.527 - ETA: 0s - loss: 7.2315 - acc: 0.525 - ETA: 0s - loss: 7.3031 - acc: 0.518 - ETA: 0s - loss: 7.2750 - acc: 0.520 - ETA: 0s - loss: 7.3211 - acc: 0.515 - ETA: 0s - loss: 7.3240 - acc: 0.515 - ETA: 0s - loss: 7.2596 - acc: 0.518 - ETA: 0s - loss: 7.2281 - acc: 0.519 - ETA: 0s - loss: 7.2203 - acc: 0.519 - ETA: 0s - loss: 7.2135 - acc: 0.521 - ETA: 0s - loss: 7.1980 - acc: 0.521 - ETA: 0s - loss: 7.2374 - acc: 0.518 - ETA: 0s - loss: 7.2091 - acc: 0.521 - ETA: 0s - loss: 7.2015 - acc: 0.522 - ETA: 0s - loss: 7.2098 - acc: 0.522 - ETA: 0s - loss: 7.2008 - acc: 0.523 - ETA: 0s - loss: 7.1647 - acc: 0.525 - ETA: 0s - loss: 7.2102 - acc: 0.522 - 1s 188us/step - loss: 7.1978 - acc: 0.5226 - val_loss: 7.8180 - val_acc: 0.4323\n",
      "\n",
      "Epoch 00008: val_loss improved from 7.94569 to 7.81800, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 5.7328 - acc: 0.550 - ETA: 1s - loss: 6.8206 - acc: 0.556 - ETA: 1s - loss: 6.9514 - acc: 0.545 - ETA: 0s - loss: 6.9019 - acc: 0.543 - ETA: 0s - loss: 6.8618 - acc: 0.546 - ETA: 0s - loss: 6.9086 - acc: 0.545 - ETA: 0s - loss: 6.9622 - acc: 0.540 - ETA: 0s - loss: 6.9649 - acc: 0.538 - ETA: 0s - loss: 7.0635 - acc: 0.532 - ETA: 0s - loss: 7.0197 - acc: 0.536 - ETA: 0s - loss: 6.9973 - acc: 0.536 - ETA: 0s - loss: 7.0247 - acc: 0.534 - ETA: 0s - loss: 7.0796 - acc: 0.531 - ETA: 0s - loss: 7.0376 - acc: 0.532 - ETA: 0s - loss: 7.0116 - acc: 0.534 - ETA: 0s - loss: 6.9232 - acc: 0.539 - ETA: 0s - loss: 6.9024 - acc: 0.540 - ETA: 0s - loss: 6.8847 - acc: 0.541 - ETA: 0s - loss: 6.9374 - acc: 0.538 - ETA: 0s - loss: 6.9342 - acc: 0.538 - ETA: 0s - loss: 6.9568 - acc: 0.536 - 1s 175us/step - loss: 6.9656 - acc: 0.5355 - val_loss: 7.6667 - val_acc: 0.4335\n",
      "\n",
      "Epoch 00009: val_loss improved from 7.81800 to 7.66668, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 8.0686 - acc: 0.500 - ETA: 1s - loss: 6.6299 - acc: 0.564 - ETA: 0s - loss: 6.4385 - acc: 0.572 - ETA: 0s - loss: 6.3905 - acc: 0.575 - ETA: 0s - loss: 6.4571 - acc: 0.572 - ETA: 0s - loss: 6.6024 - acc: 0.563 - ETA: 0s - loss: 6.5881 - acc: 0.564 - ETA: 0s - loss: 6.5844 - acc: 0.564 - ETA: 0s - loss: 6.5907 - acc: 0.564 - ETA: 0s - loss: 6.5750 - acc: 0.565 - ETA: 0s - loss: 6.6240 - acc: 0.561 - ETA: 0s - loss: 6.6210 - acc: 0.562 - ETA: 0s - loss: 6.6818 - acc: 0.558 - ETA: 0s - loss: 6.6698 - acc: 0.560 - ETA: 0s - loss: 6.6688 - acc: 0.559 - ETA: 0s - loss: 6.6845 - acc: 0.558 - ETA: 0s - loss: 6.6851 - acc: 0.558 - ETA: 0s - loss: 6.6975 - acc: 0.557 - ETA: 0s - loss: 6.7016 - acc: 0.557 - ETA: 0s - loss: 6.7221 - acc: 0.555 - ETA: 0s - loss: 6.7538 - acc: 0.553 - 1s 175us/step - loss: 6.7579 - acc: 0.5533 - val_loss: 7.6268 - val_acc: 0.4431\n",
      "\n",
      "Epoch 00010: val_loss improved from 7.66668 to 7.62682, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 6.4478 - acc: 0.600 - ETA: 1s - loss: 6.0635 - acc: 0.605 - ETA: 0s - loss: 6.1341 - acc: 0.603 - ETA: 0s - loss: 6.3107 - acc: 0.591 - ETA: 0s - loss: 6.3333 - acc: 0.590 - ETA: 0s - loss: 6.5056 - acc: 0.580 - ETA: 0s - loss: 6.4152 - acc: 0.585 - ETA: 0s - loss: 6.3939 - acc: 0.586 - ETA: 0s - loss: 6.3590 - acc: 0.587 - ETA: 0s - loss: 6.4437 - acc: 0.581 - ETA: 0s - loss: 6.3923 - acc: 0.585 - ETA: 0s - loss: 6.4492 - acc: 0.582 - ETA: 0s - loss: 6.4300 - acc: 0.583 - ETA: 0s - loss: 6.4545 - acc: 0.581 - ETA: 0s - loss: 6.5000 - acc: 0.578 - ETA: 0s - loss: 6.5049 - acc: 0.577 - ETA: 0s - loss: 6.5768 - acc: 0.572 - ETA: 0s - loss: 6.6406 - acc: 0.568 - ETA: 0s - loss: 6.6724 - acc: 0.565 - ETA: 0s - loss: 6.6851 - acc: 0.564 - ETA: 0s - loss: 6.6770 - acc: 0.565 - ETA: 0s - loss: 6.6916 - acc: 0.564 - 1s 176us/step - loss: 6.6957 - acc: 0.5645 - val_loss: 7.5602 - val_acc: 0.4515\n",
      "\n",
      "Epoch 00011: val_loss improved from 7.62682 to 7.56025, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 5.6560 - acc: 0.650 - ETA: 1s - loss: 7.1493 - acc: 0.544 - ETA: 0s - loss: 6.4848 - acc: 0.581 - ETA: 0s - loss: 6.6381 - acc: 0.571 - ETA: 0s - loss: 6.7223 - acc: 0.566 - ETA: 0s - loss: 6.6503 - acc: 0.569 - ETA: 0s - loss: 6.6299 - acc: 0.569 - ETA: 0s - loss: 6.6791 - acc: 0.568 - ETA: 0s - loss: 6.7668 - acc: 0.563 - ETA: 0s - loss: 6.7681 - acc: 0.562 - ETA: 0s - loss: 6.6981 - acc: 0.566 - ETA: 0s - loss: 6.6996 - acc: 0.567 - ETA: 0s - loss: 6.6970 - acc: 0.567 - ETA: 0s - loss: 6.7489 - acc: 0.563 - ETA: 0s - loss: 6.7450 - acc: 0.564 - ETA: 0s - loss: 6.7534 - acc: 0.564 - ETA: 0s - loss: 6.7577 - acc: 0.564 - ETA: 0s - loss: 6.7589 - acc: 0.564 - ETA: 0s - loss: 6.7108 - acc: 0.567 - ETA: 0s - loss: 6.6936 - acc: 0.567 - ETA: 0s - loss: 6.6851 - acc: 0.567 - ETA: 0s - loss: 6.6579 - acc: 0.568 - 1s 183us/step - loss: 6.6490 - acc: 0.5696 - val_loss: 7.4290 - val_acc: 0.4647\n",
      "\n",
      "Epoch 00012: val_loss improved from 7.56025 to 7.42896, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 4.8504 - acc: 0.700 - ETA: 1s - loss: 6.8265 - acc: 0.568 - ETA: 1s - loss: 6.3441 - acc: 0.601 - ETA: 0s - loss: 6.5475 - acc: 0.586 - ETA: 0s - loss: 6.6995 - acc: 0.574 - ETA: 0s - loss: 6.7171 - acc: 0.571 - ETA: 0s - loss: 6.6665 - acc: 0.573 - ETA: 0s - loss: 6.6615 - acc: 0.573 - ETA: 0s - loss: 6.7528 - acc: 0.567 - ETA: 0s - loss: 6.7137 - acc: 0.569 - ETA: 0s - loss: 6.6573 - acc: 0.571 - ETA: 0s - loss: 6.7076 - acc: 0.568 - ETA: 0s - loss: 6.6934 - acc: 0.567 - ETA: 0s - loss: 6.6605 - acc: 0.568 - ETA: 0s - loss: 6.6380 - acc: 0.569 - ETA: 0s - loss: 6.6282 - acc: 0.569 - ETA: 0s - loss: 6.6435 - acc: 0.567 - ETA: 0s - loss: 6.6198 - acc: 0.568 - ETA: 0s - loss: 6.6365 - acc: 0.566 - ETA: 0s - loss: 6.5747 - acc: 0.569 - ETA: 0s - loss: 6.5306 - acc: 0.571 - ETA: 0s - loss: 6.5460 - acc: 0.570 - ETA: 0s - loss: 6.5081 - acc: 0.572 - 1s 192us/step - loss: 6.5069 - acc: 0.5728 - val_loss: 7.2556 - val_acc: 0.4575\n",
      "\n",
      "Epoch 00013: val_loss improved from 7.42896 to 7.25555, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 6.3662 - acc: 0.600 - ETA: 1s - loss: 6.9424 - acc: 0.541 - ETA: 1s - loss: 6.3341 - acc: 0.582 - ETA: 0s - loss: 6.4762 - acc: 0.569 - ETA: 0s - loss: 6.3635 - acc: 0.574 - ETA: 0s - loss: 6.1382 - acc: 0.590 - ETA: 0s - loss: 6.0133 - acc: 0.596 - ETA: 0s - loss: 6.0384 - acc: 0.592 - ETA: 0s - loss: 6.1814 - acc: 0.586 - ETA: 0s - loss: 6.1317 - acc: 0.590 - ETA: 0s - loss: 6.1622 - acc: 0.586 - ETA: 0s - loss: 6.2295 - acc: 0.582 - ETA: 0s - loss: 6.1874 - acc: 0.585 - ETA: 0s - loss: 6.1763 - acc: 0.585 - ETA: 0s - loss: 6.1719 - acc: 0.585 - ETA: 0s - loss: 6.2390 - acc: 0.582 - ETA: 0s - loss: 6.2232 - acc: 0.583 - ETA: 0s - loss: 6.1870 - acc: 0.586 - ETA: 0s - loss: 6.1571 - acc: 0.587 - ETA: 0s - loss: 6.1310 - acc: 0.588 - ETA: 0s - loss: 6.1045 - acc: 0.589 - ETA: 0s - loss: 6.0966 - acc: 0.590 - 1s 181us/step - loss: 6.0823 - acc: 0.5910 - val_loss: 6.8590 - val_acc: 0.4826\n",
      "\n",
      "Epoch 00014: val_loss improved from 7.25555 to 6.85897, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1s - loss: 4.2032 - acc: 0.700 - ETA: 1s - loss: 5.0657 - acc: 0.636 - ETA: 1s - loss: 5.4960 - acc: 0.624 - ETA: 1s - loss: 5.6884 - acc: 0.615 - ETA: 0s - loss: 5.7198 - acc: 0.616 - ETA: 0s - loss: 5.6634 - acc: 0.619 - ETA: 0s - loss: 5.6789 - acc: 0.620 - ETA: 0s - loss: 5.6567 - acc: 0.623 - ETA: 0s - loss: 5.6591 - acc: 0.622 - ETA: 0s - loss: 5.7703 - acc: 0.615 - ETA: 0s - loss: 5.7409 - acc: 0.614 - ETA: 0s - loss: 5.7657 - acc: 0.611 - ETA: 0s - loss: 5.7575 - acc: 0.611 - ETA: 0s - loss: 5.7087 - acc: 0.614 - ETA: 0s - loss: 5.7484 - acc: 0.611 - ETA: 0s - loss: 5.7619 - acc: 0.611 - ETA: 0s - loss: 5.7691 - acc: 0.610 - ETA: 0s - loss: 5.7574 - acc: 0.612 - ETA: 0s - loss: 5.7214 - acc: 0.615 - ETA: 0s - loss: 5.6979 - acc: 0.616 - ETA: 0s - loss: 5.7203 - acc: 0.614 - ETA: 0s - loss: 5.7360 - acc: 0.614 - ETA: 0s - loss: 5.7588 - acc: 0.612 - 1s 190us/step - loss: 5.7693 - acc: 0.6129 - val_loss: 6.6974 - val_acc: 0.4922\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.85897 to 6.69735, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 0s - loss: 7.2533 - acc: 0.550 - ETA: 1s - loss: 6.2767 - acc: 0.591 - ETA: 0s - loss: 6.2182 - acc: 0.598 - ETA: 0s - loss: 6.2469 - acc: 0.596 - ETA: 0s - loss: 6.1909 - acc: 0.600 - ETA: 0s - loss: 6.2557 - acc: 0.598 - ETA: 0s - loss: 6.1085 - acc: 0.607 - ETA: 0s - loss: 6.0696 - acc: 0.607 - ETA: 0s - loss: 6.0385 - acc: 0.609 - ETA: 0s - loss: 6.0292 - acc: 0.606 - ETA: 0s - loss: 5.9549 - acc: 0.611 - ETA: 0s - loss: 5.9196 - acc: 0.613 - ETA: 0s - loss: 5.8889 - acc: 0.615 - ETA: 0s - loss: 5.8686 - acc: 0.616 - ETA: 0s - loss: 5.8780 - acc: 0.616 - ETA: 0s - loss: 5.8350 - acc: 0.618 - ETA: 0s - loss: 5.7073 - acc: 0.625 - ETA: 0s - loss: 5.6970 - acc: 0.626 - ETA: 0s - loss: 5.6608 - acc: 0.627 - ETA: 0s - loss: 5.6343 - acc: 0.629 - ETA: 0s - loss: 5.6243 - acc: 0.629 - ETA: 0s - loss: 5.5920 - acc: 0.631 - ETA: 0s - loss: 5.5715 - acc: 0.632 - 1s 191us/step - loss: 5.5791 - acc: 0.6325 - val_loss: 6.5168 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00016: val_loss improved from 6.69735 to 6.51679, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 5.8389 - acc: 0.600 - ETA: 1s - loss: 6.4547 - acc: 0.580 - ETA: 1s - loss: 6.0500 - acc: 0.603 - ETA: 1s - loss: 5.8994 - acc: 0.614 - ETA: 1s - loss: 5.8299 - acc: 0.621 - ETA: 1s - loss: 5.7408 - acc: 0.627 - ETA: 0s - loss: 5.5680 - acc: 0.638 - ETA: 0s - loss: 5.5490 - acc: 0.639 - ETA: 0s - loss: 5.6653 - acc: 0.633 - ETA: 0s - loss: 5.6436 - acc: 0.632 - ETA: 0s - loss: 5.5988 - acc: 0.635 - ETA: 0s - loss: 5.7134 - acc: 0.628 - ETA: 0s - loss: 5.6681 - acc: 0.631 - ETA: 0s - loss: 5.6368 - acc: 0.633 - ETA: 0s - loss: 5.7171 - acc: 0.627 - ETA: 0s - loss: 5.6902 - acc: 0.628 - ETA: 0s - loss: 5.6479 - acc: 0.631 - ETA: 0s - loss: 5.6100 - acc: 0.633 - ETA: 0s - loss: 5.5823 - acc: 0.635 - ETA: 0s - loss: 5.5522 - acc: 0.637 - ETA: 0s - loss: 5.5209 - acc: 0.639 - ETA: 0s - loss: 5.4982 - acc: 0.641 - ETA: 0s - loss: 5.4719 - acc: 0.641 - 1s 192us/step - loss: 5.4563 - acc: 0.6427 - val_loss: 6.4423 - val_acc: 0.5150\n",
      "\n",
      "Epoch 00017: val_loss improved from 6.51679 to 6.44228, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 6.4601 - acc: 0.600 - ETA: 1s - loss: 4.6549 - acc: 0.703 - ETA: 1s - loss: 4.9090 - acc: 0.683 - ETA: 0s - loss: 5.1043 - acc: 0.670 - ETA: 0s - loss: 5.1772 - acc: 0.668 - ETA: 0s - loss: 5.2042 - acc: 0.664 - ETA: 0s - loss: 5.2491 - acc: 0.662 - ETA: 0s - loss: 5.1645 - acc: 0.664 - ETA: 0s - loss: 5.1988 - acc: 0.661 - ETA: 0s - loss: 5.1824 - acc: 0.661 - ETA: 0s - loss: 5.1683 - acc: 0.662 - ETA: 0s - loss: 5.1702 - acc: 0.661 - ETA: 0s - loss: 5.1453 - acc: 0.662 - ETA: 0s - loss: 5.1679 - acc: 0.661 - ETA: 0s - loss: 5.1819 - acc: 0.660 - ETA: 0s - loss: 5.2089 - acc: 0.658 - ETA: 0s - loss: 5.2167 - acc: 0.658 - ETA: 0s - loss: 5.1916 - acc: 0.659 - ETA: 0s - loss: 5.1876 - acc: 0.660 - ETA: 0s - loss: 5.2406 - acc: 0.656 - ETA: 0s - loss: 5.2543 - acc: 0.656 - ETA: 0s - loss: 5.2493 - acc: 0.656 - ETA: 0s - loss: 5.2739 - acc: 0.655 - 1s 187us/step - loss: 5.2686 - acc: 0.6555 - val_loss: 6.3220 - val_acc: 0.5114\n",
      "\n",
      "Epoch 00018: val_loss improved from 6.44228 to 6.32205, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 5.7656 - acc: 0.600 - ETA: 1s - loss: 5.4748 - acc: 0.650 - ETA: 0s - loss: 5.5279 - acc: 0.648 - ETA: 0s - loss: 5.3274 - acc: 0.662 - ETA: 0s - loss: 5.1279 - acc: 0.676 - ETA: 0s - loss: 5.1751 - acc: 0.672 - ETA: 0s - loss: 5.1851 - acc: 0.671 - ETA: 0s - loss: 5.0811 - acc: 0.676 - ETA: 0s - loss: 5.1435 - acc: 0.672 - ETA: 0s - loss: 5.1692 - acc: 0.671 - ETA: 0s - loss: 5.1171 - acc: 0.674 - ETA: 0s - loss: 5.1513 - acc: 0.671 - ETA: 0s - loss: 5.1707 - acc: 0.669 - ETA: 0s - loss: 5.1389 - acc: 0.670 - ETA: 0s - loss: 5.1106 - acc: 0.672 - ETA: 0s - loss: 5.1493 - acc: 0.670 - ETA: 0s - loss: 5.1265 - acc: 0.670 - ETA: 0s - loss: 5.1552 - acc: 0.668 - ETA: 0s - loss: 5.2004 - acc: 0.666 - ETA: 0s - loss: 5.1915 - acc: 0.666 - ETA: 0s - loss: 5.1885 - acc: 0.666 - ETA: 0s - loss: 5.1778 - acc: 0.666 - ETA: 0s - loss: 5.1892 - acc: 0.665 - 1s 188us/step - loss: 5.1903 - acc: 0.6657 - val_loss: 6.2247 - val_acc: 0.5222\n",
      "\n",
      "Epoch 00019: val_loss improved from 6.32205 to 6.22466, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 7.2532 - acc: 0.550 - ETA: 1s - loss: 4.8104 - acc: 0.693 - ETA: 1s - loss: 5.1026 - acc: 0.676 - ETA: 0s - loss: 5.0469 - acc: 0.682 - ETA: 0s - loss: 4.9443 - acc: 0.687 - ETA: 0s - loss: 5.0700 - acc: 0.681 - ETA: 0s - loss: 5.1981 - acc: 0.673 - ETA: 0s - loss: 5.1644 - acc: 0.675 - ETA: 0s - loss: 5.1245 - acc: 0.678 - ETA: 0s - loss: 4.9940 - acc: 0.685 - ETA: 0s - loss: 5.0941 - acc: 0.679 - ETA: 0s - loss: 5.0914 - acc: 0.678 - ETA: 0s - loss: 5.1389 - acc: 0.674 - ETA: 0s - loss: 5.1392 - acc: 0.673 - ETA: 0s - loss: 5.1280 - acc: 0.673 - ETA: 0s - loss: 5.1596 - acc: 0.671 - ETA: 0s - loss: 5.1841 - acc: 0.668 - ETA: 0s - loss: 5.1451 - acc: 0.670 - ETA: 0s - loss: 5.1557 - acc: 0.670 - ETA: 0s - loss: 5.1861 - acc: 0.668 - ETA: 0s - loss: 5.1710 - acc: 0.669 - ETA: 0s - loss: 5.1781 - acc: 0.669 - 1s 179us/step - loss: 5.1561 - acc: 0.6705 - val_loss: 6.2108 - val_acc: 0.5281\n",
      "\n",
      "Epoch 00020: val_loss improved from 6.22466 to 6.21085, saving model to saved_models/weights.best.VGG16.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x247164ff438>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 52.9904%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogXceptionData.npz')\n",
    "train_Xcept = bottleneck_features['train']\n",
    "valid_Xcept = bottleneck_features['valid']\n",
    "test_Xcept = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ Using the previous example of transfer learning, I used a GAP to pool and flatten the data and a fully connected layer to classify the data into dog breeds.\n",
    "\n",
    "A GAP layer was used to flatten the result of the Xception model from a 4D tensor to a 2D tensor.\n",
    "\n",
    "A fully connected layer is used for classification of our dog breeds with the input coming from the GAP layer above and an output of one of 133 dog breeds. \n",
    "\n",
    "\n",
    "This architecture should have a better classification result because the transfer learning algorithm we are utilizing has already been trained with a lot of data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "Xcept_model = Sequential()\n",
    "\n",
    "Xcept_model.add(GlobalAveragePooling2D(input_shape=train_Xcept.shape[1:]))\n",
    "Xcept_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Xcept_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "Xcept_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/25\n",
      "6680/6680 [==============================] - ETA: 2:30 - loss: 5.0539 - acc: 0.0000e+0 - ETA: 18s - loss: 4.8497 - acc: 0.0778    - ETA: 10s - loss: 4.4142 - acc: 0.17 - ETA: 7s - loss: 4.0072 - acc: 0.2400 - ETA: 6s - loss: 3.6314 - acc: 0.304 - ETA: 5s - loss: 3.3178 - acc: 0.353 - ETA: 4s - loss: 3.0901 - acc: 0.391 - ETA: 4s - loss: 2.8520 - acc: 0.439 - ETA: 3s - loss: 2.6676 - acc: 0.466 - ETA: 3s - loss: 2.5109 - acc: 0.493 - ETA: 3s - loss: 2.3502 - acc: 0.522 - ETA: 2s - loss: 2.2269 - acc: 0.542 - ETA: 2s - loss: 2.1103 - acc: 0.562 - ETA: 2s - loss: 2.0148 - acc: 0.578 - ETA: 2s - loss: 1.9244 - acc: 0.591 - ETA: 2s - loss: 1.8460 - acc: 0.604 - ETA: 2s - loss: 1.7871 - acc: 0.613 - ETA: 2s - loss: 1.7261 - acc: 0.622 - ETA: 1s - loss: 1.6696 - acc: 0.629 - ETA: 1s - loss: 1.6219 - acc: 0.637 - ETA: 1s - loss: 1.5809 - acc: 0.645 - ETA: 1s - loss: 1.5395 - acc: 0.650 - ETA: 1s - loss: 1.4933 - acc: 0.659 - ETA: 1s - loss: 1.4532 - acc: 0.667 - ETA: 1s - loss: 1.4179 - acc: 0.674 - ETA: 1s - loss: 1.3890 - acc: 0.678 - ETA: 1s - loss: 1.3590 - acc: 0.682 - ETA: 1s - loss: 1.3322 - acc: 0.687 - ETA: 1s - loss: 1.3062 - acc: 0.691 - ETA: 0s - loss: 1.2792 - acc: 0.696 - ETA: 0s - loss: 1.2608 - acc: 0.698 - ETA: 0s - loss: 1.2412 - acc: 0.702 - ETA: 0s - loss: 1.2193 - acc: 0.705 - ETA: 0s - loss: 1.1977 - acc: 0.709 - ETA: 0s - loss: 1.1797 - acc: 0.712 - ETA: 0s - loss: 1.1611 - acc: 0.716 - ETA: 0s - loss: 1.1466 - acc: 0.718 - ETA: 0s - loss: 1.1287 - acc: 0.722 - ETA: 0s - loss: 1.1164 - acc: 0.724 - ETA: 0s - loss: 1.1019 - acc: 0.727 - ETA: 0s - loss: 1.0900 - acc: 0.729 - ETA: 0s - loss: 1.0763 - acc: 0.731 - ETA: 0s - loss: 1.0627 - acc: 0.734 - 3s 458us/step - loss: 1.0550 - acc: 0.7356 - val_loss: 0.5210 - val_acc: 0.8228\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52103, saving model to saved_models/weights.best.Xcept.hdf5\n",
      "Epoch 2/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1685 - acc: 0.950 - ETA: 2s - loss: 0.4199 - acc: 0.883 - ETA: 2s - loss: 0.3582 - acc: 0.893 - ETA: 2s - loss: 0.3683 - acc: 0.887 - ETA: 2s - loss: 0.4060 - acc: 0.866 - ETA: 2s - loss: 0.3804 - acc: 0.879 - ETA: 2s - loss: 0.3657 - acc: 0.884 - ETA: 2s - loss: 0.3643 - acc: 0.881 - ETA: 1s - loss: 0.3680 - acc: 0.879 - ETA: 1s - loss: 0.3771 - acc: 0.875 - ETA: 1s - loss: 0.3729 - acc: 0.878 - ETA: 1s - loss: 0.3698 - acc: 0.880 - ETA: 1s - loss: 0.3688 - acc: 0.879 - ETA: 1s - loss: 0.3707 - acc: 0.879 - ETA: 1s - loss: 0.3670 - acc: 0.880 - ETA: 1s - loss: 0.3743 - acc: 0.878 - ETA: 1s - loss: 0.3828 - acc: 0.876 - ETA: 1s - loss: 0.3831 - acc: 0.876 - ETA: 1s - loss: 0.3878 - acc: 0.873 - ETA: 1s - loss: 0.3924 - acc: 0.872 - ETA: 1s - loss: 0.3968 - acc: 0.871 - ETA: 1s - loss: 0.3958 - acc: 0.871 - ETA: 1s - loss: 0.3917 - acc: 0.872 - ETA: 1s - loss: 0.3915 - acc: 0.872 - ETA: 1s - loss: 0.3932 - acc: 0.872 - ETA: 0s - loss: 0.3922 - acc: 0.871 - ETA: 0s - loss: 0.3933 - acc: 0.871 - ETA: 0s - loss: 0.3923 - acc: 0.871 - ETA: 0s - loss: 0.3930 - acc: 0.870 - ETA: 0s - loss: 0.3912 - acc: 0.871 - ETA: 0s - loss: 0.3877 - acc: 0.872 - ETA: 0s - loss: 0.3866 - acc: 0.873 - ETA: 0s - loss: 0.3869 - acc: 0.873 - ETA: 0s - loss: 0.3873 - acc: 0.873 - ETA: 0s - loss: 0.3909 - acc: 0.872 - ETA: 0s - loss: 0.3959 - acc: 0.871 - ETA: 0s - loss: 0.3955 - acc: 0.870 - ETA: 0s - loss: 0.3972 - acc: 0.868 - ETA: 0s - loss: 0.4024 - acc: 0.867 - ETA: 0s - loss: 0.4030 - acc: 0.867 - ETA: 0s - loss: 0.4011 - acc: 0.867 - ETA: 0s - loss: 0.4021 - acc: 0.867 - ETA: 0s - loss: 0.4010 - acc: 0.868 - ETA: 0s - loss: 0.3978 - acc: 0.869 - 3s 388us/step - loss: 0.3971 - acc: 0.8692 - val_loss: 0.5050 - val_acc: 0.8347\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52103 to 0.50501, saving model to saved_models/weights.best.Xcept.hdf5\n",
      "Epoch 3/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1063 - acc: 0.950 - ETA: 2s - loss: 0.2149 - acc: 0.944 - ETA: 2s - loss: 0.2077 - acc: 0.943 - ETA: 2s - loss: 0.1978 - acc: 0.943 - ETA: 2s - loss: 0.2195 - acc: 0.930 - ETA: 2s - loss: 0.2402 - acc: 0.924 - ETA: 2s - loss: 0.2363 - acc: 0.923 - ETA: 2s - loss: 0.2537 - acc: 0.926 - ETA: 2s - loss: 0.2717 - acc: 0.919 - ETA: 2s - loss: 0.2655 - acc: 0.918 - ETA: 1s - loss: 0.2830 - acc: 0.912 - ETA: 1s - loss: 0.2759 - acc: 0.914 - ETA: 1s - loss: 0.2805 - acc: 0.912 - ETA: 1s - loss: 0.2893 - acc: 0.909 - ETA: 1s - loss: 0.2983 - acc: 0.905 - ETA: 1s - loss: 0.3065 - acc: 0.901 - ETA: 1s - loss: 0.3070 - acc: 0.900 - ETA: 1s - loss: 0.3097 - acc: 0.900 - ETA: 1s - loss: 0.3144 - acc: 0.899 - ETA: 1s - loss: 0.3167 - acc: 0.898 - ETA: 1s - loss: 0.3181 - acc: 0.896 - ETA: 1s - loss: 0.3101 - acc: 0.899 - ETA: 1s - loss: 0.3032 - acc: 0.901 - ETA: 1s - loss: 0.3110 - acc: 0.899 - ETA: 1s - loss: 0.3117 - acc: 0.898 - ETA: 1s - loss: 0.3068 - acc: 0.900 - ETA: 1s - loss: 0.3035 - acc: 0.901 - ETA: 1s - loss: 0.3067 - acc: 0.900 - ETA: 1s - loss: 0.3111 - acc: 0.899 - ETA: 0s - loss: 0.3116 - acc: 0.899 - ETA: 0s - loss: 0.3098 - acc: 0.899 - ETA: 0s - loss: 0.3149 - acc: 0.897 - ETA: 0s - loss: 0.3127 - acc: 0.898 - ETA: 0s - loss: 0.3104 - acc: 0.899 - ETA: 0s - loss: 0.3101 - acc: 0.898 - ETA: 0s - loss: 0.3094 - acc: 0.899 - ETA: 0s - loss: 0.3102 - acc: 0.899 - ETA: 0s - loss: 0.3100 - acc: 0.900 - ETA: 0s - loss: 0.3093 - acc: 0.901 - ETA: 0s - loss: 0.3136 - acc: 0.901 - ETA: 0s - loss: 0.3125 - acc: 0.901 - ETA: 0s - loss: 0.3132 - acc: 0.901 - ETA: 0s - loss: 0.3157 - acc: 0.901 - ETA: 0s - loss: 0.3166 - acc: 0.901 - ETA: 0s - loss: 0.3185 - acc: 0.900 - ETA: 0s - loss: 0.3209 - acc: 0.899 - ETA: 0s - loss: 0.3199 - acc: 0.899 - ETA: 0s - loss: 0.3214 - acc: 0.898 - 3s 415us/step - loss: 0.3203 - acc: 0.8993 - val_loss: 0.5073 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50501\n",
      "Epoch 4/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.3003 - acc: 0.800 - ETA: 2s - loss: 0.2903 - acc: 0.900 - ETA: 2s - loss: 0.2956 - acc: 0.900 - ETA: 2s - loss: 0.2826 - acc: 0.902 - ETA: 2s - loss: 0.2651 - acc: 0.910 - ETA: 2s - loss: 0.2758 - acc: 0.905 - ETA: 2s - loss: 0.2693 - acc: 0.909 - ETA: 2s - loss: 0.2726 - acc: 0.910 - ETA: 2s - loss: 0.2702 - acc: 0.909 - ETA: 1s - loss: 0.2636 - acc: 0.911 - ETA: 1s - loss: 0.2574 - acc: 0.910 - ETA: 1s - loss: 0.2648 - acc: 0.909 - ETA: 1s - loss: 0.2662 - acc: 0.909 - ETA: 1s - loss: 0.2647 - acc: 0.907 - ETA: 1s - loss: 0.2568 - acc: 0.911 - ETA: 1s - loss: 0.2588 - acc: 0.911 - ETA: 1s - loss: 0.2575 - acc: 0.912 - ETA: 1s - loss: 0.2598 - acc: 0.911 - ETA: 1s - loss: 0.2647 - acc: 0.910 - ETA: 1s - loss: 0.2674 - acc: 0.909 - ETA: 1s - loss: 0.2743 - acc: 0.908 - ETA: 1s - loss: 0.2788 - acc: 0.907 - ETA: 1s - loss: 0.2767 - acc: 0.907 - ETA: 1s - loss: 0.2735 - acc: 0.909 - ETA: 1s - loss: 0.2776 - acc: 0.907 - ETA: 1s - loss: 0.2809 - acc: 0.906 - ETA: 1s - loss: 0.2769 - acc: 0.907 - ETA: 0s - loss: 0.2765 - acc: 0.907 - ETA: 0s - loss: 0.2762 - acc: 0.907 - ETA: 0s - loss: 0.2766 - acc: 0.907 - ETA: 0s - loss: 0.2792 - acc: 0.907 - ETA: 0s - loss: 0.2788 - acc: 0.906 - ETA: 0s - loss: 0.2760 - acc: 0.907 - ETA: 0s - loss: 0.2789 - acc: 0.906 - ETA: 0s - loss: 0.2746 - acc: 0.907 - ETA: 0s - loss: 0.2741 - acc: 0.908 - ETA: 0s - loss: 0.2790 - acc: 0.907 - ETA: 0s - loss: 0.2830 - acc: 0.906 - ETA: 0s - loss: 0.2794 - acc: 0.907 - ETA: 0s - loss: 0.2791 - acc: 0.907 - ETA: 0s - loss: 0.2782 - acc: 0.907 - ETA: 0s - loss: 0.2805 - acc: 0.906 - ETA: 0s - loss: 0.2795 - acc: 0.906 - ETA: 0s - loss: 0.2779 - acc: 0.907 - ETA: 0s - loss: 0.2784 - acc: 0.907 - ETA: 0s - loss: 0.2764 - acc: 0.908 - 3s 399us/step - loss: 0.2758 - acc: 0.9087 - val_loss: 0.4889 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.50501 to 0.48890, saving model to saved_models/weights.best.Xcept.hdf5\n",
      "Epoch 5/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.3069 - acc: 0.950 - ETA: 2s - loss: 0.1520 - acc: 0.966 - ETA: 2s - loss: 0.2753 - acc: 0.926 - ETA: 2s - loss: 0.2073 - acc: 0.944 - ETA: 2s - loss: 0.2021 - acc: 0.940 - ETA: 2s - loss: 0.1883 - acc: 0.942 - ETA: 2s - loss: 0.1950 - acc: 0.938 - ETA: 2s - loss: 0.1975 - acc: 0.936 - ETA: 2s - loss: 0.2086 - acc: 0.934 - ETA: 2s - loss: 0.2173 - acc: 0.929 - ETA: 2s - loss: 0.2289 - acc: 0.927 - ETA: 2s - loss: 0.2228 - acc: 0.929 - ETA: 2s - loss: 0.2182 - acc: 0.930 - ETA: 2s - loss: 0.2078 - acc: 0.934 - ETA: 2s - loss: 0.2170 - acc: 0.931 - ETA: 2s - loss: 0.2196 - acc: 0.930 - ETA: 2s - loss: 0.2236 - acc: 0.930 - ETA: 2s - loss: 0.2285 - acc: 0.930 - ETA: 2s - loss: 0.2371 - acc: 0.929 - ETA: 2s - loss: 0.2347 - acc: 0.929 - ETA: 1s - loss: 0.2306 - acc: 0.930 - ETA: 1s - loss: 0.2293 - acc: 0.930 - ETA: 1s - loss: 0.2274 - acc: 0.930 - ETA: 1s - loss: 0.2272 - acc: 0.930 - ETA: 1s - loss: 0.2258 - acc: 0.930 - ETA: 1s - loss: 0.2271 - acc: 0.930 - ETA: 1s - loss: 0.2295 - acc: 0.929 - ETA: 1s - loss: 0.2255 - acc: 0.930 - ETA: 1s - loss: 0.2297 - acc: 0.929 - ETA: 1s - loss: 0.2275 - acc: 0.930 - ETA: 1s - loss: 0.2278 - acc: 0.931 - ETA: 1s - loss: 0.2281 - acc: 0.930 - ETA: 1s - loss: 0.2289 - acc: 0.931 - ETA: 1s - loss: 0.2325 - acc: 0.931 - ETA: 1s - loss: 0.2306 - acc: 0.932 - ETA: 1s - loss: 0.2298 - acc: 0.932 - ETA: 1s - loss: 0.2287 - acc: 0.932 - ETA: 1s - loss: 0.2298 - acc: 0.932 - ETA: 1s - loss: 0.2294 - acc: 0.932 - ETA: 1s - loss: 0.2275 - acc: 0.933 - ETA: 1s - loss: 0.2281 - acc: 0.932 - ETA: 1s - loss: 0.2316 - acc: 0.932 - ETA: 1s - loss: 0.2322 - acc: 0.932 - ETA: 1s - loss: 0.2308 - acc: 0.932 - ETA: 1s - loss: 0.2320 - acc: 0.932 - ETA: 1s - loss: 0.2317 - acc: 0.932 - ETA: 1s - loss: 0.2317 - acc: 0.932 - ETA: 1s - loss: 0.2335 - acc: 0.931 - ETA: 1s - loss: 0.2322 - acc: 0.932 - ETA: 1s - loss: 0.2338 - acc: 0.931 - ETA: 1s - loss: 0.2326 - acc: 0.932 - ETA: 0s - loss: 0.2318 - acc: 0.932 - ETA: 0s - loss: 0.2303 - acc: 0.932 - ETA: 0s - loss: 0.2304 - acc: 0.932 - ETA: 0s - loss: 0.2327 - acc: 0.931 - ETA: 0s - loss: 0.2363 - acc: 0.931 - ETA: 0s - loss: 0.2362 - acc: 0.931 - ETA: 0s - loss: 0.2365 - acc: 0.930 - ETA: 0s - loss: 0.2357 - acc: 0.930 - ETA: 0s - loss: 0.2358 - acc: 0.930 - ETA: 0s - loss: 0.2356 - acc: 0.930 - ETA: 0s - loss: 0.2358 - acc: 0.930 - ETA: 0s - loss: 0.2340 - acc: 0.930 - ETA: 0s - loss: 0.2351 - acc: 0.929 - ETA: 0s - loss: 0.2364 - acc: 0.929 - ETA: 0s - loss: 0.2348 - acc: 0.929 - ETA: 0s - loss: 0.2342 - acc: 0.930 - ETA: 0s - loss: 0.2340 - acc: 0.930 - ETA: 0s - loss: 0.2336 - acc: 0.930 - ETA: 0s - loss: 0.2339 - acc: 0.929 - ETA: 0s - loss: 0.2343 - acc: 0.929 - ETA: 0s - loss: 0.2362 - acc: 0.928 - ETA: 0s - loss: 0.2372 - acc: 0.928 - ETA: 0s - loss: 0.2385 - acc: 0.927 - ETA: 0s - loss: 0.2392 - acc: 0.927 - ETA: 0s - loss: 0.2396 - acc: 0.927 - 5s 713us/step - loss: 0.2404 - acc: 0.9274 - val_loss: 0.5040 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48890\n",
      "Epoch 6/25\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0752 - acc: 1.000 - ETA: 5s - loss: 0.2061 - acc: 0.950 - ETA: 5s - loss: 0.2019 - acc: 0.938 - ETA: 5s - loss: 0.1774 - acc: 0.945 - ETA: 5s - loss: 0.1744 - acc: 0.946 - ETA: 5s - loss: 0.1760 - acc: 0.941 - ETA: 5s - loss: 0.1777 - acc: 0.940 - ETA: 5s - loss: 0.1775 - acc: 0.942 - ETA: 5s - loss: 0.1922 - acc: 0.936 - ETA: 5s - loss: 0.2050 - acc: 0.933 - ETA: 4s - loss: 0.1886 - acc: 0.939 - ETA: 4s - loss: 0.1945 - acc: 0.937 - ETA: 4s - loss: 0.1843 - acc: 0.940 - ETA: 4s - loss: 0.1863 - acc: 0.940 - ETA: 4s - loss: 0.1793 - acc: 0.943 - ETA: 4s - loss: 0.1722 - acc: 0.944 - ETA: 4s - loss: 0.1689 - acc: 0.944 - ETA: 4s - loss: 0.1664 - acc: 0.944 - ETA: 4s - loss: 0.1709 - acc: 0.944 - ETA: 4s - loss: 0.1895 - acc: 0.940 - ETA: 4s - loss: 0.1887 - acc: 0.941 - ETA: 4s - loss: 0.1860 - acc: 0.941 - ETA: 4s - loss: 0.1860 - acc: 0.940 - ETA: 4s - loss: 0.1859 - acc: 0.941 - ETA: 4s - loss: 0.1848 - acc: 0.941 - ETA: 4s - loss: 0.1872 - acc: 0.941 - ETA: 3s - loss: 0.1836 - acc: 0.942 - ETA: 3s - loss: 0.1841 - acc: 0.942 - ETA: 3s - loss: 0.1816 - acc: 0.942 - ETA: 3s - loss: 0.1877 - acc: 0.941 - ETA: 3s - loss: 0.1839 - acc: 0.942 - ETA: 3s - loss: 0.1810 - acc: 0.943 - ETA: 3s - loss: 0.1841 - acc: 0.942 - ETA: 3s - loss: 0.1825 - acc: 0.943 - ETA: 3s - loss: 0.1865 - acc: 0.942 - ETA: 3s - loss: 0.1857 - acc: 0.943 - ETA: 3s - loss: 0.1862 - acc: 0.943 - ETA: 3s - loss: 0.1868 - acc: 0.942 - ETA: 3s - loss: 0.1877 - acc: 0.942 - ETA: 3s - loss: 0.1881 - acc: 0.942 - ETA: 3s - loss: 0.1881 - acc: 0.942 - ETA: 3s - loss: 0.1894 - acc: 0.941 - ETA: 2s - loss: 0.1910 - acc: 0.941 - ETA: 2s - loss: 0.1886 - acc: 0.941 - ETA: 2s - loss: 0.1959 - acc: 0.938 - ETA: 2s - loss: 0.1979 - acc: 0.938 - ETA: 2s - loss: 0.1969 - acc: 0.938 - ETA: 2s - loss: 0.1951 - acc: 0.938 - ETA: 2s - loss: 0.2001 - acc: 0.937 - ETA: 1s - loss: 0.2010 - acc: 0.937 - ETA: 1s - loss: 0.2048 - acc: 0.936 - ETA: 1s - loss: 0.2035 - acc: 0.937 - ETA: 1s - loss: 0.2008 - acc: 0.937 - ETA: 1s - loss: 0.1999 - acc: 0.937 - ETA: 1s - loss: 0.2043 - acc: 0.935 - ETA: 1s - loss: 0.2053 - acc: 0.935 - ETA: 1s - loss: 0.2043 - acc: 0.935 - ETA: 0s - loss: 0.2046 - acc: 0.934 - ETA: 0s - loss: 0.2038 - acc: 0.934 - ETA: 0s - loss: 0.2030 - acc: 0.935 - ETA: 0s - loss: 0.2062 - acc: 0.934 - ETA: 0s - loss: 0.2054 - acc: 0.934 - ETA: 0s - loss: 0.2066 - acc: 0.934 - ETA: 0s - loss: 0.2077 - acc: 0.934 - ETA: 0s - loss: 0.2083 - acc: 0.934 - ETA: 0s - loss: 0.2110 - acc: 0.933 - ETA: 0s - loss: 0.2146 - acc: 0.932 - 4s 619us/step - loss: 0.2149 - acc: 0.9323 - val_loss: 0.5344 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48890\n",
      "Epoch 7/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0282 - acc: 1.000 - ETA: 2s - loss: 0.1178 - acc: 0.975 - ETA: 2s - loss: 0.1158 - acc: 0.962 - ETA: 2s - loss: 0.1211 - acc: 0.958 - ETA: 2s - loss: 0.1416 - acc: 0.954 - ETA: 2s - loss: 0.1547 - acc: 0.952 - ETA: 2s - loss: 0.1493 - acc: 0.954 - ETA: 1s - loss: 0.1497 - acc: 0.954 - ETA: 1s - loss: 0.1675 - acc: 0.950 - ETA: 1s - loss: 0.1772 - acc: 0.945 - ETA: 1s - loss: 0.1819 - acc: 0.944 - ETA: 1s - loss: 0.1808 - acc: 0.945 - ETA: 1s - loss: 0.1729 - acc: 0.948 - ETA: 1s - loss: 0.1740 - acc: 0.947 - ETA: 1s - loss: 0.1788 - acc: 0.945 - ETA: 1s - loss: 0.1781 - acc: 0.946 - ETA: 1s - loss: 0.1749 - acc: 0.947 - ETA: 1s - loss: 0.1770 - acc: 0.945 - ETA: 1s - loss: 0.1718 - acc: 0.947 - ETA: 1s - loss: 0.1749 - acc: 0.946 - ETA: 1s - loss: 0.1730 - acc: 0.947 - ETA: 1s - loss: 0.1737 - acc: 0.946 - ETA: 1s - loss: 0.1797 - acc: 0.944 - ETA: 1s - loss: 0.1780 - acc: 0.945 - ETA: 1s - loss: 0.1796 - acc: 0.944 - ETA: 0s - loss: 0.1773 - acc: 0.944 - ETA: 0s - loss: 0.1790 - acc: 0.944 - ETA: 0s - loss: 0.1794 - acc: 0.944 - ETA: 0s - loss: 0.1780 - acc: 0.944 - ETA: 0s - loss: 0.1790 - acc: 0.944 - ETA: 0s - loss: 0.1809 - acc: 0.944 - ETA: 0s - loss: 0.1815 - acc: 0.944 - ETA: 0s - loss: 0.1865 - acc: 0.942 - ETA: 0s - loss: 0.1873 - acc: 0.942 - ETA: 0s - loss: 0.1874 - acc: 0.942 - ETA: 0s - loss: 0.1877 - acc: 0.942 - ETA: 0s - loss: 0.1863 - acc: 0.942 - ETA: 0s - loss: 0.1889 - acc: 0.941 - ETA: 0s - loss: 0.1889 - acc: 0.942 - ETA: 0s - loss: 0.1898 - acc: 0.941 - ETA: 0s - loss: 0.1886 - acc: 0.941 - ETA: 0s - loss: 0.1898 - acc: 0.942 - ETA: 0s - loss: 0.1913 - acc: 0.941 - 3s 384us/step - loss: 0.1920 - acc: 0.9410 - val_loss: 0.5370 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48890\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1228 - acc: 0.950 - ETA: 2s - loss: 0.1008 - acc: 0.977 - ETA: 2s - loss: 0.1133 - acc: 0.961 - ETA: 2s - loss: 0.1298 - acc: 0.960 - ETA: 2s - loss: 0.1249 - acc: 0.962 - ETA: 2s - loss: 0.1284 - acc: 0.961 - ETA: 2s - loss: 0.1252 - acc: 0.961 - ETA: 1s - loss: 0.1391 - acc: 0.960 - ETA: 1s - loss: 0.1488 - acc: 0.957 - ETA: 1s - loss: 0.1429 - acc: 0.958 - ETA: 1s - loss: 0.1338 - acc: 0.961 - ETA: 1s - loss: 0.1354 - acc: 0.959 - ETA: 1s - loss: 0.1417 - acc: 0.956 - ETA: 1s - loss: 0.1425 - acc: 0.952 - ETA: 1s - loss: 0.1477 - acc: 0.952 - ETA: 1s - loss: 0.1517 - acc: 0.952 - ETA: 1s - loss: 0.1510 - acc: 0.953 - ETA: 1s - loss: 0.1475 - acc: 0.953 - ETA: 1s - loss: 0.1456 - acc: 0.953 - ETA: 1s - loss: 0.1484 - acc: 0.953 - ETA: 1s - loss: 0.1504 - acc: 0.952 - ETA: 1s - loss: 0.1548 - acc: 0.951 - ETA: 1s - loss: 0.1521 - acc: 0.952 - ETA: 1s - loss: 0.1590 - acc: 0.950 - ETA: 1s - loss: 0.1602 - acc: 0.950 - ETA: 0s - loss: 0.1614 - acc: 0.950 - ETA: 0s - loss: 0.1627 - acc: 0.950 - ETA: 0s - loss: 0.1632 - acc: 0.950 - ETA: 0s - loss: 0.1686 - acc: 0.948 - ETA: 0s - loss: 0.1650 - acc: 0.949 - ETA: 0s - loss: 0.1636 - acc: 0.949 - ETA: 0s - loss: 0.1686 - acc: 0.948 - ETA: 0s - loss: 0.1673 - acc: 0.949 - ETA: 0s - loss: 0.1675 - acc: 0.948 - ETA: 0s - loss: 0.1652 - acc: 0.949 - ETA: 0s - loss: 0.1676 - acc: 0.948 - ETA: 0s - loss: 0.1696 - acc: 0.948 - ETA: 0s - loss: 0.1696 - acc: 0.948 - ETA: 0s - loss: 0.1690 - acc: 0.948 - ETA: 0s - loss: 0.1699 - acc: 0.947 - ETA: 0s - loss: 0.1693 - acc: 0.947 - ETA: 0s - loss: 0.1698 - acc: 0.947 - ETA: 0s - loss: 0.1720 - acc: 0.946 - 3s 382us/step - loss: 0.1734 - acc: 0.9464 - val_loss: 0.5381 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48890\n",
      "Epoch 9/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0236 - acc: 1.000 - ETA: 2s - loss: 0.1149 - acc: 0.968 - ETA: 2s - loss: 0.0981 - acc: 0.968 - ETA: 2s - loss: 0.1250 - acc: 0.960 - ETA: 2s - loss: 0.1216 - acc: 0.962 - ETA: 2s - loss: 0.1092 - acc: 0.967 - ETA: 1s - loss: 0.1083 - acc: 0.967 - ETA: 1s - loss: 0.1255 - acc: 0.963 - ETA: 1s - loss: 0.1425 - acc: 0.961 - ETA: 1s - loss: 0.1418 - acc: 0.960 - ETA: 1s - loss: 0.1354 - acc: 0.959 - ETA: 1s - loss: 0.1331 - acc: 0.959 - ETA: 1s - loss: 0.1347 - acc: 0.959 - ETA: 1s - loss: 0.1372 - acc: 0.957 - ETA: 1s - loss: 0.1356 - acc: 0.957 - ETA: 1s - loss: 0.1350 - acc: 0.956 - ETA: 1s - loss: 0.1394 - acc: 0.956 - ETA: 1s - loss: 0.1406 - acc: 0.955 - ETA: 1s - loss: 0.1367 - acc: 0.957 - ETA: 1s - loss: 0.1389 - acc: 0.957 - ETA: 1s - loss: 0.1390 - acc: 0.957 - ETA: 1s - loss: 0.1391 - acc: 0.956 - ETA: 1s - loss: 0.1410 - acc: 0.956 - ETA: 1s - loss: 0.1406 - acc: 0.956 - ETA: 1s - loss: 0.1386 - acc: 0.957 - ETA: 1s - loss: 0.1380 - acc: 0.957 - ETA: 0s - loss: 0.1439 - acc: 0.955 - ETA: 0s - loss: 0.1442 - acc: 0.955 - ETA: 0s - loss: 0.1458 - acc: 0.954 - ETA: 0s - loss: 0.1444 - acc: 0.955 - ETA: 0s - loss: 0.1442 - acc: 0.954 - ETA: 0s - loss: 0.1459 - acc: 0.954 - ETA: 0s - loss: 0.1503 - acc: 0.953 - ETA: 0s - loss: 0.1539 - acc: 0.952 - ETA: 0s - loss: 0.1544 - acc: 0.952 - ETA: 0s - loss: 0.1541 - acc: 0.952 - ETA: 0s - loss: 0.1547 - acc: 0.952 - ETA: 0s - loss: 0.1538 - acc: 0.952 - ETA: 0s - loss: 0.1541 - acc: 0.952 - ETA: 0s - loss: 0.1566 - acc: 0.951 - ETA: 0s - loss: 0.1594 - acc: 0.951 - ETA: 0s - loss: 0.1582 - acc: 0.951 - ETA: 0s - loss: 0.1587 - acc: 0.951 - ETA: 0s - loss: 0.1567 - acc: 0.951 - 3s 392us/step - loss: 0.1562 - acc: 0.9515 - val_loss: 0.5680 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48890\n",
      "Epoch 10/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.4525 - acc: 0.900 - ETA: 2s - loss: 0.1518 - acc: 0.981 - ETA: 2s - loss: 0.1225 - acc: 0.980 - ETA: 2s - loss: 0.1512 - acc: 0.972 - ETA: 2s - loss: 0.1628 - acc: 0.968 - ETA: 2s - loss: 0.1804 - acc: 0.960 - ETA: 2s - loss: 0.1721 - acc: 0.960 - ETA: 2s - loss: 0.1620 - acc: 0.963 - ETA: 1s - loss: 0.1593 - acc: 0.960 - ETA: 1s - loss: 0.1604 - acc: 0.958 - ETA: 1s - loss: 0.1585 - acc: 0.956 - ETA: 1s - loss: 0.1509 - acc: 0.958 - ETA: 1s - loss: 0.1493 - acc: 0.959 - ETA: 1s - loss: 0.1507 - acc: 0.960 - ETA: 1s - loss: 0.1488 - acc: 0.961 - ETA: 1s - loss: 0.1520 - acc: 0.959 - ETA: 1s - loss: 0.1573 - acc: 0.957 - ETA: 1s - loss: 0.1568 - acc: 0.957 - ETA: 1s - loss: 0.1527 - acc: 0.958 - ETA: 1s - loss: 0.1521 - acc: 0.958 - ETA: 1s - loss: 0.1509 - acc: 0.958 - ETA: 1s - loss: 0.1465 - acc: 0.959 - ETA: 1s - loss: 0.1439 - acc: 0.959 - ETA: 1s - loss: 0.1442 - acc: 0.959 - ETA: 1s - loss: 0.1417 - acc: 0.959 - ETA: 1s - loss: 0.1434 - acc: 0.959 - ETA: 0s - loss: 0.1405 - acc: 0.960 - ETA: 0s - loss: 0.1385 - acc: 0.959 - ETA: 0s - loss: 0.1365 - acc: 0.960 - ETA: 0s - loss: 0.1368 - acc: 0.960 - ETA: 0s - loss: 0.1386 - acc: 0.960 - ETA: 0s - loss: 0.1376 - acc: 0.960 - ETA: 0s - loss: 0.1404 - acc: 0.960 - ETA: 0s - loss: 0.1402 - acc: 0.960 - ETA: 0s - loss: 0.1400 - acc: 0.960 - ETA: 0s - loss: 0.1390 - acc: 0.960 - ETA: 0s - loss: 0.1398 - acc: 0.960 - ETA: 0s - loss: 0.1411 - acc: 0.959 - ETA: 0s - loss: 0.1427 - acc: 0.958 - ETA: 0s - loss: 0.1425 - acc: 0.958 - ETA: 0s - loss: 0.1425 - acc: 0.957 - ETA: 0s - loss: 0.1437 - acc: 0.957 - ETA: 0s - loss: 0.1439 - acc: 0.957 - ETA: 0s - loss: 0.1429 - acc: 0.957 - ETA: 0s - loss: 0.1426 - acc: 0.957 - 3s 394us/step - loss: 0.1431 - acc: 0.9575 - val_loss: 0.5580 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48890\n",
      "Epoch 11/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1799 - acc: 0.900 - ETA: 2s - loss: 0.0917 - acc: 0.972 - ETA: 2s - loss: 0.0956 - acc: 0.973 - ETA: 2s - loss: 0.0928 - acc: 0.972 - ETA: 2s - loss: 0.0902 - acc: 0.971 - ETA: 2s - loss: 0.0988 - acc: 0.966 - ETA: 1s - loss: 0.0910 - acc: 0.969 - ETA: 1s - loss: 0.1001 - acc: 0.970 - ETA: 1s - loss: 0.1176 - acc: 0.964 - ETA: 1s - loss: 0.1096 - acc: 0.967 - ETA: 1s - loss: 0.1151 - acc: 0.967 - ETA: 1s - loss: 0.1176 - acc: 0.967 - ETA: 1s - loss: 0.1188 - acc: 0.968 - ETA: 1s - loss: 0.1206 - acc: 0.966 - ETA: 1s - loss: 0.1244 - acc: 0.966 - ETA: 1s - loss: 0.1206 - acc: 0.966 - ETA: 1s - loss: 0.1313 - acc: 0.964 - ETA: 1s - loss: 0.1287 - acc: 0.965 - ETA: 1s - loss: 0.1239 - acc: 0.966 - ETA: 1s - loss: 0.1238 - acc: 0.966 - ETA: 1s - loss: 0.1275 - acc: 0.964 - ETA: 1s - loss: 0.1241 - acc: 0.965 - ETA: 1s - loss: 0.1254 - acc: 0.964 - ETA: 1s - loss: 0.1235 - acc: 0.964 - ETA: 1s - loss: 0.1269 - acc: 0.963 - ETA: 0s - loss: 0.1257 - acc: 0.963 - ETA: 0s - loss: 0.1267 - acc: 0.963 - ETA: 0s - loss: 0.1246 - acc: 0.964 - ETA: 0s - loss: 0.1237 - acc: 0.963 - ETA: 0s - loss: 0.1246 - acc: 0.963 - ETA: 0s - loss: 0.1232 - acc: 0.963 - ETA: 0s - loss: 0.1223 - acc: 0.963 - ETA: 0s - loss: 0.1227 - acc: 0.964 - ETA: 0s - loss: 0.1229 - acc: 0.963 - ETA: 0s - loss: 0.1230 - acc: 0.963 - ETA: 0s - loss: 0.1218 - acc: 0.963 - ETA: 0s - loss: 0.1221 - acc: 0.963 - ETA: 0s - loss: 0.1239 - acc: 0.963 - ETA: 0s - loss: 0.1246 - acc: 0.963 - ETA: 0s - loss: 0.1240 - acc: 0.963 - ETA: 0s - loss: 0.1232 - acc: 0.963 - ETA: 0s - loss: 0.1244 - acc: 0.963 - ETA: 0s - loss: 0.1258 - acc: 0.962 - ETA: 0s - loss: 0.1276 - acc: 0.962 - 3s 387us/step - loss: 0.1320 - acc: 0.9615 - val_loss: 0.6072 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48890\n",
      "Epoch 12/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0234 - acc: 1.000 - ETA: 2s - loss: 0.1420 - acc: 0.943 - ETA: 2s - loss: 0.1152 - acc: 0.956 - ETA: 2s - loss: 0.1325 - acc: 0.961 - ETA: 2s - loss: 0.1370 - acc: 0.956 - ETA: 2s - loss: 0.1184 - acc: 0.963 - ETA: 2s - loss: 0.1226 - acc: 0.964 - ETA: 2s - loss: 0.1267 - acc: 0.962 - ETA: 1s - loss: 0.1183 - acc: 0.965 - ETA: 1s - loss: 0.1125 - acc: 0.963 - ETA: 1s - loss: 0.1131 - acc: 0.963 - ETA: 1s - loss: 0.1089 - acc: 0.964 - ETA: 1s - loss: 0.1099 - acc: 0.963 - ETA: 1s - loss: 0.1101 - acc: 0.961 - ETA: 1s - loss: 0.1134 - acc: 0.961 - ETA: 1s - loss: 0.1123 - acc: 0.961 - ETA: 1s - loss: 0.1109 - acc: 0.962 - ETA: 1s - loss: 0.1113 - acc: 0.962 - ETA: 1s - loss: 0.1151 - acc: 0.961 - ETA: 1s - loss: 0.1133 - acc: 0.962 - ETA: 1s - loss: 0.1149 - acc: 0.961 - ETA: 1s - loss: 0.1159 - acc: 0.961 - ETA: 1s - loss: 0.1173 - acc: 0.960 - ETA: 1s - loss: 0.1166 - acc: 0.961 - ETA: 1s - loss: 0.1188 - acc: 0.960 - ETA: 1s - loss: 0.1182 - acc: 0.960 - ETA: 1s - loss: 0.1163 - acc: 0.961 - ETA: 0s - loss: 0.1141 - acc: 0.962 - ETA: 1s - loss: 0.1147 - acc: 0.962 - ETA: 0s - loss: 0.1160 - acc: 0.961 - ETA: 0s - loss: 0.1159 - acc: 0.961 - ETA: 0s - loss: 0.1176 - acc: 0.960 - ETA: 0s - loss: 0.1211 - acc: 0.960 - ETA: 0s - loss: 0.1233 - acc: 0.959 - ETA: 0s - loss: 0.1245 - acc: 0.959 - ETA: 0s - loss: 0.1247 - acc: 0.959 - ETA: 0s - loss: 0.1230 - acc: 0.959 - ETA: 0s - loss: 0.1241 - acc: 0.959 - ETA: 0s - loss: 0.1282 - acc: 0.958 - ETA: 0s - loss: 0.1272 - acc: 0.959 - ETA: 0s - loss: 0.1281 - acc: 0.959 - ETA: 0s - loss: 0.1259 - acc: 0.960 - ETA: 0s - loss: 0.1258 - acc: 0.960 - ETA: 0s - loss: 0.1240 - acc: 0.960 - ETA: 0s - loss: 0.1243 - acc: 0.960 - 3s 415us/step - loss: 0.1246 - acc: 0.9608 - val_loss: 0.5981 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48890\n",
      "Epoch 13/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0911 - acc: 0.900 - ETA: 2s - loss: 0.0444 - acc: 0.977 - ETA: 2s - loss: 0.0451 - acc: 0.982 - ETA: 2s - loss: 0.0565 - acc: 0.980 - ETA: 2s - loss: 0.0489 - acc: 0.983 - ETA: 2s - loss: 0.0544 - acc: 0.981 - ETA: 1s - loss: 0.0627 - acc: 0.979 - ETA: 1s - loss: 0.0710 - acc: 0.978 - ETA: 1s - loss: 0.0721 - acc: 0.978 - ETA: 1s - loss: 0.0848 - acc: 0.973 - ETA: 1s - loss: 0.0880 - acc: 0.973 - ETA: 1s - loss: 0.0883 - acc: 0.972 - ETA: 1s - loss: 0.0930 - acc: 0.973 - ETA: 1s - loss: 0.0944 - acc: 0.971 - ETA: 1s - loss: 0.0964 - acc: 0.970 - ETA: 1s - loss: 0.0952 - acc: 0.971 - ETA: 1s - loss: 0.0977 - acc: 0.970 - ETA: 1s - loss: 0.1021 - acc: 0.969 - ETA: 1s - loss: 0.1035 - acc: 0.968 - ETA: 1s - loss: 0.1031 - acc: 0.968 - ETA: 1s - loss: 0.1022 - acc: 0.968 - ETA: 1s - loss: 0.1039 - acc: 0.968 - ETA: 1s - loss: 0.1067 - acc: 0.967 - ETA: 1s - loss: 0.1090 - acc: 0.966 - ETA: 1s - loss: 0.1086 - acc: 0.967 - ETA: 0s - loss: 0.1071 - acc: 0.966 - ETA: 0s - loss: 0.1065 - acc: 0.967 - ETA: 0s - loss: 0.1109 - acc: 0.966 - ETA: 0s - loss: 0.1115 - acc: 0.965 - ETA: 0s - loss: 0.1114 - acc: 0.966 - ETA: 0s - loss: 0.1123 - acc: 0.965 - ETA: 0s - loss: 0.1112 - acc: 0.965 - ETA: 0s - loss: 0.1103 - acc: 0.965 - ETA: 0s - loss: 0.1108 - acc: 0.964 - ETA: 0s - loss: 0.1116 - acc: 0.965 - ETA: 0s - loss: 0.1116 - acc: 0.965 - ETA: 0s - loss: 0.1119 - acc: 0.964 - ETA: 0s - loss: 0.1108 - acc: 0.965 - ETA: 0s - loss: 0.1112 - acc: 0.964 - ETA: 0s - loss: 0.1118 - acc: 0.964 - ETA: 0s - loss: 0.1111 - acc: 0.964 - ETA: 0s - loss: 0.1129 - acc: 0.964 - ETA: 0s - loss: 0.1124 - acc: 0.964 - ETA: 0s - loss: 0.1139 - acc: 0.963 - 3s 391us/step - loss: 0.1136 - acc: 0.9638 - val_loss: 0.6242 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48890\n",
      "Epoch 14/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1999 - acc: 0.950 - ETA: 2s - loss: 0.1031 - acc: 0.955 - ETA: 2s - loss: 0.0880 - acc: 0.965 - ETA: 2s - loss: 0.0971 - acc: 0.967 - ETA: 2s - loss: 0.1386 - acc: 0.966 - ETA: 2s - loss: 0.1262 - acc: 0.968 - ETA: 2s - loss: 0.1238 - acc: 0.967 - ETA: 2s - loss: 0.1155 - acc: 0.970 - ETA: 2s - loss: 0.1076 - acc: 0.972 - ETA: 1s - loss: 0.1073 - acc: 0.973 - ETA: 1s - loss: 0.1184 - acc: 0.971 - ETA: 1s - loss: 0.1187 - acc: 0.970 - ETA: 1s - loss: 0.1176 - acc: 0.968 - ETA: 1s - loss: 0.1147 - acc: 0.968 - ETA: 1s - loss: 0.1127 - acc: 0.969 - ETA: 1s - loss: 0.1069 - acc: 0.970 - ETA: 1s - loss: 0.1018 - acc: 0.972 - ETA: 1s - loss: 0.1009 - acc: 0.971 - ETA: 1s - loss: 0.0992 - acc: 0.971 - ETA: 1s - loss: 0.0980 - acc: 0.971 - ETA: 1s - loss: 0.0992 - acc: 0.970 - ETA: 1s - loss: 0.0971 - acc: 0.971 - ETA: 1s - loss: 0.0964 - acc: 0.970 - ETA: 1s - loss: 0.0945 - acc: 0.971 - ETA: 1s - loss: 0.0944 - acc: 0.971 - ETA: 1s - loss: 0.0951 - acc: 0.970 - ETA: 1s - loss: 0.0942 - acc: 0.970 - ETA: 0s - loss: 0.0954 - acc: 0.970 - ETA: 0s - loss: 0.0979 - acc: 0.969 - ETA: 0s - loss: 0.0961 - acc: 0.969 - ETA: 0s - loss: 0.0982 - acc: 0.969 - ETA: 0s - loss: 0.1017 - acc: 0.968 - ETA: 0s - loss: 0.1010 - acc: 0.968 - ETA: 0s - loss: 0.1016 - acc: 0.968 - ETA: 0s - loss: 0.1008 - acc: 0.968 - ETA: 0s - loss: 0.1014 - acc: 0.968 - ETA: 0s - loss: 0.1010 - acc: 0.968 - ETA: 0s - loss: 0.1013 - acc: 0.967 - ETA: 0s - loss: 0.1006 - acc: 0.967 - ETA: 0s - loss: 0.1000 - acc: 0.967 - ETA: 0s - loss: 0.1002 - acc: 0.967 - ETA: 0s - loss: 0.1005 - acc: 0.967 - ETA: 0s - loss: 0.1045 - acc: 0.967 - ETA: 0s - loss: 0.1053 - acc: 0.967 - ETA: 0s - loss: 0.1073 - acc: 0.967 - 3s 397us/step - loss: 0.1059 - acc: 0.9677 - val_loss: 0.6098 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48890\n",
      "Epoch 15/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0037 - acc: 1.000 - ETA: 2s - loss: 0.0858 - acc: 0.977 - ETA: 2s - loss: 0.0912 - acc: 0.970 - ETA: 2s - loss: 0.0743 - acc: 0.975 - ETA: 2s - loss: 0.0766 - acc: 0.973 - ETA: 2s - loss: 0.0682 - acc: 0.976 - ETA: 2s - loss: 0.0765 - acc: 0.975 - ETA: 1s - loss: 0.0736 - acc: 0.976 - ETA: 1s - loss: 0.0730 - acc: 0.977 - ETA: 1s - loss: 0.0736 - acc: 0.975 - ETA: 1s - loss: 0.0757 - acc: 0.974 - ETA: 1s - loss: 0.0751 - acc: 0.973 - ETA: 1s - loss: 0.0809 - acc: 0.971 - ETA: 1s - loss: 0.0783 - acc: 0.972 - ETA: 1s - loss: 0.0862 - acc: 0.970 - ETA: 1s - loss: 0.0841 - acc: 0.970 - ETA: 1s - loss: 0.0863 - acc: 0.971 - ETA: 1s - loss: 0.0905 - acc: 0.972 - ETA: 1s - loss: 0.0904 - acc: 0.971 - ETA: 1s - loss: 0.0901 - acc: 0.971 - ETA: 1s - loss: 0.0889 - acc: 0.972 - ETA: 1s - loss: 0.0906 - acc: 0.971 - ETA: 1s - loss: 0.0898 - acc: 0.972 - ETA: 1s - loss: 0.0893 - acc: 0.972 - ETA: 1s - loss: 0.0899 - acc: 0.972 - ETA: 1s - loss: 0.0895 - acc: 0.972 - ETA: 0s - loss: 0.0902 - acc: 0.972 - ETA: 0s - loss: 0.0894 - acc: 0.972 - ETA: 0s - loss: 0.0923 - acc: 0.971 - ETA: 0s - loss: 0.0918 - acc: 0.971 - ETA: 0s - loss: 0.0950 - acc: 0.971 - ETA: 0s - loss: 0.0942 - acc: 0.970 - ETA: 0s - loss: 0.0931 - acc: 0.971 - ETA: 0s - loss: 0.0928 - acc: 0.970 - ETA: 0s - loss: 0.0923 - acc: 0.970 - ETA: 0s - loss: 0.0917 - acc: 0.971 - ETA: 0s - loss: 0.0914 - acc: 0.971 - ETA: 0s - loss: 0.0937 - acc: 0.970 - ETA: 0s - loss: 0.0944 - acc: 0.970 - ETA: 0s - loss: 0.0955 - acc: 0.970 - ETA: 0s - loss: 0.0971 - acc: 0.970 - ETA: 0s - loss: 0.0981 - acc: 0.970 - ETA: 0s - loss: 0.0994 - acc: 0.970 - ETA: 0s - loss: 0.0998 - acc: 0.970 - 3s 389us/step - loss: 0.0989 - acc: 0.9704 - val_loss: 0.6057 - val_acc: 0.8647\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48890\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0087 - acc: 1.000 - ETA: 2s - loss: 0.0171 - acc: 1.000 - ETA: 2s - loss: 0.0339 - acc: 0.985 - ETA: 2s - loss: 0.0303 - acc: 0.990 - ETA: 2s - loss: 0.0492 - acc: 0.987 - ETA: 2s - loss: 0.0570 - acc: 0.985 - ETA: 1s - loss: 0.0590 - acc: 0.983 - ETA: 1s - loss: 0.0575 - acc: 0.983 - ETA: 1s - loss: 0.0599 - acc: 0.982 - ETA: 1s - loss: 0.0615 - acc: 0.980 - ETA: 1s - loss: 0.0602 - acc: 0.981 - ETA: 1s - loss: 0.0655 - acc: 0.978 - ETA: 1s - loss: 0.0717 - acc: 0.977 - ETA: 1s - loss: 0.0688 - acc: 0.978 - ETA: 1s - loss: 0.0702 - acc: 0.978 - ETA: 1s - loss: 0.0715 - acc: 0.977 - ETA: 1s - loss: 0.0698 - acc: 0.978 - ETA: 1s - loss: 0.0698 - acc: 0.979 - ETA: 1s - loss: 0.0704 - acc: 0.978 - ETA: 1s - loss: 0.0742 - acc: 0.976 - ETA: 1s - loss: 0.0753 - acc: 0.976 - ETA: 1s - loss: 0.0740 - acc: 0.976 - ETA: 1s - loss: 0.0735 - acc: 0.976 - ETA: 1s - loss: 0.0795 - acc: 0.974 - ETA: 1s - loss: 0.0802 - acc: 0.974 - ETA: 0s - loss: 0.0812 - acc: 0.974 - ETA: 0s - loss: 0.0825 - acc: 0.974 - ETA: 0s - loss: 0.0828 - acc: 0.974 - ETA: 0s - loss: 0.0831 - acc: 0.974 - ETA: 0s - loss: 0.0832 - acc: 0.974 - ETA: 0s - loss: 0.0851 - acc: 0.974 - ETA: 0s - loss: 0.0831 - acc: 0.975 - ETA: 0s - loss: 0.0866 - acc: 0.974 - ETA: 0s - loss: 0.0853 - acc: 0.974 - ETA: 0s - loss: 0.0855 - acc: 0.974 - ETA: 0s - loss: 0.0855 - acc: 0.974 - ETA: 0s - loss: 0.0854 - acc: 0.974 - ETA: 0s - loss: 0.0867 - acc: 0.974 - ETA: 0s - loss: 0.0862 - acc: 0.974 - ETA: 0s - loss: 0.0872 - acc: 0.973 - ETA: 0s - loss: 0.0868 - acc: 0.973 - ETA: 0s - loss: 0.0856 - acc: 0.974 - ETA: 0s - loss: 0.0867 - acc: 0.973 - 3s 382us/step - loss: 0.0877 - acc: 0.9734 - val_loss: 0.6226 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48890\n",
      "Epoch 17/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0086 - acc: 1.000 - ETA: 2s - loss: 0.0374 - acc: 0.983 - ETA: 2s - loss: 0.0412 - acc: 0.982 - ETA: 2s - loss: 0.0426 - acc: 0.984 - ETA: 2s - loss: 0.0589 - acc: 0.978 - ETA: 2s - loss: 0.0640 - acc: 0.978 - ETA: 1s - loss: 0.0574 - acc: 0.980 - ETA: 1s - loss: 0.0566 - acc: 0.979 - ETA: 1s - loss: 0.0537 - acc: 0.979 - ETA: 1s - loss: 0.0535 - acc: 0.980 - ETA: 1s - loss: 0.0582 - acc: 0.979 - ETA: 1s - loss: 0.0609 - acc: 0.979 - ETA: 1s - loss: 0.0639 - acc: 0.978 - ETA: 1s - loss: 0.0643 - acc: 0.978 - ETA: 1s - loss: 0.0676 - acc: 0.977 - ETA: 1s - loss: 0.0702 - acc: 0.977 - ETA: 1s - loss: 0.0672 - acc: 0.978 - ETA: 1s - loss: 0.0736 - acc: 0.977 - ETA: 1s - loss: 0.0788 - acc: 0.976 - ETA: 1s - loss: 0.0792 - acc: 0.976 - ETA: 1s - loss: 0.0800 - acc: 0.975 - ETA: 1s - loss: 0.0796 - acc: 0.976 - ETA: 1s - loss: 0.0868 - acc: 0.974 - ETA: 1s - loss: 0.0909 - acc: 0.974 - ETA: 1s - loss: 0.0884 - acc: 0.975 - ETA: 0s - loss: 0.0884 - acc: 0.974 - ETA: 0s - loss: 0.0871 - acc: 0.974 - ETA: 0s - loss: 0.0854 - acc: 0.975 - ETA: 0s - loss: 0.0873 - acc: 0.975 - ETA: 0s - loss: 0.0873 - acc: 0.975 - ETA: 0s - loss: 0.0875 - acc: 0.975 - ETA: 0s - loss: 0.0866 - acc: 0.974 - ETA: 0s - loss: 0.0869 - acc: 0.975 - ETA: 0s - loss: 0.0896 - acc: 0.974 - ETA: 0s - loss: 0.0894 - acc: 0.974 - ETA: 0s - loss: 0.0883 - acc: 0.974 - ETA: 0s - loss: 0.0887 - acc: 0.974 - ETA: 0s - loss: 0.0882 - acc: 0.974 - ETA: 0s - loss: 0.0888 - acc: 0.974 - ETA: 0s - loss: 0.0880 - acc: 0.974 - ETA: 0s - loss: 0.0869 - acc: 0.975 - ETA: 0s - loss: 0.0863 - acc: 0.975 - ETA: 0s - loss: 0.0851 - acc: 0.975 - 3s 384us/step - loss: 0.0850 - acc: 0.9757 - val_loss: 0.6818 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.48890\n",
      "Epoch 18/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0247 - acc: 1.000 - ETA: 2s - loss: 0.0811 - acc: 0.962 - ETA: 2s - loss: 0.0576 - acc: 0.973 - ETA: 2s - loss: 0.0591 - acc: 0.978 - ETA: 2s - loss: 0.0528 - acc: 0.980 - ETA: 2s - loss: 0.0806 - acc: 0.980 - ETA: 2s - loss: 0.0789 - acc: 0.980 - ETA: 2s - loss: 0.0862 - acc: 0.979 - ETA: 1s - loss: 0.0844 - acc: 0.978 - ETA: 1s - loss: 0.0769 - acc: 0.981 - ETA: 1s - loss: 0.0867 - acc: 0.978 - ETA: 1s - loss: 0.0868 - acc: 0.979 - ETA: 1s - loss: 0.0848 - acc: 0.979 - ETA: 1s - loss: 0.0854 - acc: 0.979 - ETA: 1s - loss: 0.0908 - acc: 0.978 - ETA: 1s - loss: 0.0877 - acc: 0.979 - ETA: 1s - loss: 0.0903 - acc: 0.977 - ETA: 1s - loss: 0.0867 - acc: 0.978 - ETA: 1s - loss: 0.0864 - acc: 0.977 - ETA: 1s - loss: 0.0845 - acc: 0.978 - ETA: 1s - loss: 0.0861 - acc: 0.977 - ETA: 1s - loss: 0.0854 - acc: 0.977 - ETA: 1s - loss: 0.0862 - acc: 0.977 - ETA: 1s - loss: 0.0873 - acc: 0.976 - ETA: 1s - loss: 0.0853 - acc: 0.977 - ETA: 1s - loss: 0.0831 - acc: 0.977 - ETA: 1s - loss: 0.0824 - acc: 0.977 - ETA: 0s - loss: 0.0842 - acc: 0.977 - ETA: 0s - loss: 0.0832 - acc: 0.977 - ETA: 0s - loss: 0.0824 - acc: 0.977 - ETA: 0s - loss: 0.0824 - acc: 0.977 - ETA: 0s - loss: 0.0805 - acc: 0.977 - ETA: 0s - loss: 0.0833 - acc: 0.977 - ETA: 0s - loss: 0.0835 - acc: 0.977 - ETA: 0s - loss: 0.0833 - acc: 0.976 - ETA: 0s - loss: 0.0819 - acc: 0.977 - ETA: 0s - loss: 0.0823 - acc: 0.977 - ETA: 0s - loss: 0.0817 - acc: 0.977 - ETA: 0s - loss: 0.0810 - acc: 0.977 - ETA: 0s - loss: 0.0800 - acc: 0.977 - ETA: 0s - loss: 0.0790 - acc: 0.977 - ETA: 0s - loss: 0.0779 - acc: 0.978 - ETA: 0s - loss: 0.0788 - acc: 0.977 - ETA: 0s - loss: 0.0792 - acc: 0.977 - ETA: 0s - loss: 0.0782 - acc: 0.977 - 3s 397us/step - loss: 0.0776 - acc: 0.9781 - val_loss: 0.6689 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48890\n",
      "Epoch 19/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0020 - acc: 1.000 - ETA: 2s - loss: 0.0907 - acc: 0.977 - ETA: 2s - loss: 0.0747 - acc: 0.982 - ETA: 2s - loss: 0.0641 - acc: 0.982 - ETA: 2s - loss: 0.0659 - acc: 0.979 - ETA: 2s - loss: 0.0688 - acc: 0.981 - ETA: 2s - loss: 0.0642 - acc: 0.981 - ETA: 1s - loss: 0.0653 - acc: 0.982 - ETA: 1s - loss: 0.0631 - acc: 0.981 - ETA: 1s - loss: 0.0614 - acc: 0.981 - ETA: 1s - loss: 0.0622 - acc: 0.980 - ETA: 1s - loss: 0.0606 - acc: 0.981 - ETA: 1s - loss: 0.0584 - acc: 0.982 - ETA: 1s - loss: 0.0597 - acc: 0.981 - ETA: 1s - loss: 0.0612 - acc: 0.980 - ETA: 1s - loss: 0.0594 - acc: 0.981 - ETA: 1s - loss: 0.0577 - acc: 0.981 - ETA: 1s - loss: 0.0622 - acc: 0.981 - ETA: 1s - loss: 0.0695 - acc: 0.979 - ETA: 1s - loss: 0.0677 - acc: 0.980 - ETA: 1s - loss: 0.0664 - acc: 0.979 - ETA: 1s - loss: 0.0664 - acc: 0.979 - ETA: 1s - loss: 0.0668 - acc: 0.978 - ETA: 1s - loss: 0.0663 - acc: 0.978 - ETA: 1s - loss: 0.0679 - acc: 0.978 - ETA: 0s - loss: 0.0688 - acc: 0.978 - ETA: 0s - loss: 0.0686 - acc: 0.979 - ETA: 0s - loss: 0.0733 - acc: 0.978 - ETA: 0s - loss: 0.0758 - acc: 0.977 - ETA: 0s - loss: 0.0737 - acc: 0.978 - ETA: 0s - loss: 0.0730 - acc: 0.978 - ETA: 0s - loss: 0.0714 - acc: 0.978 - ETA: 0s - loss: 0.0716 - acc: 0.978 - ETA: 0s - loss: 0.0775 - acc: 0.977 - ETA: 0s - loss: 0.0765 - acc: 0.977 - ETA: 0s - loss: 0.0755 - acc: 0.977 - ETA: 0s - loss: 0.0779 - acc: 0.977 - ETA: 0s - loss: 0.0781 - acc: 0.976 - ETA: 0s - loss: 0.0773 - acc: 0.976 - ETA: 0s - loss: 0.0767 - acc: 0.977 - ETA: 0s - loss: 0.0770 - acc: 0.977 - ETA: 0s - loss: 0.0774 - acc: 0.977 - ETA: 0s - loss: 0.0776 - acc: 0.976 - ETA: 0s - loss: 0.0763 - acc: 0.977 - 3s 388us/step - loss: 0.0765 - acc: 0.9772 - val_loss: 0.6764 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48890\n",
      "Epoch 20/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0090 - acc: 1.000 - ETA: 2s - loss: 0.0292 - acc: 0.994 - ETA: 2s - loss: 0.0202 - acc: 0.997 - ETA: 2s - loss: 0.0282 - acc: 0.992 - ETA: 2s - loss: 0.0409 - acc: 0.987 - ETA: 2s - loss: 0.0488 - acc: 0.985 - ETA: 1s - loss: 0.0497 - acc: 0.983 - ETA: 1s - loss: 0.0493 - acc: 0.983 - ETA: 1s - loss: 0.0519 - acc: 0.982 - ETA: 1s - loss: 0.0491 - acc: 0.984 - ETA: 1s - loss: 0.0472 - acc: 0.985 - ETA: 1s - loss: 0.0517 - acc: 0.984 - ETA: 1s - loss: 0.0558 - acc: 0.983 - ETA: 1s - loss: 0.0579 - acc: 0.982 - ETA: 1s - loss: 0.0549 - acc: 0.983 - ETA: 1s - loss: 0.0614 - acc: 0.982 - ETA: 1s - loss: 0.0602 - acc: 0.982 - ETA: 1s - loss: 0.0632 - acc: 0.982 - ETA: 1s - loss: 0.0640 - acc: 0.981 - ETA: 1s - loss: 0.0613 - acc: 0.982 - ETA: 1s - loss: 0.0607 - acc: 0.982 - ETA: 1s - loss: 0.0612 - acc: 0.982 - ETA: 1s - loss: 0.0611 - acc: 0.982 - ETA: 1s - loss: 0.0608 - acc: 0.982 - ETA: 1s - loss: 0.0615 - acc: 0.981 - ETA: 1s - loss: 0.0690 - acc: 0.980 - ETA: 0s - loss: 0.0691 - acc: 0.980 - ETA: 0s - loss: 0.0708 - acc: 0.980 - ETA: 0s - loss: 0.0702 - acc: 0.980 - ETA: 0s - loss: 0.0730 - acc: 0.979 - ETA: 0s - loss: 0.0722 - acc: 0.980 - ETA: 0s - loss: 0.0705 - acc: 0.980 - ETA: 0s - loss: 0.0692 - acc: 0.980 - ETA: 0s - loss: 0.0692 - acc: 0.980 - ETA: 0s - loss: 0.0700 - acc: 0.980 - ETA: 0s - loss: 0.0687 - acc: 0.980 - ETA: 0s - loss: 0.0692 - acc: 0.980 - ETA: 0s - loss: 0.0703 - acc: 0.980 - ETA: 0s - loss: 0.0693 - acc: 0.980 - ETA: 0s - loss: 0.0687 - acc: 0.980 - ETA: 0s - loss: 0.0706 - acc: 0.980 - ETA: 0s - loss: 0.0706 - acc: 0.980 - ETA: 0s - loss: 0.0704 - acc: 0.980 - ETA: 0s - loss: 0.0703 - acc: 0.980 - ETA: 0s - loss: 0.0696 - acc: 0.980 - 3s 391us/step - loss: 0.0696 - acc: 0.9802 - val_loss: 0.6744 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48890\n",
      "Epoch 21/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0049 - acc: 1.000 - ETA: 2s - loss: 0.0951 - acc: 0.975 - ETA: 2s - loss: 0.0603 - acc: 0.986 - ETA: 2s - loss: 0.0465 - acc: 0.990 - ETA: 2s - loss: 0.0388 - acc: 0.991 - ETA: 2s - loss: 0.0392 - acc: 0.990 - ETA: 2s - loss: 0.0335 - acc: 0.992 - ETA: 2s - loss: 0.0387 - acc: 0.991 - ETA: 1s - loss: 0.0438 - acc: 0.990 - ETA: 1s - loss: 0.0436 - acc: 0.990 - ETA: 1s - loss: 0.0430 - acc: 0.990 - ETA: 1s - loss: 0.0419 - acc: 0.991 - ETA: 1s - loss: 0.0423 - acc: 0.990 - ETA: 1s - loss: 0.0444 - acc: 0.989 - ETA: 1s - loss: 0.0450 - acc: 0.989 - ETA: 1s - loss: 0.0444 - acc: 0.988 - ETA: 1s - loss: 0.0464 - acc: 0.987 - ETA: 1s - loss: 0.0521 - acc: 0.987 - ETA: 1s - loss: 0.0514 - acc: 0.987 - ETA: 1s - loss: 0.0522 - acc: 0.986 - ETA: 1s - loss: 0.0529 - acc: 0.986 - ETA: 1s - loss: 0.0519 - acc: 0.986 - ETA: 1s - loss: 0.0509 - acc: 0.987 - ETA: 1s - loss: 0.0555 - acc: 0.986 - ETA: 1s - loss: 0.0612 - acc: 0.984 - ETA: 1s - loss: 0.0616 - acc: 0.983 - ETA: 0s - loss: 0.0608 - acc: 0.984 - ETA: 0s - loss: 0.0625 - acc: 0.983 - ETA: 0s - loss: 0.0630 - acc: 0.983 - ETA: 0s - loss: 0.0627 - acc: 0.983 - ETA: 0s - loss: 0.0656 - acc: 0.982 - ETA: 0s - loss: 0.0648 - acc: 0.983 - ETA: 0s - loss: 0.0660 - acc: 0.982 - ETA: 0s - loss: 0.0643 - acc: 0.983 - ETA: 0s - loss: 0.0635 - acc: 0.983 - ETA: 0s - loss: 0.0644 - acc: 0.983 - ETA: 0s - loss: 0.0668 - acc: 0.982 - ETA: 0s - loss: 0.0665 - acc: 0.982 - ETA: 0s - loss: 0.0668 - acc: 0.982 - ETA: 0s - loss: 0.0682 - acc: 0.982 - ETA: 0s - loss: 0.0670 - acc: 0.982 - ETA: 0s - loss: 0.0665 - acc: 0.982 - ETA: 0s - loss: 0.0674 - acc: 0.981 - ETA: 0s - loss: 0.0667 - acc: 0.981 - ETA: 0s - loss: 0.0680 - acc: 0.981 - 3s 395us/step - loss: 0.0680 - acc: 0.9813 - val_loss: 0.6980 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48890\n",
      "Epoch 22/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0048 - acc: 1.000 - ETA: 2s - loss: 0.0169 - acc: 0.994 - ETA: 2s - loss: 0.0509 - acc: 0.981 - ETA: 2s - loss: 0.0674 - acc: 0.982 - ETA: 2s - loss: 0.0688 - acc: 0.980 - ETA: 2s - loss: 0.0698 - acc: 0.978 - ETA: 2s - loss: 0.0615 - acc: 0.980 - ETA: 2s - loss: 0.0626 - acc: 0.981 - ETA: 2s - loss: 0.0626 - acc: 0.981 - ETA: 1s - loss: 0.0567 - acc: 0.983 - ETA: 1s - loss: 0.0529 - acc: 0.984 - ETA: 1s - loss: 0.0522 - acc: 0.983 - ETA: 1s - loss: 0.0523 - acc: 0.983 - ETA: 1s - loss: 0.0500 - acc: 0.984 - ETA: 1s - loss: 0.0526 - acc: 0.983 - ETA: 1s - loss: 0.0532 - acc: 0.982 - ETA: 1s - loss: 0.0516 - acc: 0.983 - ETA: 1s - loss: 0.0504 - acc: 0.983 - ETA: 1s - loss: 0.0506 - acc: 0.983 - ETA: 1s - loss: 0.0486 - acc: 0.984 - ETA: 1s - loss: 0.0473 - acc: 0.984 - ETA: 1s - loss: 0.0561 - acc: 0.982 - ETA: 1s - loss: 0.0580 - acc: 0.981 - ETA: 1s - loss: 0.0601 - acc: 0.981 - ETA: 1s - loss: 0.0592 - acc: 0.982 - ETA: 1s - loss: 0.0585 - acc: 0.982 - ETA: 0s - loss: 0.0577 - acc: 0.982 - ETA: 0s - loss: 0.0572 - acc: 0.982 - ETA: 0s - loss: 0.0566 - acc: 0.982 - ETA: 0s - loss: 0.0557 - acc: 0.982 - ETA: 0s - loss: 0.0603 - acc: 0.981 - ETA: 0s - loss: 0.0618 - acc: 0.982 - ETA: 0s - loss: 0.0632 - acc: 0.981 - ETA: 0s - loss: 0.0631 - acc: 0.981 - ETA: 0s - loss: 0.0634 - acc: 0.980 - ETA: 0s - loss: 0.0631 - acc: 0.980 - ETA: 0s - loss: 0.0625 - acc: 0.980 - ETA: 0s - loss: 0.0631 - acc: 0.980 - ETA: 0s - loss: 0.0630 - acc: 0.980 - ETA: 0s - loss: 0.0634 - acc: 0.980 - ETA: 0s - loss: 0.0629 - acc: 0.980 - ETA: 0s - loss: 0.0629 - acc: 0.980 - ETA: 0s - loss: 0.0619 - acc: 0.980 - ETA: 0s - loss: 0.0622 - acc: 0.980 - 3s 390us/step - loss: 0.0620 - acc: 0.9807 - val_loss: 0.7001 - val_acc: 0.8395\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48890\n",
      "Epoch 23/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1443 - acc: 0.950 - ETA: 2s - loss: 0.0260 - acc: 0.994 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0313 - acc: 0.992 - ETA: 2s - loss: 0.0342 - acc: 0.992 - ETA: 2s - loss: 0.0388 - acc: 0.990 - ETA: 1s - loss: 0.0375 - acc: 0.988 - ETA: 1s - loss: 0.0719 - acc: 0.982 - ETA: 1s - loss: 0.0652 - acc: 0.983 - ETA: 1s - loss: 0.0637 - acc: 0.982 - ETA: 1s - loss: 0.0594 - acc: 0.983 - ETA: 1s - loss: 0.0685 - acc: 0.981 - ETA: 1s - loss: 0.0688 - acc: 0.980 - ETA: 1s - loss: 0.0667 - acc: 0.981 - ETA: 1s - loss: 0.0638 - acc: 0.981 - ETA: 1s - loss: 0.0662 - acc: 0.981 - ETA: 1s - loss: 0.0648 - acc: 0.981 - ETA: 1s - loss: 0.0632 - acc: 0.981 - ETA: 1s - loss: 0.0625 - acc: 0.981 - ETA: 1s - loss: 0.0645 - acc: 0.981 - ETA: 1s - loss: 0.0623 - acc: 0.982 - ETA: 1s - loss: 0.0609 - acc: 0.982 - ETA: 1s - loss: 0.0595 - acc: 0.982 - ETA: 1s - loss: 0.0582 - acc: 0.983 - ETA: 1s - loss: 0.0566 - acc: 0.983 - ETA: 0s - loss: 0.0558 - acc: 0.983 - ETA: 0s - loss: 0.0554 - acc: 0.983 - ETA: 0s - loss: 0.0542 - acc: 0.983 - ETA: 0s - loss: 0.0585 - acc: 0.983 - ETA: 0s - loss: 0.0587 - acc: 0.983 - ETA: 0s - loss: 0.0598 - acc: 0.983 - ETA: 0s - loss: 0.0598 - acc: 0.982 - ETA: 0s - loss: 0.0583 - acc: 0.983 - ETA: 0s - loss: 0.0568 - acc: 0.983 - ETA: 0s - loss: 0.0563 - acc: 0.983 - ETA: 0s - loss: 0.0558 - acc: 0.983 - ETA: 0s - loss: 0.0555 - acc: 0.983 - ETA: 0s - loss: 0.0557 - acc: 0.984 - ETA: 0s - loss: 0.0555 - acc: 0.984 - ETA: 0s - loss: 0.0572 - acc: 0.983 - ETA: 0s - loss: 0.0560 - acc: 0.983 - ETA: 0s - loss: 0.0557 - acc: 0.983 - ETA: 0s - loss: 0.0557 - acc: 0.983 - 3s 383us/step - loss: 0.0553 - acc: 0.9834 - val_loss: 0.7162 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48890\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0429 - acc: 1.000 - ETA: 2s - loss: 0.0312 - acc: 0.988 - ETA: 2s - loss: 0.0344 - acc: 0.990 - ETA: 2s - loss: 0.0549 - acc: 0.989 - ETA: 2s - loss: 0.0601 - acc: 0.985 - ETA: 2s - loss: 0.0512 - acc: 0.988 - ETA: 1s - loss: 0.0627 - acc: 0.987 - ETA: 1s - loss: 0.0561 - acc: 0.989 - ETA: 1s - loss: 0.0512 - acc: 0.989 - ETA: 1s - loss: 0.0495 - acc: 0.988 - ETA: 1s - loss: 0.0551 - acc: 0.988 - ETA: 1s - loss: 0.0560 - acc: 0.987 - ETA: 1s - loss: 0.0565 - acc: 0.986 - ETA: 1s - loss: 0.0558 - acc: 0.987 - ETA: 1s - loss: 0.0541 - acc: 0.987 - ETA: 1s - loss: 0.0521 - acc: 0.988 - ETA: 1s - loss: 0.0521 - acc: 0.987 - ETA: 1s - loss: 0.0502 - acc: 0.988 - ETA: 1s - loss: 0.0492 - acc: 0.987 - ETA: 1s - loss: 0.0528 - acc: 0.987 - ETA: 1s - loss: 0.0536 - acc: 0.987 - ETA: 1s - loss: 0.0557 - acc: 0.986 - ETA: 1s - loss: 0.0578 - acc: 0.986 - ETA: 1s - loss: 0.0573 - acc: 0.986 - ETA: 1s - loss: 0.0570 - acc: 0.986 - ETA: 0s - loss: 0.0586 - acc: 0.986 - ETA: 0s - loss: 0.0568 - acc: 0.986 - ETA: 0s - loss: 0.0569 - acc: 0.986 - ETA: 0s - loss: 0.0565 - acc: 0.986 - ETA: 0s - loss: 0.0578 - acc: 0.985 - ETA: 0s - loss: 0.0575 - acc: 0.985 - ETA: 0s - loss: 0.0577 - acc: 0.985 - ETA: 0s - loss: 0.0590 - acc: 0.984 - ETA: 0s - loss: 0.0582 - acc: 0.984 - ETA: 0s - loss: 0.0581 - acc: 0.984 - ETA: 0s - loss: 0.0574 - acc: 0.985 - ETA: 0s - loss: 0.0579 - acc: 0.984 - ETA: 0s - loss: 0.0587 - acc: 0.984 - ETA: 0s - loss: 0.0577 - acc: 0.984 - ETA: 0s - loss: 0.0584 - acc: 0.984 - ETA: 0s - loss: 0.0574 - acc: 0.984 - ETA: 0s - loss: 0.0565 - acc: 0.985 - ETA: 0s - loss: 0.0565 - acc: 0.985 - ETA: 0s - loss: 0.0576 - acc: 0.984 - 3s 392us/step - loss: 0.0572 - acc: 0.9849 - val_loss: 0.7502 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48890\n",
      "Epoch 25/25\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.6623e-04 - acc: 1.000 - ETA: 2s - loss: 0.0183 - acc: 0.9944    - ETA: 2s - loss: 0.0339 - acc: 0.988 - ETA: 2s - loss: 0.0377 - acc: 0.990 - ETA: 2s - loss: 0.0373 - acc: 0.989 - ETA: 2s - loss: 0.0346 - acc: 0.990 - ETA: 1s - loss: 0.0456 - acc: 0.986 - ETA: 1s - loss: 0.0417 - acc: 0.987 - ETA: 1s - loss: 0.0401 - acc: 0.987 - ETA: 1s - loss: 0.0383 - acc: 0.988 - ETA: 1s - loss: 0.0378 - acc: 0.987 - ETA: 1s - loss: 0.0372 - acc: 0.987 - ETA: 1s - loss: 0.0355 - acc: 0.988 - ETA: 1s - loss: 0.0333 - acc: 0.989 - ETA: 1s - loss: 0.0324 - acc: 0.989 - ETA: 1s - loss: 0.0341 - acc: 0.988 - ETA: 1s - loss: 0.0402 - acc: 0.988 - ETA: 1s - loss: 0.0417 - acc: 0.987 - ETA: 1s - loss: 0.0411 - acc: 0.987 - ETA: 1s - loss: 0.0399 - acc: 0.987 - ETA: 1s - loss: 0.0414 - acc: 0.987 - ETA: 1s - loss: 0.0419 - acc: 0.987 - ETA: 1s - loss: 0.0422 - acc: 0.987 - ETA: 1s - loss: 0.0413 - acc: 0.987 - ETA: 0s - loss: 0.0420 - acc: 0.987 - ETA: 0s - loss: 0.0413 - acc: 0.987 - ETA: 0s - loss: 0.0402 - acc: 0.987 - ETA: 0s - loss: 0.0417 - acc: 0.987 - ETA: 0s - loss: 0.0418 - acc: 0.987 - ETA: 0s - loss: 0.0413 - acc: 0.987 - ETA: 0s - loss: 0.0452 - acc: 0.987 - ETA: 0s - loss: 0.0459 - acc: 0.987 - ETA: 0s - loss: 0.0463 - acc: 0.987 - ETA: 0s - loss: 0.0481 - acc: 0.986 - ETA: 0s - loss: 0.0493 - acc: 0.986 - ETA: 0s - loss: 0.0526 - acc: 0.985 - ETA: 0s - loss: 0.0526 - acc: 0.985 - ETA: 0s - loss: 0.0523 - acc: 0.985 - ETA: 0s - loss: 0.0530 - acc: 0.985 - ETA: 0s - loss: 0.0544 - acc: 0.985 - ETA: 0s - loss: 0.0541 - acc: 0.985 - ETA: 0s - loss: 0.0534 - acc: 0.985 - ETA: 0s - loss: 0.0535 - acc: 0.985 - 3s 384us/step - loss: 0.0540 - acc: 0.9853 - val_loss: 0.7006 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24720cb8b00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xcept.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Xcept_model.fit(train_Xcept, train_targets, \n",
    "          validation_data=(valid_Xcept, valid_targets),\n",
    "          epochs=25, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "Xcept_model.load_weights('saved_models/weights.best.Xcept.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 85.6459%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "Xcept_predictions = [np.argmax(Xcept_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xcept]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Xcept_predictions)==np.argmax(test_targets, axis=1))/len(Xcept_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "def Xcept_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Xception(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = Xcept_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def dog_detector_Xception(img_path):\n",
    "    \n",
    "    print('\\n-------' + img_path + '-------')\n",
    "    \n",
    "    if(dog_detector(img_path)):\n",
    "        print('Hello dog,')\n",
    "        print('You look like a: ' + Xcept_predict_breed(img_path))\n",
    "    \n",
    "    elif(face_detector(img_path)):\n",
    "        print('Hello Human,')\n",
    "        print('You look like a: ' + Xcept_predict_breed(img_path))\n",
    "    \n",
    "    else:\n",
    "        print('What have you sent me??')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ I think the algorithm works quite well when there is no noise in the picture.  The few misclassifications that I observed were of dog breeds that I cannot tell a difference between(Newfoundland vs Chow Chow and German Pinscher vs Doberman Pinscher).   \n",
    "\n",
    "Some methods of improving the algorithm:\n",
    "\n",
    "- Augment the images and run the augmented images through the algorithm to update the weights.  \n",
    "- Finding a better algorithm for face detection vs dog detection would help better classify a few misclassifications\n",
    "- Since there are multiple breeds that are close (Pinschers and Labs) I wonder if there is a method of making a more complex classifier to help resolve the misclassification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------dogImages/test\\106.Newfoundland\\Newfoundland_07009.jpg-------\n",
      "Hello dog,\n",
      "You look like a: Chow_chow\n",
      "\n",
      "-------extra_images/Bm_LB0785.jpg-------\n",
      "Hello Human,\n",
      "You look like a: Chinese_crested\n",
      "\n",
      "-------dogImages/test\\024.Bichon_frise\\Bichon_frise_01707.jpg-------\n",
      "Hello dog,\n",
      "You look like a: Bichon_frise\n",
      "\n",
      "-------extra_images/download.png-------\n",
      "Hello Human,\n",
      "You look like a: Lowchen\n",
      "\n",
      "-------dogImages/test\\070.German_pinscher\\German_pinscher_04843.jpg-------\n",
      "Hello dog,\n",
      "You look like a: Doberman_pinscher\n",
      "\n",
      "-------extra_images/processed_image4.jpg-------\n",
      "Hello Human,\n",
      "You look like a: Chinese_crested\n",
      "\n",
      "-------extra_images/pukey.jpg-------\n",
      "What have you sent me??\n",
      "\n",
      "-------extra_images/IMG-0939.JPG-------\n",
      "Hello Human,\n",
      "You look like a: Dachshund\n",
      "\n",
      "-------extra_images/IMG-0940.JPG-------\n",
      "Hello Human,\n",
      "You look like a: Chinese_shar-pei\n",
      "\n",
      "-------extra_images/IMG-0942.JPG-------\n",
      "Hello Human,\n",
      "You look like a: Chinese_crested\n",
      "\n",
      "-------extra_images/IMG-0959.JPG-------\n",
      "Hello dog,\n",
      "You look like a: Maltese\n",
      "\n",
      "-------extra_images/IMG-0958.JPG-------\n",
      "Hello dog,\n",
      "You look like a: Maltese\n",
      "\n",
      "-------extra_images/IMG-0952.JPG-------\n",
      "Hello Human,\n",
      "You look like a: Petit_basset_griffon_vendeen\n",
      "\n",
      "-------extra_images/IMG-1972.JPG-------\n",
      "Hello Human,\n",
      "You look like a: Portuguese_water_dog\n"
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "\n",
    "dog_detector_Xception(test_files[34])\n",
    "\n",
    "dog_detector_Xception('extra_images/Bm_LB0785.jpg')\n",
    "\n",
    "dog_detector_Xception(test_files[103])\n",
    "\n",
    "dog_detector_Xception('extra_images/download.png')\n",
    "\n",
    "dog_detector_Xception(test_files[97])\n",
    "\n",
    "dog_detector_Xception('extra_images/processed_image4.jpg')\n",
    "\n",
    "dog_detector_Xception('extra_images/pukey.jpg')\n",
    "\n",
    "dog_detector_Xception('extra_images/IMG-0939.JPG')\n",
    "\n",
    "dog_detector_Xception('extra_images/IMG-0940.JPG')\n",
    "\n",
    "dog_detector_Xception('extra_images/IMG-0942.JPG')\n",
    "\n",
    "dog_detector_Xception('extra_images/IMG-0959.JPG')\n",
    "\n",
    "dog_detector_Xception('extra_images/IMG-0958.JPG')\n",
    "\n",
    "dog_detector_Xception('extra_images/IMG-0952.JPG')\n",
    "\n",
    "dog_detector_Xception('extra_images/IMG-1972.JPG')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
